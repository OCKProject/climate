<?xml version="1.0" encoding="UTF-8"?>
<sec id="s2c" class="sec">
 <div class="title" xmlns="http://www.w3.org/1999/xhtml">Pattern Matching</div>
 <p xmlns="http://www.w3.org/1999/xhtml">The automatic classification of species by machine learning techniques has a common theme. The features extracted from the unknown recordings are compared to prototypes extracted from labelled reference data in order to find possible matches. The label of the best matching reference becomes the label of the unknown recording. The prototypes can be probabilistic descriptions as in GMMs, HMMs 
  <a rid="pone.0096936-Potamitis1" ref-type="bibr" href="#pone.0096936-Potamitis1">[7]</a>, 
  <a rid="pone.0096936-Skowronski1" ref-type="bibr" href="#pone.0096936-Skowronski1">[12]</a> or spectrographic patches serving as templates as in the general detection framework X-Bat (
  <span ext-link-type="uri" xlink:href="http://www.birds.cornell.edu/brp/software/xbat-introduction" class="ext-link" xmlns:xlink="http://www.w3.org/1999/xlink">http://www.birds.cornell.edu/brp/software/xbat-introduction</span>). The matching is based on calculating a distance between the target prototypes and the unknown recordings. Again the distance can be one suited against a probabilistic approach e.g. a likelihood score, a probability or a cross correlation score. The final decision on which species are to be found in an unknown recording comes after comparing the distance to a threshold.
 </p>
 <p xmlns="http://www.w3.org/1999/xhtml">Species classification approaches based on Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) as typically applied to speech are related to animal vocalization classification quite well. In fact HMM’s were among the first applied classifiers following their success in speech/speaker recognition. After a thorough experimentation on private dataset of annual recordings with sophisticated versions of these tools 
  <a rid="pone.0096936-Potamitis1" ref-type="bibr" href="#pone.0096936-Potamitis1">[7]</a> we discovered 2 major drawbacks of the HMM/GMM approach:
 </p>
 <div list-type="alpha-lower" class="list" xmlns="http://www.w3.org/1999/xhtml">
  <div class="list-item">
   <p>Recordings in the wild can be very noisy due to their exposure to a large number of audio sources originating from all distances and directions, the number and identity of which cannot be known 
    <span class="italic">a-priori</span>. The co-existence of the target vocalisation with other species and abiotic interferences is inefficiently treated by current approaches of audio signal enhancement and separation when the number and the nature of audio sources is unknown as when coming from an unconstrained environment. These audio sources often appear simultaneously with target vocalisations over a single time frame (see 
    <a ref-type="fig" rid="pone-0096936-g001" href="#pone-0096936-g001">Fig. 1</a> and 
    <a ref-type="fig" rid="pone-0096936-g003" href="#pone-0096936-g003">Fig. 3</a> for common examples). GMMs/HMMs model the features extracted from overlapping frame analysis of sound. An overlapping time-frame sound analysis will inevitably include in its spectrum some of these interferences as well as possibly and quite often the vocalisations of a number of species.
   </p>
  </div>
  <div class="list-item">
   <p>The GMMs/HMMs species detectors derive a probability per frame (target vs. everything else -the so called ‘world model’). In the case of applying this classifier to wild-life recordings the vocalizing species will change from season to season and therefore the world-model as well. A GMM/HMM detector produces erroneous results if it is not properly updated by re-training or adapting to new species. The update procedure requires an expert in birds’ vocalisations to be available to sort out which species change and are probable to be confused with the targeted ones. This problem is not obvious when one is analysing e.g. one month of data but is prevalent in annual data analyses.</p>
  </div>
 </div>
 <div id="pone-0096936-g003" orientation="portrait" position="float" class="fig" xmlns="http://www.w3.org/1999/xhtml">
  <span pub-id-type="doi" class="object-id">10.1371/journal.pone.0096936.g003</span>
  <span class="label">Figure 3</span>
  <div class="caption">
   <div class="title">Spectrogram corresponding to a recording with 3 partially overlapping bird species (trainfile005 in NIPS20134B database).</div>
   <p>The lower part of the spectrum is coloured by the sound of running water and strong wind.</p>
  </div>
  <div xlink:href="pone.0096936.g003" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
 </div>
 <p xmlns="http://www.w3.org/1999/xhtml">Spectrographic patches serving as templates is also one of the very first approaches employed and still is used mostly as providing a proof of concept on manually selected example recordings 
  <a rid="pone.0096936-Keen1" ref-type="bibr" href="#pone.0096936-Keen1">[25]</a>. In unconstrained real life scenarios the variability between vocalisations of a single individual with limited repertoire not to mention variability among different individuals of the same species can be so large that templates are not capable to grasp the individuality of a target species. Moreover, spectrogram patches are vulnerable to noise and to competing species while the slightest spectral deformation can result to a large distance between the unknown vocalisation and the template.
 </p>
 <p xmlns="http://www.w3.org/1999/xhtml">The base classifier in our approach is a random forest under the multi-label formulation of one vs. all (the so – called binary relevance approach 
  <a rid="pone.0096936-Briggs1" ref-type="bibr" href="#pone.0096936-Briggs1">[5]</a>). Random forests are an ensemble learning method for classification that is based on constructing many decision trees at training time and outputting the class that is the mode of the classes to the end of individual trees 
  <a rid="pone.0096936-Breiman1" ref-type="bibr" href="#pone.0096936-Breiman1">[22]</a>. Random forests are the machine learning technique of choice here as our approach is based on deriving multiple - heterogeneous sets of features that aim to grasp different aspects of the spectrogram picture. The final dimensionality of the feature set is much larger than the size of the training set. There are few classifiers that can deal with such large dimensionalities. The class of Support Vector Machines and Extra Randomised Trees 
  <a rid="pone.0096936-Hastie1" ref-type="bibr" href="#pone.0096936-Hastie1">[24]</a> that can also deal with high dimensional features were also tried out but with inferior results.
 </p>
</sec>
