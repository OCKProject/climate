<?xml version="1.0" encoding="UTF-8"?>
<sec sec-type="methods" id="s2" class="sec">
 <div class="title" xmlns="http://www.w3.org/1999/xhtml">Methods</div>
 <sec id="s2a" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">The Database and its Specificities</div>
  <p xmlns="http://www.w3.org/1999/xhtml">The NIPS 2013 database is composed of 1687 field-recordings each containing vocalisations of 0–6 different species. A subset of 687 recordings is offered as a training set accompanied with known annotation from bioacoustics experts and we seek the species existing in the other 1000 recordings. The hardware used to collect the data was a number of ARUs placed at different locations of the French Provence and provided by the BIOTOPE society (see 
   <span ext-link-type="uri" xlink:href="http://www.biotope.fr/for" class="ext-link" xmlns:xlink="http://www.w3.org/1999/xlink">http://www.biotope.fr/for</span> details). All recordings are monophonic, sampled at 44100 Hz with variable duration from 0.25 up to 5.75 secs. We did not downsample the original recordings as there were insect species singing up to 20 kHz and night birds calling as low as at 500 Hz.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">The training set matches the test set conditions. The list of animals can be found in 
   <a rid="pone.0096936-Glotin1" ref-type="bibr" href="#pone.0096936-Glotin1">[14]</a> and contains the most encountered species in central Europe. Another unique aspect of the NIPS dataset is the distinction between calls and songs of the same species that are treated as distinct categories that the algorithms should resolve. The recordings are selected in such a way that different kinds of difficulties are encountered in almost every recording, namely:
  </p>
  <div list-type="alpha-lower" class="list" xmlns="http://www.w3.org/1999/xhtml">
   <div class="list-item">
    <p>Different bird species can and often do overlap in the same time-frame. Vocalizations can also overlap in frequency but, do not generally overlap both in time and frequency simultaneously. This can be attributed to the sparsity of the time-frequency domain but also to the observed fact that co-existing species make use of different bandwidths in order to communicate efficiently.</p>
   </div>
   <div class="list-item">
    <p>Different gains of signals due to distance: as many vocalizing animals can be at any distance from the microphone, often weak calls are picked up along with a dominating vocalization (see 
     <a ref-type="fig" rid="pone-0096936-g001" href="#pone-0096936-g001">Fig. 1</a>).
    </p>
   </div>
   <div class="list-item">
    <p>Anthropogenic noises such as airplanes, sirens, footsteps, human speech in biotopes near urban territories can lead any classifier to error, because they either obscure the target signal causing a miss or become themselves classified as the target signal (false alarm).</p>
   </div>
   <div class="list-item">
    <p>Abiotic sounds due to heavy wind, rain, and recording device failures can substantially reduce the quality of recordings. The sound of rain-drops and recording device failures produce sound events that are short-time with broadband spectral characteristics and can be misled with insect stridulations (see 
     <a ref-type="fig" rid="pone-0096936-g002" href="#pone-0096936-g002">Fig. 2</a>).
    </p>
   </div>
  </div>
  <div id="pone-0096936-g001" orientation="portrait" position="float" class="fig" xmlns="http://www.w3.org/1999/xhtml">
   <span pub-id-type="doi" class="object-id">10.1371/journal.pone.0096936.g001</span>
   <span class="label">Figure 1</span>
   <div class="caption">
    <div class="title">3D spectrogram of a typical recording of 3 species (trainfile115 in NIPS20134B database) demonstrating the difficulties in recognition due to different gains in target signals and noise.</div>
    <p>From 14–19 kHz a Cicada, 8–13 kHz Bush Cricket, 2.5–3.8 Eurasian Blackcap, 0–2 kHz strong airplane noise.</p>
   </div>
   <div xlink:href="pone.0096936.g001" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
  </div>
  <div id="pone-0096936-g002" orientation="portrait" position="float" class="fig" xmlns="http://www.w3.org/1999/xhtml">
   <span pub-id-type="doi" class="object-id">10.1371/journal.pone.0096936.g002</span>
   <span class="label">Figure 2</span>
   <div class="caption">
    <div class="title">Types of anthropogenic and abiotic interfering sounds.</div>
   </div>
   <div xlink:href="pone.0096936.g002" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
  </div>
 </sec>
 <sec id="s2b" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Signal Processing &amp; Pattern Recognition as Applied to Animal Vocalisations</div>
  <sec id="s2b1" class="sec">
   <div class="title" xmlns="http://www.w3.org/1999/xhtml">Single- and multi-label approaches</div>
   <p xmlns="http://www.w3.org/1999/xhtml">Recognizers of animal vocalisations use prototypes of the target species extracted from examples of annotated training data. These recordings come out of large bioacoustics inventories (e.g. DORSA (
    <span ext-link-type="uri" xlink:href="http://www.dorsa.de" class="ext-link" xmlns:xlink="http://www.w3.org/1999/xlink">http://www.dorsa.de</span>), MacAulay Library (
    <span ext-link-type="uri" xlink:href="http://macaulaylibrary.org/index.do" class="ext-link" xmlns:xlink="http://www.w3.org/1999/xlink">http://macaulaylibrary.org/index.do</span>), Tierstimmenarchivb (
    <span ext-link-type="uri" xlink:href="http://www.tierstimmenarchiv.de" class="ext-link" xmlns:xlink="http://www.w3.org/1999/xlink">http://www.tierstimmenarchiv.de</span>), and Xeno-canto (
    <span ext-link-type="uri" xlink:href="http://www.xeno-canto.org" class="ext-link" xmlns:xlink="http://www.w3.org/1999/xlink">http://www.xeno-canto.org</span>) or from private corpora.
   </p>
   <p xmlns="http://www.w3.org/1999/xhtml">These inventories, at the time of their compilation did not follow a common protocol on how to log the target species. Some contain just a clip of the target species, others contain the target species in the presence of several others or in the presence of noise or they are even human narrated. The vast majority of reported research on classifying animal vocalisations follow the single label paradigm (a recording holds a single species). The reference library of a single instance recognizer should contain a large number of only the target species and nothing else, as the recognizer will learn the sound that is unrelated to the target species, which in the end will increase its false alarms. The single label approach comes with a major drawback. Not many people are qualified to recognize species efficiently by sound. In the single-instance case, the construction of a reference library is a quite laborious task as the expert must screen all recordings and apply delicate time-frequency cleaning to any possible interference. Most experts are unwilling to undertake this task for a large number of species and recordings. Again, single-instance recognizers are vulnerable to false positives, i.e. species that are not the target but sound like it and are included by mistake in the reference library. The Multi-label approach on bird recognition is in our opinion a breakthrough and was originally proposed for the task of bird recognition in 
    <a rid="pone.0096936-Briggs1" ref-type="bibr" href="#pone.0096936-Briggs1">[5]</a>. In this approach the expert does not need to clean or time-stamp the recordings at all.
   </p>
   <p xmlns="http://www.w3.org/1999/xhtml">The expert just writes down the acoustic classes that appear in a recording e.g. European Robin, Common Chaffinch, without any particular order or time stamp. That is, he/she has to select the set of labels attributed to the recording without having to clean, remove, isolate or timestamp parts of the recording. Each recording in the NIPS 20134B database has vocalisations from a varying number of species and the training corpus is annotated to the species level. We transformed the annotation data of all recordings to a binary matrix of a dimension 687×88 (687 training files, 87 for the species and 1 for noise only recordings as seen in 
    <a rid="pone-0096936-t001" ref-type="table" href="#pone-0096936-t001">Table 1</a>). Note that any all noise types as well as recording void of vocalisations can be pooled to a general noise class.
   </p>
   <div id="pone-0096936-t001" orientation="portrait" position="float" class="table-wrap" xmlns="http://www.w3.org/1999/xhtml">
    <span pub-id-type="doi" class="object-id">10.1371/journal.pone.0096936.t001</span>
    <span class="label">Table 1</span>
    <div class="caption">
     <div class="title">Annotation sample of the training data under the multi-label framework.</div>
    </div>
    <div class="alternatives">
     <div id="pone-0096936-t001-1" xlink:href="pone.0096936.t001" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
     <table frame="hsides" rules="groups">
      <colgroup span="1">
       <col align="left" span="1"/>
       <col align="center" span="1"/>
       <col align="center" span="1"/>
       <col align="center" span="1"/>
       <col align="center" span="1"/>
      </colgroup>
      <thead>
       <tr>
        <td align="left" rowspan="1" colspan="1">#File</td>
        <td align="left" rowspan="1" colspan="1">Noise</td>
        <td align="left" rowspan="1" colspan="1">Long-tailed Tit</td>
        <td align="left" rowspan="1" colspan="1">…</td>
        <td align="left" rowspan="1" colspan="1">Turdus philomelos</td>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td align="left" rowspan="1" colspan="1">001</td>
        <td align="left" rowspan="1" colspan="1">0</td>
        <td align="left" rowspan="1" colspan="1">1</td>
        <td align="left" rowspan="1" colspan="1"/>
        <td align="left" rowspan="1" colspan="1">1</td>
       </tr>
       <tr>
        <td align="left" rowspan="1" colspan="1">002</td>
        <td align="left" rowspan="1" colspan="1">1</td>
        <td align="left" rowspan="1" colspan="1">0</td>
        <td align="left" rowspan="1" colspan="1">1</td>
        <td align="left" rowspan="1" colspan="1">1</td>
       </tr>
      </tbody>
     </table>
    </div>
   </div>
   <p xmlns="http://www.w3.org/1999/xhtml">The multi-label approach is responsible for marginalizing over the multiple labels and associate probabilities to species. This procedure accelerates the annotation of the human expert by far, as it alleviates the necessity of isolating the target signal. The expert, either by spectrographic analysis or by listening to the dubious cases, selects the appropriate tags and moves on to the next recording. Therefore, the multi-label approach relaxes by far the effort of constructing more complete reference databases.</p>
  </sec>
 </sec>
 <sec id="s2c" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Pattern Matching</div>
  <p xmlns="http://www.w3.org/1999/xhtml">The automatic classification of species by machine learning techniques has a common theme. The features extracted from the unknown recordings are compared to prototypes extracted from labelled reference data in order to find possible matches. The label of the best matching reference becomes the label of the unknown recording. The prototypes can be probabilistic descriptions as in GMMs, HMMs 
   <a rid="pone.0096936-Potamitis1" ref-type="bibr" href="#pone.0096936-Potamitis1">[7]</a>, 
   <a rid="pone.0096936-Skowronski1" ref-type="bibr" href="#pone.0096936-Skowronski1">[12]</a> or spectrographic patches serving as templates as in the general detection framework X-Bat (
   <span ext-link-type="uri" xlink:href="http://www.birds.cornell.edu/brp/software/xbat-introduction" class="ext-link" xmlns:xlink="http://www.w3.org/1999/xlink">http://www.birds.cornell.edu/brp/software/xbat-introduction</span>). The matching is based on calculating a distance between the target prototypes and the unknown recordings. Again the distance can be one suited against a probabilistic approach e.g. a likelihood score, a probability or a cross correlation score. The final decision on which species are to be found in an unknown recording comes after comparing the distance to a threshold.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">Species classification approaches based on Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) as typically applied to speech are related to animal vocalization classification quite well. In fact HMM’s were among the first applied classifiers following their success in speech/speaker recognition. After a thorough experimentation on private dataset of annual recordings with sophisticated versions of these tools 
   <a rid="pone.0096936-Potamitis1" ref-type="bibr" href="#pone.0096936-Potamitis1">[7]</a> we discovered 2 major drawbacks of the HMM/GMM approach:
  </p>
  <div list-type="alpha-lower" class="list" xmlns="http://www.w3.org/1999/xhtml">
   <div class="list-item">
    <p>Recordings in the wild can be very noisy due to their exposure to a large number of audio sources originating from all distances and directions, the number and identity of which cannot be known 
     <span class="italic">a-priori</span>. The co-existence of the target vocalisation with other species and abiotic interferences is inefficiently treated by current approaches of audio signal enhancement and separation when the number and the nature of audio sources is unknown as when coming from an unconstrained environment. These audio sources often appear simultaneously with target vocalisations over a single time frame (see 
     <a ref-type="fig" rid="pone-0096936-g001" href="#pone-0096936-g001">Fig. 1</a> and 
     <a ref-type="fig" rid="pone-0096936-g003" href="#pone-0096936-g003">Fig. 3</a> for common examples). GMMs/HMMs model the features extracted from overlapping frame analysis of sound. An overlapping time-frame sound analysis will inevitably include in its spectrum some of these interferences as well as possibly and quite often the vocalisations of a number of species.
    </p>
   </div>
   <div class="list-item">
    <p>The GMMs/HMMs species detectors derive a probability per frame (target vs. everything else -the so called ‘world model’). In the case of applying this classifier to wild-life recordings the vocalizing species will change from season to season and therefore the world-model as well. A GMM/HMM detector produces erroneous results if it is not properly updated by re-training or adapting to new species. The update procedure requires an expert in birds’ vocalisations to be available to sort out which species change and are probable to be confused with the targeted ones. This problem is not obvious when one is analysing e.g. one month of data but is prevalent in annual data analyses.</p>
   </div>
  </div>
  <div id="pone-0096936-g003" orientation="portrait" position="float" class="fig" xmlns="http://www.w3.org/1999/xhtml">
   <span pub-id-type="doi" class="object-id">10.1371/journal.pone.0096936.g003</span>
   <span class="label">Figure 3</span>
   <div class="caption">
    <div class="title">Spectrogram corresponding to a recording with 3 partially overlapping bird species (trainfile005 in NIPS20134B database).</div>
    <p>The lower part of the spectrum is coloured by the sound of running water and strong wind.</p>
   </div>
   <div xlink:href="pone.0096936.g003" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
  </div>
  <p xmlns="http://www.w3.org/1999/xhtml">Spectrographic patches serving as templates is also one of the very first approaches employed and still is used mostly as providing a proof of concept on manually selected example recordings 
   <a rid="pone.0096936-Keen1" ref-type="bibr" href="#pone.0096936-Keen1">[25]</a>. In unconstrained real life scenarios the variability between vocalisations of a single individual with limited repertoire not to mention variability among different individuals of the same species can be so large that templates are not capable to grasp the individuality of a target species. Moreover, spectrogram patches are vulnerable to noise and to competing species while the slightest spectral deformation can result to a large distance between the unknown vocalisation and the template.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">The base classifier in our approach is a random forest under the multi-label formulation of one vs. all (the so – called binary relevance approach 
   <a rid="pone.0096936-Briggs1" ref-type="bibr" href="#pone.0096936-Briggs1">[5]</a>). Random forests are an ensemble learning method for classification that is based on constructing many decision trees at training time and outputting the class that is the mode of the classes to the end of individual trees 
   <a rid="pone.0096936-Breiman1" ref-type="bibr" href="#pone.0096936-Breiman1">[22]</a>. Random forests are the machine learning technique of choice here as our approach is based on deriving multiple - heterogeneous sets of features that aim to grasp different aspects of the spectrogram picture. The final dimensionality of the feature set is much larger than the size of the training set. There are few classifiers that can deal with such large dimensionalities. The class of Support Vector Machines and Extra Randomised Trees 
   <a rid="pone.0096936-Hastie1" ref-type="bibr" href="#pone.0096936-Hastie1">[24]</a> that can also deal with high dimensional features were also tried out but with inferior results.
  </p>
 </sec>
 <sec id="s2d" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Feature Extraction</div>
  <sec id="s2d1" class="sec">
   <div class="title" xmlns="http://www.w3.org/1999/xhtml">The spectrogram as an image</div>
   <p xmlns="http://www.w3.org/1999/xhtml">In our work, we do not follow the framework of audio analysis but the framework of image analysis. Treating the audio scene as a picture means looking at the spectrogram as a canvas where the acoustic events appear as localised spectral blobs on a two-dimensional matrix (see 
    <a ref-type="fig" rid="pone-0096936-g003" href="#pone-0096936-g003">Fig. 3</a>). Once these spectral blobs are extracted, many feature extraction techniques can be applied exclusively on these spectral patches while ignoring the rest of the spectrum. In this section we describe how we extract the regions of interest (ROIs) from the spectrogram.
   </p>
   <p xmlns="http://www.w3.org/1999/xhtml">The recording is firstly amplitude normalized. Then morphological operations are applied on the image. These operation have as function to derive masks of spectral blobs by connecting regions of high amplitude that correspond to calls or phrases and to eliminate small regions of high amplitude that cannot belong to animal vocalizations because they are too small. Several approaches have been tried as: removing from the image a blurred version of the same image as well as removing the morphological opening of the image. First a morphological opening on an image is applied that can remove small bright spots. Opening is defined as an erosion followed by a dilation. Erosion shrinks bright regions and enlarges dark regions. Dilation has the opposite effect of erosion and enlarges bright regions and shrinks dark regions. The border segments are dropped as the lower part correspond almost always to low-pass noise spectral patches. All these different are standard approaches in image processing having as a result to remove background illumination and extract spectral blobs 
    <a rid="pone.0096936-Shih1" ref-type="bibr" href="#pone.0096936-Shih1">[21]</a>.
   </p>
   <p xmlns="http://www.w3.org/1999/xhtml">As an example, in the sample image (see 
    <a ref-type="fig" rid="pone-0096936-g003" href="#pone-0096936-g003">Fig. 3</a>), the background illumination is less bright at the bottom of the image than at the centre or top. This is due to the sound of nearby running water and wind that corrupt the low frequencies of the spectrum. After the morphological operations are applied the picture of the spectrogram is made binary in order to mark the masks of the spectral blobs.
   </p>
   <p xmlns="http://www.w3.org/1999/xhtml">Binarization is realised by thresholding the image to the 90% percentile of the data (i.e. that is the highest 90% value). Subsequently we label the connected components in the 2-D binary image to derive the masks where the ROIs exist. We call ROIs the spectral patches cropped by the associated masks. Small masks (smaller than a fixed number of pixels) are discarded (see 
    <a ref-type="fig" rid="pone-0096936-g004" href="#pone-0096936-g004">Fig. 4</a>). The threshold for removing small masks is derived by observing short but perceptible calls from the training data and the number of pixels is set to 100. The ROIs are the patches from the original spectrogram that correspond to the pixel coordinates of the masks. All the ROIs extracted from the training set and test set are stored along with the frequency location from which they were cropped. The time information is dropped as an animal can vocalise at any time within a recording but only at the frequencies of its repertoire (see 
    <a ref-type="fig" rid="pone-0096936-g005" href="#pone-0096936-g005">Fig. 5</a> for the segments extracted from the recording of 
    <a ref-type="fig" rid="pone-0096936-g003" href="#pone-0096936-g003">Fig. 3</a>).
   </p>
   <div id="pone-0096936-g004" orientation="portrait" position="float" class="fig" xmlns="http://www.w3.org/1999/xhtml">
    <span pub-id-type="doi" class="object-id">10.1371/journal.pone.0096936.g004</span>
    <span class="label">Figure 4</span>
    <div class="caption">
     <div class="title">Detected spectrogram blobs of 
      <a ref-type="fig" rid="pone-0096936-g003" href="#pone-0096936-g003">Fig. 3</a>.
     </div>
     <p>Derivations and enumeration of the masks. Axis are enumerated according to their pixel index.</p>
    </div>
    <div xlink:href="pone.0096936.g004" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
   </div>
   <div id="pone-0096936-g005" orientation="portrait" position="float" class="fig" xmlns="http://www.w3.org/1999/xhtml">
    <span pub-id-type="doi" class="object-id">10.1371/journal.pone.0096936.g005</span>
    <span class="label">Figure 5</span>
    <div class="caption">
     <div class="title">ROIs extracted after applying the masks of Fig. 4 onto the spectrogram of 
      <a ref-type="fig" rid="pone-0096936-g003" href="#pone-0096936-g003">Fig. 3</a>, enumerated and catalogued.
     </div>
     <p>The same procedure is followed for all recordings.</p>
    </div>
    <div xlink:href="pone.0096936.g005" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
   </div>
   <p xmlns="http://www.w3.org/1999/xhtml">
    <a ref-type="fig" rid="pone-0096936-g005" href="#pone-0096936-g005">Fig. 5</a> illustrates the ROI’s extracted from a single recording. The 16798 ROIs of all 1687 recordings are automatically extracted, enumerated and catalogued in under 12 minutes using an i7, 3.4 GHz machine. This is a distinct difference from the seminal approach of 
    <a rid="pone.0096936-Briggs1" ref-type="bibr" href="#pone.0096936-Briggs1">[5]</a> where the ROIs are manually extracted, which is very time-consuming and practically impossible when the number of recordings is large. Cataloguing entails storing the spectral patches and the frequency borders in pixel coordinates from where it was extracted. Everything else but the ROIs is discarded from the data.
   </p>
   <p xmlns="http://www.w3.org/1999/xhtml">The benefits of extracting the ROIs are:</p>
   <div list-type="alpha-lower" class="list" xmlns="http://www.w3.org/1999/xhtml">
    <div class="list-item">
     <p>The great reduction of variability between recordings. Every further analysis of our database will be done in reference to these ROIs alone while the rest of the spectrum is disregarded (compare 
      <a ref-type="fig" rid="pone-0096936-g005" href="#pone-0096936-g005">Fig. 5</a> to the original 
      <a ref-type="fig" rid="pone-0096936-g003" href="#pone-0096936-g003">Fig. 3</a>).
     </p>
    </div>
    <div class="list-item">
     <p>Once extracted these ROIs allow us to derive a plethora of features with gradual increase in sophistication namely: Marginal over time measurements, statistical descriptors of the shape of the ROIs and finally how the ROIs of the training set alone correlate to the spectrograms of the test set.</p>
    </div>
   </div>
  </sec>
 </sec>
 <sec id="s2e" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Extraction of Marginal Over Time Measurements</div>
  <p xmlns="http://www.w3.org/1999/xhtml">Spectrogram reconstruction from ROIs entails that an enhanced spectrogram is extracted from the original one by imposing the ROIs of the original spectrum on an empty spectrogram (see 
   <a ref-type="fig" rid="pone-0096936-g004" href="#pone-0096936-g004">Fig. 4</a>). Each spectrogram is partitioned in 16 equal and non-overlapping horizontal bands. Subsequently several statistics are derived over this enhanced spectrum on a per band basis. This approach resembles filterbank analysis in audio. We do not apply mel-spacing as different birds and insects can vocalize anywhere in the spectrum. This approach allows us to make use of the band-limited character of most species and leave the marginalization process to the multi-label approach. For each band a set of basic descriptive statistics to represent the essence of each band concisely is derived from the intensities of each band, namely: Mean, standard deviation, median and kurtosis and the 90% percentile. The median is found by sorting the values and picking the middle one and is a way to summarize statistical distribution. Kurtosis (fourth central moment) is used as a measure of peaky vs. flat. In order to derive the 90% percentiles the data are ordered and the one at the highest 90% position is derived. A percentile is a statistical measure below which a given percentage of measurements in a group of measurements fall. Other descriptive statistics have been tried, such as entropy and skewness, but without presenting any statistically significant difference in the classification accuracy. Therefore, 5 descriptors for each of the 16 bands is derived resulting to a 5×16 = 80 dimensional vector per recording and a total training matrix 
   <span class="bold">S
    <sub>1</sub>
   </span>
   <span class="sup">687×80</span>. One should note that with this feature alone the multi-label approach achieves 86.37% average accuracy (see Results section).
  </p>
 </sec>
 <sec id="s2f" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Bag of Segments, Statistics of Segments and Coding Over Multiple Codebooks</div>
  <p xmlns="http://www.w3.org/1999/xhtml">
   <span class="bold">S
    <sub>1</sub>
   </span> offers a gross description of the spectrum. The 
   <span class="bold">S
    <sub>1</sub>
   </span> features set can capture quite well species vocalising in different frequency bands but it fails to capture species vocalising in the same band. Another set of features, named 
   <span class="bold">S
    <sub>2</sub>
   </span> is extracted out of the examination of the morphology of each ROI. Each recording is considered as a bag of segments where the bag is the set composed of all ROIs in a recording and each ROI is an instance. For the ROIs corresponding to each recording we derive the following features:
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">Area of the binary mask noted as 
   <span class="italic">M
    <sub>area</sub>
   </span> of the mask 
   <span class="italic">M</span>:
   <div id="pone.0096936.e001" class="disp-formula">
    <div xlink:href="pone.0096936.e001.jpg" position="anchor" orientation="portrait" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
    <span class="label">(1)</span>
   </div>where 
   <span class="italic">t, f</span> are pixel indices belonging to the area of the mask.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">Mean off the spectral patch corresponding to a ROI:
   <div id="pone.0096936.e002" class="disp-formula">
    <div xlink:href="pone.0096936.e002.jpg" position="anchor" orientation="portrait" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
    <span class="label">(2)</span>
   </div>where S
   <span class="italic">
    <sub>t,f</sub>
   </span> corresponds to the amplitude of the spectral chunk of the underlying binary mask 
   <span class="italic">M
    <sub>t,f</sub>.
   </span>
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">Standard deviation of the spectral ROI:
   <div id="pone.0096936.e003" class="disp-formula">
    <div xlink:href="pone.0096936.e003.jpg" position="anchor" orientation="portrait" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
    <span class="label">(3)</span>
   </div>
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">The Median, Minimum and Maximum value of the spectral patch defined by ROIs and the Maximum position of the spectral ROIs (only the frequency index retained) as well as bandwidth and duration of each ROI are set as features. Additionally, we calculate the mel-frequency cepstral coefficients (13 features) only in the frames containing the ROIs. These coefficients are derived by multiplying the amplitude of the spectrogram with a mel-scaled filterbank, then the logarithm is applied and finally the result is decorrelated using the discrete cosine transform.</p>
  <p xmlns="http://www.w3.org/1999/xhtml">However, each recording has a varying number of ROIs. In order to derive a useful description per recording this must have a fixed size so that it can be compared to the descriptions of other recordings. Therefore we follow the following steps:</p>
  <div list-type="alpha-lower" class="list" xmlns="http://www.w3.org/1999/xhtml">
   <div class="list-item">
    <p>The ROIs of all training and test recordings are pooled together, whitened and clustered using a widely accepted non-supervised clustering approach namely K-means clustering 
     <a rid="pone.0096936-Breiman1" ref-type="bibr" href="#pone.0096936-Breiman1">[22]</a>.
    </p>
   </div>
   <div class="list-item">
    <p>Each ROI belonging to the same recording is associated to the cluster mean from which it has the lowest distance.</p>
   </div>
   <div class="list-item">
    <p>From all ROIs belonging to each recording we make a histogram of occurrences of cluster indices. This clearly has a fixed dimensionality equal to the size of the K-means partition. This procedure resembles the bag-of-words analysis used in text processing that is adapted to our bag of spectral segments case as in 
     <a rid="pone.0096936-Briggs1" ref-type="bibr" href="#pone.0096936-Briggs1">[5]</a>.
    </p>
   </div>
  </div>
  <p xmlns="http://www.w3.org/1999/xhtml">We have found the use of multiple code-books over the single codebook approach beneficial. A codebook with a small number of clusters will cluster using the gross-characteristic of the ROIs while gradual augmentation of the number of clusters leads to finer detail. We have been using 3 codebooks with 25, 15 and 70 clusters respectively resulting to a 687×25, 687×15, 687×70 training set which, after column stacking leads to a future set of 
   <span class="bold">S
    <sub>2</sub>
   </span>
   <span class="sup">687×110</span> for the training set.
  </p>
 </sec>
 <sec id="s2g" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Spectrographic Cross-correlation of ROIs</div>
  <p xmlns="http://www.w3.org/1999/xhtml">The final feature set 
   <span class="bold">S
    <sub>3</sub>
   </span> is the most powerful one and has originally appeared in 
   <a rid="pone.0096936-Fodor1" ref-type="bibr" href="#pone.0096936-Fodor1">[19]</a>. The key idea is to span the acoustic space of all recordings with respect to the swarm of extracted ROIs. That is, the swarm of the ROIs extracted from the training set scans the recordings and the highest normalized cross-correlation achieved from each ROI serves as a feature.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">The Normalized 2D Cross-correlation of ROIs is calculated as follows:
   <div id="pone.0096936.e004" class="disp-formula">
    <div xlink:href="pone.0096936.e004.jpg" position="anchor" orientation="portrait" class="graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
    <span class="label">(4)</span>
   </div>
   <span class="italic">f</span> is the image.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">
   <div class="inline-formula">
    <span xlink:href="pone.0096936.e005.jpg" class="inline-graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
   </div> is the mean of the template.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">
   <div class="inline-formula">
    <span xlink:href="pone.0096936.e006.jpg" class="inline-graphic" xmlns:xlink="http://www.w3.org/1999/xlink"/>
   </div> is the mean of 
   <span class="italic">f</span> (x, y) in the region under the template.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">This feature set alone surpasses all others combined. We attribute its success to the following facts:</p>
  <div list-type="alpha-lower" class="list" xmlns="http://www.w3.org/1999/xhtml">
   <div class="list-item">
    <p>Each ROI scans each spectrogram of the test set in the bandwidth limits of the ROI (enlarged±4 pixels across frequency to account for variability among individuals of the same species). Frequency constrained scanning is important since many species can have similar shapes of calls and what makes them different is their exact location in frequency.</p>
   </div>
   <div class="list-item">
    <p>Lets take the case of a targeted call that must be detected in a highly degraded environment (say the Bush Cricket in 
     <a ref-type="fig" rid="pone-0096936-g001" href="#pone-0096936-g001">Fig. 1</a>). If this degradation does not take place exactly on the time-frequency patch of the targeted call (this is indeed the case of airplane noise in 
     <a ref-type="fig" rid="pone-0096936-g001" href="#pone-0096936-g001">Fig. 1</a>) this approach will locate this call even if the background noise is orders of magnitude higher. They will locate the target call because ROIs can move in a 2D search space while seeking similar patches and they can ignore everything else (including all sorts of interferences) since the path they search is constrained by the permissible frequency bounds of each ROI.
    </p>
   </div>
   <div class="list-item">
    <p>Spectral segments are treated as wholes and do not face the problem of frame-wise processing of audio signals as GMMs/HMMs typically do.</p>
   </div>
   <div class="list-item">
    <p>The set holding all ROIs derived from the training set hold different manifestations of the same call either because it is a repetition or because it comes from another individual or is a call/song from a different distance and location. Therefore, a target call/song can be tracked even if the bird has a complex repertoire (in such case the ROIs will be from different parts of the spectrum) or its call shows variations due to distance and reflections.</p>
   </div>
  </div>
  <p xmlns="http://www.w3.org/1999/xhtml">Since the extraction of ROIs is automatic and unsupervised spectral patches corresponding to noise will be included in the swarm of ROIs (see segments 2 and 16 in 
   <a ref-type="fig" rid="pone-0096936-g004" href="#pone-0096936-g004">Fig. 4</a>). In fact there will be a large number of them. The way to deal with them is to apply a selection of features during building the trees of the random forest. By monitoring the out-of-bag (oob) error and rejecting these features whose replacement does not contribute to this error we discard almost the 2/3 of the total ROIs that are really valuable for classification. The reason why most of the noise ROIs are automatically discarded is that noise patches appear spuriously and therefore do not have descriptive capability. Noise types that are not spurious such as a siren or rain are pooled in the noise class that is the 0 class in our 87 class problem and discarded during recognition as the 0 class is deleted.
  </p>
  <p xmlns="http://www.w3.org/1999/xhtml">One should note that detection results based on a simple cross-correlation search of a template or even multiple templates on the spectrogram return poor classification results when applied to wildlife recordings. The present approach is successful because it models the way in which the total set of ROIs fits an unknown recording and therefore integrates votes coming from all available ROIs.</p>
  <p xmlns="http://www.w3.org/1999/xhtml">A strong similarity is to be observed between our work and 
   <a rid="pone.0096936-Lasseck1" ref-type="bibr" href="#pone.0096936-Lasseck1">[20]</a>, a work developed simultaneously to but independently from ours. The difference in 
   <a rid="pone.0096936-Lasseck1" ref-type="bibr" href="#pone.0096936-Lasseck1">[20]</a> and the proposed work is that the author presents the idea of median clipping per frequency band and time frame that removes most of the background noise. The spectrum is enhanced which results into deriving a much lower number of ROIs. Moreover he does not apply a one vs. all multi-label framework but rather isolates the ROIs that belong to each species as in 
   <a rid="pone.0096936-Fodor1" ref-type="bibr" href="#pone.0096936-Fodor1">[19]</a>.
  </p>
 </sec>
</sec>
