<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-archivearticle1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4020809</article-id><article-id pub-id-type="pmid">24826989</article-id><article-id pub-id-type="publisher-id">PONE-D-13-51856</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0096936</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and Life Sciences</subject><subj-group><subject>Biotechnology</subject><subj-group><subject>Bioengineering</subject><subj-group><subject>Biological Systems Engineering</subject></subj-group></subj-group></subj-group><subj-group><subject>Ecology</subject><subj-group><subject>Industrial Ecology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and Information Sciences</subject><subj-group><subject>Computer Applications</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Earth Sciences</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Ecology and Environmental Sciences</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Engineering and Technology</subject><subj-group><subject>Environmental Engineering</subject></subj-group><subj-group><subject>Signal Processing</subject><subj-group><subject>Audio Signal Processing</subject><subject>Signal Filtering</subject><subject>Statistical Signal Processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physical Sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied Mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group></article-categories><title-group><article-title>Automatic Classification of a Taxon-Rich Community Recorded in the Wild</article-title><alt-title alt-title-type="running-head">Automatic Classification of a Taxon-Rich Community</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Potamitis</surname><given-names>Ilyas</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1">
<sup>*</sup>
</xref></contrib></contrib-group><aff id="aff1">
<addr-line>Technological Educational Institute of Crete, Department of Music Technology and Acoustics, Crete, Greece</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>Pavan</surname><given-names>Gianni</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>University of Pavia, Italy</addr-line>
</aff><author-notes><corresp id="cor1">* E-mail: <email>potamitis@staff.teicrete.gr</email></corresp><fn fn-type="COI-statement"><p><bold>Competing Interests: </bold>The author has declared that no competing interests exist.</p></fn><fn fn-type="con"><p>Performed the experiments: IP. Analyzed the data: IP. Contributed reagents/materials/analysis tools: IP. Wrote the paper: IP.</p></fn></author-notes><pub-date pub-type="collection"><year>2014</year></pub-date><pub-date pub-type="epub"><day>14</day><month>5</month><year>2014</year></pub-date><volume>9</volume><issue>5</issue><elocation-id>e96936</elocation-id><history><date date-type="received"><day>9</day><month>12</month><year>2013</year></date><date date-type="accepted"><day>21</day><month>3</month><year>2014</year></date></history><permissions><copyright-statement>&#x000a9; 2014 Ilyas Potamitis</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Ilyas Potamitis</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><abstract><p>There is a rich literature on automatic species identification of a specific target taxon as regards various vocalizing animals. Research usually is restricted to specific species &#x02013; in most cases a single one. It is only very recently that the number of monitored species has started to increase for certain habitats involving birds. Automatic acoustic monitoring has not yet been proven to be generic enough to scale to other taxa and habitats than the ones described in the original research. Although attracting much attention, the acoustic monitoring procedure is neither well established yet nor universally adopted as a biodiversity monitoring tool. Recently, the multi-instance multi-label framework on bird vocalizations has been introduced to face the obstacle of simultaneously vocalizing birds of different species. We build on this framework to integrate novel, image-based heterogeneous features designed to capture different aspects of the spectrum. We applied our approach to a taxon-rich habitat that included 78 birds, 8 insect species and 1 amphibian. This dataset constituted the Multi-label Bird Species Classification Challenge-NIPS 2013 where the proposed approach achieved an average accuracy of 91.25% on unseen data.</p></abstract><funding-group><funding-statement>This work was funded by Project AMIBIO. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="11"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>There are many questions that scientists are called to address regarding the state of knowledge of global biodiversity. For all taxonomic groups only a percentage of 10&#x02013;20% is known and logged. Even as regards known groups, their population, distribution and dynamic changes are mostly unknown to us. The urgent problem of grave importance is to be able to assess when an ecosystem reaches a degraded state to the point of irreversibility and answer such questions as &#x02018;What is the state of the habitat in order to design policies and take conservation action to achieve sustainable use of the environment?&#x02019; or &#x02018;How is climate change related to rates of species loss and migration?&#x02019;.</p><p>Biodiversity monitoring provides the essential information on which conservation action is based. The monitoring process is typically carried out by qualified humans who observe and write down notes while taking logs of instruments that concentrate on the particular observation site. While the ability of a qualified scientist can be unparalleled to any kind of machine, there are also some limitations in human monitoring. The ability and the qualifications of the observers vary and this introduces a bias <xref rid="pone.0096936-Fitzpatrick1" ref-type="bibr">[1]</xref> in the total assessment. In addition, human expeditions are costly and can cover a limited number of sites and only work for a limited time. Moreover, the monitoring process can become more limited or even dangerous in remote, inaccessible areas. Besides, it can also be obtrusive on the species of observation.</p><p>Automatic acoustic monitoring of biodiversity is a means to provide information on species diversity with a view to the ones that are endemic, in a threatened or endangered status, or have special importance serving as indicator species. Moreover, each species is unique of its kind but can also have an additional importance due to its social, scientific, cultural or economic role.</p><p>Acoustic monitoring is limited to the part of the fauna that emits sound (birds, certain insects, certain amphibians, bats etc), which is only a subsample of the total biodiversity. However, inferring the correct taxa of this fauna and their densities can also give us an indirect cue for non-emitting sound species through their inter-dependence in the life-cycle, since animal populations are correlated.</p><p>Acoustic monitoring of biodiversity becomes, with time, a more attractive approach, as autonomous recording units (ARUs) become more affordable, can be power sufficient and can transmit their data through GSM or satellite connections from remote areas straight to the laboratory that can be located as far as in another continent. Therefore ARUs will eventually allow gathering of observations at larger spatial and time scales.</p><p>The challenge that comes along with such approach is the processing and logging of the deluge of data coming from the network of these sensors. Considering that ARUs can be operated in 24/7 modus and that several recorders can be used simultaneously, huge amounts of audio data can be gathered in relatively short periods of time. It is hardly feasible for human experts to listen to or visually inspect the complete sample of recordings. Thus semi-automatic processing of the sound files is a prerequisite for analysing the information within reasonable time limits.</p><p>The possible applications of acoustic monitoring in terrestrial environments using microphone arrays are thoroughly presented in <xref rid="pone.0096936-Blumstein1" ref-type="bibr">[2]</xref>. Acoustic entropy indices <xref rid="pone.0096936-Sueur1" ref-type="bibr">[3]</xref>&#x02013;<xref rid="pone.0096936-Riede1" ref-type="bibr">[4]</xref> are also proposed as a means to assess the type of the habitat and the extent of human intervention.</p><p>There is a rich literature on automatic species identification of specific target taxon like birds <xref rid="pone.0096936-Briggs1" ref-type="bibr">[5]</xref>&#x02013;<xref rid="pone.0096936-Potamitis1" ref-type="bibr">[7]</xref>, amphibians <xref rid="pone.0096936-Slimani1" ref-type="bibr">[8]</xref>, cetaceans <xref rid="pone.0096936-Halkias1" ref-type="bibr">[9]</xref>, insects <xref rid="pone.0096936-Riede2" ref-type="bibr">[10]</xref>, fish <xref rid="pone.0096936-Kottege1" ref-type="bibr">[11]</xref>, bats <xref rid="pone.0096936-Skowronski1" ref-type="bibr">[12]</xref> and mice <xref rid="pone.0096936-Grimsley1" ref-type="bibr">[13]</xref>. All these approaches are valuable as they pave the way for automatic analysis of animal vocalisations. However, they have not yet proven generic enough to scale to taxa and habitats other than the ones described in their original publication. Moreover, the misses and false alarms of these methods will cause a very large number of cases to be further investigated manually for a system that works on a 24/7 basis. Clearly the classification scores must face the high level of false positive and false negative results in order to make acoustic monitoring practical.</p><p>The present work is an attempt to shed some light on the possibility of monitoring taxon-rich communities recorded in the wild. It is our belief, shared by others, that in order to have solid progress in this research field, researchers should depart from using private data and focus on unprocessed data as typically recorded in nature. This paper introduces our approach that took part in a taxon-rich classification challenge organized by NIPS 2013 conference for bioacoustics <xref rid="pone.0096936-Glotin1" ref-type="bibr">[14]</xref> that involves 87 species (78 birds, 8 insects and 1 amphibian) in unprocessed wild life recordings. Competition challenges are very useful as a means to put theories into practice and assess what is the state of the art, as all competing approaches are bound by a common corpus; moreover, the assessment is guaranteed not to be biased towards a private approach, as the organizers of the challenge have the responsibility of assessing the approaches and base their final ranking on data that are unknown to the contestants <xref rid="pone.0096936-Glotin2" ref-type="bibr">[15]</xref>.</p><p>We have examined all reported approaches to bioacoustic challenges <xref rid="pone.0096936-Glotin2" ref-type="bibr">[15]</xref>, <xref rid="pone.0096936-Bas1" ref-type="bibr">[18]</xref>, <xref rid="pone.0096936-Briggs2" ref-type="bibr">[23]</xref> to find structural parallels among the best performing approaches, gather candidate functional sub-components, integrate our novel features, and finally shape our personal strategy towards the task of classifying large taxa of animal vocalisation in unprocessed field recordings. Our research concluded into three key observations that we will thoroughly analyse in the following sections:</p><list list-type="alpha-lower"><list-item><p>The treatment of sound as picture through the spectrogram as well as the application of image-based transformations to identify local properties of the target spectral blobs can have certain advantages over frame-based audio classification approaches to the task of species identification in noisy, real environments. This work belongs to the research trend the treats audio as a picture through its spectrogram <xref rid="pone.0096936-Briggs1" ref-type="bibr">[5]</xref>, <xref rid="pone.0096936-Halkias1" ref-type="bibr">[9]</xref>, <xref rid="pone.0096936-Brandes1" ref-type="bibr">[16]</xref>, <xref rid="pone.0096936-Lundy1" ref-type="bibr">[17]</xref>.</p></list-item><list-item><p>Multi-instance, multi-label approaches have an indisputable advantage over single- label approaches in species classification tasks <xref rid="pone.0096936-Briggs1" ref-type="bibr">[5]</xref>.</p></list-item><list-item><p>The synthesis of heterogeneous features is beneficial as it can provide complementary information that can be naturally integrated into the framework of decision trees in random forest ensembles.</p></list-item></list><p>This paper is organized as follows: In Section entitled &#x02018;Methods&#x02019; we present: a) the database and its main difficulties as regards species recognition, b) the feature extraction and classifier from the perspective of image analysis of spectrograms, c) the single and multi-label approaches as well as the benefits of the latter. In Section entitled &#x02018;Results&#x02019; we analyse the fine-tuning of the pattern recognition methods, perform experiments with real-field data and analyse the results of the current work. A discussion of the results concludes this work by presenting possible extensions and summarizing the implications of the results.</p></sec><sec sec-type="methods" id="s2"><title>Methods</title><sec id="s2a"><title>The Database and its Specificities</title><p>The NIPS 2013 database is composed of 1687 field-recordings each containing vocalisations of 0&#x02013;6 different species. A subset of 687 recordings is offered as a training set accompanied with known annotation from bioacoustics experts and we seek the species existing in the other 1000 recordings. The hardware used to collect the data was a number of ARUs placed at different locations of the French Provence and provided by the BIOTOPE society (see <ext-link ext-link-type="uri" xlink:href="http://www.biotope.fr/for">http://www.biotope.fr/for</ext-link> details). All recordings are monophonic, sampled at 44100 Hz with variable duration from 0.25 up to 5.75 secs. We did not downsample the original recordings as there were insect species singing up to 20 kHz and night birds calling as low as at 500 Hz.</p><p>The training set matches the test set conditions. The list of animals can be found in <xref rid="pone.0096936-Glotin1" ref-type="bibr">[14]</xref> and contains the most encountered species in central Europe. Another unique aspect of the NIPS dataset is the distinction between calls and songs of the same species that are treated as distinct categories that the algorithms should resolve. The recordings are selected in such a way that different kinds of difficulties are encountered in almost every recording, namely:</p><list list-type="alpha-lower"><list-item><p>Different bird species can and often do overlap in the same time-frame. Vocalizations can also overlap in frequency but, do not generally overlap both in time and frequency simultaneously. This can be attributed to the sparsity of the time-frequency domain but also to the observed fact that co-existing species make use of different bandwidths in order to communicate efficiently.</p></list-item><list-item><p>Different gains of signals due to distance: as many vocalizing animals can be at any distance from the microphone, often weak calls are picked up along with a dominating vocalization (see <xref ref-type="fig" rid="pone-0096936-g001">Fig. 1</xref>).</p></list-item><list-item><p>Anthropogenic noises such as airplanes, sirens, footsteps, human speech in biotopes near urban territories can lead any classifier to error, because they either obscure the target signal causing a miss or become themselves classified as the target signal (false alarm).</p></list-item><list-item><p>Abiotic sounds due to heavy wind, rain, and recording device failures can substantially reduce the quality of recordings. The sound of rain-drops and recording device failures produce sound events that are short-time with broadband spectral characteristics and can be misled with insect stridulations (see <xref ref-type="fig" rid="pone-0096936-g002">Fig. 2</xref>).</p></list-item></list><fig id="pone-0096936-g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.g001</object-id><label>Figure 1</label><caption><title>3D spectrogram of a typical recording of 3 species (trainfile115 in NIPS20134B database) demonstrating the difficulties in recognition due to different gains in target signals and noise.</title><p>From 14&#x02013;19 kHz a Cicada, 8&#x02013;13 kHz Bush Cricket, 2.5&#x02013;3.8 Eurasian Blackcap, 0&#x02013;2 kHz strong airplane noise.</p></caption><graphic xlink:href="pone.0096936.g001"/></fig><fig id="pone-0096936-g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.g002</object-id><label>Figure 2</label><caption><title>Types of anthropogenic and abiotic interfering sounds.</title></caption><graphic xlink:href="pone.0096936.g002"/></fig></sec><sec id="s2b"><title>Signal Processing &#x00026; Pattern Recognition as Applied to Animal Vocalisations</title><sec id="s2b1"><title>Single- and multi-label approaches</title><p>Recognizers of animal vocalisations use prototypes of the target species extracted from examples of annotated training data. These recordings come out of large bioacoustics inventories (e.g. DORSA (<ext-link ext-link-type="uri" xlink:href="http://www.dorsa.de">http://www.dorsa.de</ext-link>), MacAulay Library (<ext-link ext-link-type="uri" xlink:href="http://macaulaylibrary.org/index.do">http://macaulaylibrary.org/index.do</ext-link>), Tierstimmenarchivb (<ext-link ext-link-type="uri" xlink:href="http://www.tierstimmenarchiv.de">http://www.tierstimmenarchiv.de</ext-link>), and Xeno-canto (<ext-link ext-link-type="uri" xlink:href="http://www.xeno-canto.org">http://www.xeno-canto.org</ext-link>) or from private corpora.</p><p>These inventories, at the time of their compilation did not follow a common protocol on how to log the target species. Some contain just a clip of the target species, others contain the target species in the presence of several others or in the presence of noise or they are even human narrated. The vast majority of reported research on classifying animal vocalisations follow the single label paradigm (a recording holds a single species). The reference library of a single instance recognizer should contain a large number of only the target species and nothing else, as the recognizer will learn the sound that is unrelated to the target species, which in the end will increase its false alarms. The single label approach comes with a major drawback. Not many people are qualified to recognize species efficiently by sound. In the single-instance case, the construction of a reference library is a quite laborious task as the expert must screen all recordings and apply delicate time-frequency cleaning to any possible interference. Most experts are unwilling to undertake this task for a large number of species and recordings. Again, single-instance recognizers are vulnerable to false positives, i.e. species that are not the target but sound like it and are included by mistake in the reference library. The Multi-label approach on bird recognition is in our opinion a breakthrough and was originally proposed for the task of bird recognition in <xref rid="pone.0096936-Briggs1" ref-type="bibr">[5]</xref>. In this approach the expert does not need to clean or time-stamp the recordings at all.</p><p>The expert just writes down the acoustic classes that appear in a recording e.g. European Robin, Common Chaffinch, without any particular order or time stamp. That is, he/she has to select the set of labels attributed to the recording without having to clean, remove, isolate or timestamp parts of the recording. Each recording in the NIPS 20134B database has vocalisations from a varying number of species and the training corpus is annotated to the species level. We transformed the annotation data of all recordings to a binary matrix of a dimension 687&#x000d7;88 (687 training files, 87 for the species and 1 for noise only recordings as seen in <xref rid="pone-0096936-t001" ref-type="table">Table 1</xref>). Note that any all noise types as well as recording void of vocalisations can be pooled to a general noise class.</p><table-wrap id="pone-0096936-t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.t001</object-id><label>Table 1</label><caption><title>Annotation sample of the training data under the multi-label framework.</title></caption><alternatives><graphic id="pone-0096936-t001-1" xlink:href="pone.0096936.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">#File</td><td align="left" rowspan="1" colspan="1">Noise</td><td align="left" rowspan="1" colspan="1">Long-tailed Tit</td><td align="left" rowspan="1" colspan="1">&#x02026;</td><td align="left" rowspan="1" colspan="1">Turdus philomelos</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">001</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">1</td></tr><tr><td align="left" rowspan="1" colspan="1">002</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">0</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">1</td></tr></tbody></table></alternatives></table-wrap><p>The multi-label approach is responsible for marginalizing over the multiple labels and associate probabilities to species. This procedure accelerates the annotation of the human expert by far, as it alleviates the necessity of isolating the target signal. The expert, either by spectrographic analysis or by listening to the dubious cases, selects the appropriate tags and moves on to the next recording. Therefore, the multi-label approach relaxes by far the effort of constructing more complete reference databases.</p></sec></sec><sec id="s2c"><title>Pattern Matching</title><p>The automatic classification of species by machine learning techniques has a common theme. The features extracted from the unknown recordings are compared to prototypes extracted from labelled reference data in order to find possible matches. The label of the best matching reference becomes the label of the unknown recording. The prototypes can be probabilistic descriptions as in GMMs, HMMs <xref rid="pone.0096936-Potamitis1" ref-type="bibr">[7]</xref>, <xref rid="pone.0096936-Skowronski1" ref-type="bibr">[12]</xref> or spectrographic patches serving as templates as in the general detection framework X-Bat (<ext-link ext-link-type="uri" xlink:href="http://www.birds.cornell.edu/brp/software/xbat-introduction">http://www.birds.cornell.edu/brp/software/xbat-introduction</ext-link>). The matching is based on calculating a distance between the target prototypes and the unknown recordings. Again the distance can be one suited against a probabilistic approach e.g. a likelihood score, a probability or a cross correlation score. The final decision on which species are to be found in an unknown recording comes after comparing the distance to a threshold.</p><p>Species classification approaches based on Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs) as typically applied to speech are related to animal vocalization classification quite well. In fact HMM&#x02019;s were among the first applied classifiers following their success in speech/speaker recognition. After a thorough experimentation on private dataset of annual recordings with sophisticated versions of these tools <xref rid="pone.0096936-Potamitis1" ref-type="bibr">[7]</xref> we discovered 2 major drawbacks of the HMM/GMM approach:</p><list list-type="alpha-lower"><list-item><p>Recordings in the wild can be very noisy due to their exposure to a large number of audio sources originating from all distances and directions, the number and identity of which cannot be known <italic>a-priori</italic>. The co-existence of the target vocalisation with other species and abiotic interferences is inefficiently treated by current approaches of audio signal enhancement and separation when the number and the nature of audio sources is unknown as when coming from an unconstrained environment. These audio sources often appear simultaneously with target vocalisations over a single time frame (see <xref ref-type="fig" rid="pone-0096936-g001">Fig. 1</xref> and <xref ref-type="fig" rid="pone-0096936-g003">Fig. 3</xref> for common examples). GMMs/HMMs model the features extracted from overlapping frame analysis of sound. An overlapping time-frame sound analysis will inevitably include in its spectrum some of these interferences as well as possibly and quite often the vocalisations of a number of species.</p></list-item><list-item><p>The GMMs/HMMs species detectors derive a probability per frame (target vs. everything else -the so called &#x02018;world model&#x02019;). In the case of applying this classifier to wild-life recordings the vocalizing species will change from season to season and therefore the world-model as well. A GMM/HMM detector produces erroneous results if it is not properly updated by re-training or adapting to new species. The update procedure requires an expert in birds&#x02019; vocalisations to be available to sort out which species change and are probable to be confused with the targeted ones. This problem is not obvious when one is analysing e.g. one month of data but is prevalent in annual data analyses.</p></list-item></list><fig id="pone-0096936-g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.g003</object-id><label>Figure 3</label><caption><title>Spectrogram corresponding to a recording with 3 partially overlapping bird species (trainfile005 in NIPS20134B database).</title><p>The lower part of the spectrum is coloured by the sound of running water and strong wind.</p></caption><graphic xlink:href="pone.0096936.g003"/></fig><p>Spectrographic patches serving as templates is also one of the very first approaches employed and still is used mostly as providing a proof of concept on manually selected example recordings <xref rid="pone.0096936-Keen1" ref-type="bibr">[25]</xref>. In unconstrained real life scenarios the variability between vocalisations of a single individual with limited repertoire not to mention variability among different individuals of the same species can be so large that templates are not capable to grasp the individuality of a target species. Moreover, spectrogram patches are vulnerable to noise and to competing species while the slightest spectral deformation can result to a large distance between the unknown vocalisation and the template.</p><p>The base classifier in our approach is a random forest under the multi-label formulation of one vs. all (the so &#x02013; called binary relevance approach <xref rid="pone.0096936-Briggs1" ref-type="bibr">[5]</xref>). Random forests are an ensemble learning method for classification that is based on constructing many decision trees at training time and outputting the class that is the mode of the classes to the end of individual trees <xref rid="pone.0096936-Breiman1" ref-type="bibr">[22]</xref>. Random forests are the machine learning technique of choice here as our approach is based on deriving multiple - heterogeneous sets of features that aim to grasp different aspects of the spectrogram picture. The final dimensionality of the feature set is much larger than the size of the training set. There are few classifiers that can deal with such large dimensionalities. The class of Support Vector Machines and Extra Randomised Trees <xref rid="pone.0096936-Hastie1" ref-type="bibr">[24]</xref> that can also deal with high dimensional features were also tried out but with inferior results.</p></sec><sec id="s2d"><title>Feature Extraction</title><sec id="s2d1"><title>The spectrogram as an image</title><p>In our work, we do not follow the framework of audio analysis but the framework of image analysis. Treating the audio scene as a picture means looking at the spectrogram as a canvas where the acoustic events appear as localised spectral blobs on a two-dimensional matrix (see <xref ref-type="fig" rid="pone-0096936-g003">Fig. 3</xref>). Once these spectral blobs are extracted, many feature extraction techniques can be applied exclusively on these spectral patches while ignoring the rest of the spectrum. In this section we describe how we extract the regions of interest (ROIs) from the spectrogram.</p><p>The recording is firstly amplitude normalized. Then morphological operations are applied on the image. These operation have as function to derive masks of spectral blobs by connecting regions of high amplitude that correspond to calls or phrases and to eliminate small regions of high amplitude that cannot belong to animal vocalizations because they are too small. Several approaches have been tried as: removing from the image a blurred version of the same image as well as removing the morphological opening of the image. First a morphological opening on an image is applied that can remove small bright spots. Opening is defined as an erosion followed by a dilation. Erosion shrinks bright regions and enlarges dark regions. Dilation has the opposite effect of erosion and enlarges bright regions and shrinks dark regions. The border segments are dropped as the lower part correspond almost always to low-pass noise spectral patches. All these different are standard approaches in image processing having as a result to remove background illumination and extract spectral blobs <xref rid="pone.0096936-Shih1" ref-type="bibr">[21]</xref>.</p><p>As an example, in the sample image (see <xref ref-type="fig" rid="pone-0096936-g003">Fig. 3</xref>), the background illumination is less bright at the bottom of the image than at the centre or top. This is due to the sound of nearby running water and wind that corrupt the low frequencies of the spectrum. After the morphological operations are applied the picture of the spectrogram is made binary in order to mark the masks of the spectral blobs.</p><p>Binarization is realised by thresholding the image to the 90% percentile of the data (i.e. that is the highest 90% value). Subsequently we label the connected components in the 2-D binary image to derive the masks where the ROIs exist. We call ROIs the spectral patches cropped by the associated masks. Small masks (smaller than a fixed number of pixels) are discarded (see <xref ref-type="fig" rid="pone-0096936-g004">Fig. 4</xref>). The threshold for removing small masks is derived by observing short but perceptible calls from the training data and the number of pixels is set to 100. The ROIs are the patches from the original spectrogram that correspond to the pixel coordinates of the masks. All the ROIs extracted from the training set and test set are stored along with the frequency location from which they were cropped. The time information is dropped as an animal can vocalise at any time within a recording but only at the frequencies of its repertoire (see <xref ref-type="fig" rid="pone-0096936-g005">Fig. 5</xref> for the segments extracted from the recording of <xref ref-type="fig" rid="pone-0096936-g003">Fig. 3</xref>).</p><fig id="pone-0096936-g004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.g004</object-id><label>Figure 4</label><caption><title>Detected spectrogram blobs of <xref ref-type="fig" rid="pone-0096936-g003">Fig. 3</xref>.</title><p>Derivations and enumeration of the masks. Axis are enumerated according to their pixel index.</p></caption><graphic xlink:href="pone.0096936.g004"/></fig><fig id="pone-0096936-g005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.g005</object-id><label>Figure 5</label><caption><title>ROIs extracted after applying the masks of Fig. 4 onto the spectrogram of <xref ref-type="fig" rid="pone-0096936-g003">Fig. 3</xref>, enumerated and catalogued.</title><p>The same procedure is followed for all recordings.</p></caption><graphic xlink:href="pone.0096936.g005"/></fig><p>
<xref ref-type="fig" rid="pone-0096936-g005">Fig. 5</xref> illustrates the ROI&#x02019;s extracted from a single recording. The 16798 ROIs of all 1687 recordings are automatically extracted, enumerated and catalogued in under 12 minutes using an i7, 3.4 GHz machine. This is a distinct difference from the seminal approach of <xref rid="pone.0096936-Briggs1" ref-type="bibr">[5]</xref> where the ROIs are manually extracted, which is very time-consuming and practically impossible when the number of recordings is large. Cataloguing entails storing the spectral patches and the frequency borders in pixel coordinates from where it was extracted. Everything else but the ROIs is discarded from the data.</p><p>The benefits of extracting the ROIs are:</p><list list-type="alpha-lower"><list-item><p>The great reduction of variability between recordings. Every further analysis of our database will be done in reference to these ROIs alone while the rest of the spectrum is disregarded (compare <xref ref-type="fig" rid="pone-0096936-g005">Fig. 5</xref> to the original <xref ref-type="fig" rid="pone-0096936-g003">Fig. 3</xref>).</p></list-item><list-item><p>Once extracted these ROIs allow us to derive a plethora of features with gradual increase in sophistication namely: Marginal over time measurements, statistical descriptors of the shape of the ROIs and finally how the ROIs of the training set alone correlate to the spectrograms of the test set.</p></list-item></list></sec></sec><sec id="s2e"><title>Extraction of Marginal Over Time Measurements</title><p>Spectrogram reconstruction from ROIs entails that an enhanced spectrogram is extracted from the original one by imposing the ROIs of the original spectrum on an empty spectrogram (see <xref ref-type="fig" rid="pone-0096936-g004">Fig. 4</xref>). Each spectrogram is partitioned in 16 equal and non-overlapping horizontal bands. Subsequently several statistics are derived over this enhanced spectrum on a per band basis. This approach resembles filterbank analysis in audio. We do not apply mel-spacing as different birds and insects can vocalize anywhere in the spectrum. This approach allows us to make use of the band-limited character of most species and leave the marginalization process to the multi-label approach. For each band a set of basic descriptive statistics to represent the essence of each band concisely is derived from the intensities of each band, namely: Mean, standard deviation, median and kurtosis and the 90% percentile. The median is found by sorting the values and picking the middle one and is a way to summarize statistical distribution. Kurtosis (fourth central moment) is used as a measure of peaky vs. flat. In order to derive the 90% percentiles the data are ordered and the one at the highest 90% position is derived. A percentile is a statistical measure below which a given percentage of measurements in a group of measurements fall. Other descriptive statistics have been tried, such as entropy and skewness, but without presenting any statistically significant difference in the classification accuracy. Therefore, 5 descriptors for each of the 16 bands is derived resulting to a 5&#x000d7;16&#x0200a;=&#x0200a;80 dimensional vector per recording and a total training matrix <bold>S<sub>1</sub></bold>
<sup>687&#x000d7;80</sup>. One should note that with this feature alone the multi-label approach achieves 86.37% average accuracy (see Results section).</p></sec><sec id="s2f"><title>Bag of Segments, Statistics of Segments and Coding Over Multiple Codebooks</title><p>
<bold>S<sub>1</sub></bold> offers a gross description of the spectrum. The <bold>S<sub>1</sub></bold> features set can capture quite well species vocalising in different frequency bands but it fails to capture species vocalising in the same band. Another set of features, named <bold>S<sub>2</sub></bold> is extracted out of the examination of the morphology of each ROI. Each recording is considered as a bag of segments where the bag is the set composed of all ROIs in a recording and each ROI is an instance. For the ROIs corresponding to each recording we derive the following features:</p><p>Area of the binary mask noted as <italic>M<sub>area</sub></italic> of the mask <italic>M</italic>:<disp-formula id="pone.0096936.e001"><graphic xlink:href="pone.0096936.e001.jpg" position="anchor" orientation="portrait"/><label>(1)</label></disp-formula>where <italic>t, f</italic> are pixel indices belonging to the area of the mask.</p><p>Mean off the spectral patch corresponding to a ROI:<disp-formula id="pone.0096936.e002"><graphic xlink:href="pone.0096936.e002.jpg" position="anchor" orientation="portrait"/><label>(2)</label></disp-formula>where S<italic><sub>t,f</sub></italic> corresponds to the amplitude of the spectral chunk of the underlying binary mask <italic>M<sub>t,f</sub>.</italic>
</p><p>Standard deviation of the spectral ROI:<disp-formula id="pone.0096936.e003"><graphic xlink:href="pone.0096936.e003.jpg" position="anchor" orientation="portrait"/><label>(3)</label></disp-formula>
</p><p>The Median, Minimum and Maximum value of the spectral patch defined by ROIs and the Maximum position of the spectral ROIs (only the frequency index retained) as well as bandwidth and duration of each ROI are set as features. Additionally, we calculate the mel-frequency cepstral coefficients (13 features) only in the frames containing the ROIs. These coefficients are derived by multiplying the amplitude of the spectrogram with a mel-scaled filterbank, then the logarithm is applied and finally the result is decorrelated using the discrete cosine transform.</p><p>However, each recording has a varying number of ROIs. In order to derive a useful description per recording this must have a fixed size so that it can be compared to the descriptions of other recordings. Therefore we follow the following steps:</p><list list-type="alpha-lower"><list-item><p>The ROIs of all training and test recordings are pooled together, whitened and clustered using a widely accepted non-supervised clustering approach namely K-means clustering <xref rid="pone.0096936-Breiman1" ref-type="bibr">[22]</xref>.</p></list-item><list-item><p>Each ROI belonging to the same recording is associated to the cluster mean from which it has the lowest distance.</p></list-item><list-item><p>From all ROIs belonging to each recording we make a histogram of occurrences of cluster indices. This clearly has a fixed dimensionality equal to the size of the K-means partition. This procedure resembles the bag-of-words analysis used in text processing that is adapted to our bag of spectral segments case as in <xref rid="pone.0096936-Briggs1" ref-type="bibr">[5]</xref>.</p></list-item></list><p>We have found the use of multiple code-books over the single codebook approach beneficial. A codebook with a small number of clusters will cluster using the gross-characteristic of the ROIs while gradual augmentation of the number of clusters leads to finer detail. We have been using 3 codebooks with 25, 15 and 70 clusters respectively resulting to a 687&#x000d7;25, 687&#x000d7;15, 687&#x000d7;70 training set which, after column stacking leads to a future set of <bold>S<sub>2</sub></bold>
<sup>687&#x000d7;110</sup> for the training set.</p></sec><sec id="s2g"><title>Spectrographic Cross-correlation of ROIs</title><p>The final feature set <bold>S<sub>3</sub></bold> is the most powerful one and has originally appeared in <xref rid="pone.0096936-Fodor1" ref-type="bibr">[19]</xref>. The key idea is to span the acoustic space of all recordings with respect to the swarm of extracted ROIs. That is, the swarm of the ROIs extracted from the training set scans the recordings and the highest normalized cross-correlation achieved from each ROI serves as a feature.</p><p>The Normalized 2D Cross-correlation of ROIs is calculated as follows:<disp-formula id="pone.0096936.e004"><graphic xlink:href="pone.0096936.e004.jpg" position="anchor" orientation="portrait"/><label>(4)</label></disp-formula>
<italic>f</italic> is the image.</p><p>
<inline-formula><inline-graphic xlink:href="pone.0096936.e005.jpg"/></inline-formula> is the mean of the template.</p><p>
<inline-formula><inline-graphic xlink:href="pone.0096936.e006.jpg"/></inline-formula> is the mean of <italic>f</italic> (x, y) in the region under the template.</p><p>This feature set alone surpasses all others combined. We attribute its success to the following facts:</p><list list-type="alpha-lower"><list-item><p>Each ROI scans each spectrogram of the test set in the bandwidth limits of the ROI (enlarged&#x000b1;4 pixels across frequency to account for variability among individuals of the same species). Frequency constrained scanning is important since many species can have similar shapes of calls and what makes them different is their exact location in frequency.</p></list-item><list-item><p>Lets take the case of a targeted call that must be detected in a highly degraded environment (say the Bush Cricket in <xref ref-type="fig" rid="pone-0096936-g001">Fig. 1</xref>). If this degradation does not take place exactly on the time-frequency patch of the targeted call (this is indeed the case of airplane noise in <xref ref-type="fig" rid="pone-0096936-g001">Fig. 1</xref>) this approach will locate this call even if the background noise is orders of magnitude higher. They will locate the target call because ROIs can move in a 2D search space while seeking similar patches and they can ignore everything else (including all sorts of interferences) since the path they search is constrained by the permissible frequency bounds of each ROI.</p></list-item><list-item><p>Spectral segments are treated as wholes and do not face the problem of frame-wise processing of audio signals as GMMs/HMMs typically do.</p></list-item><list-item><p>The set holding all ROIs derived from the training set hold different manifestations of the same call either because it is a repetition or because it comes from another individual or is a call/song from a different distance and location. Therefore, a target call/song can be tracked even if the bird has a complex repertoire (in such case the ROIs will be from different parts of the spectrum) or its call shows variations due to distance and reflections.</p></list-item></list><p>Since the extraction of ROIs is automatic and unsupervised spectral patches corresponding to noise will be included in the swarm of ROIs (see segments 2 and 16 in <xref ref-type="fig" rid="pone-0096936-g004">Fig. 4</xref>). In fact there will be a large number of them. The way to deal with them is to apply a selection of features during building the trees of the random forest. By monitoring the out-of-bag (oob) error and rejecting these features whose replacement does not contribute to this error we discard almost the 2/3 of the total ROIs that are really valuable for classification. The reason why most of the noise ROIs are automatically discarded is that noise patches appear spuriously and therefore do not have descriptive capability. Noise types that are not spurious such as a siren or rain are pooled in the noise class that is the 0 class in our 87 class problem and discarded during recognition as the 0 class is deleted.</p><p>One should note that detection results based on a simple cross-correlation search of a template or even multiple templates on the spectrogram return poor classification results when applied to wildlife recordings. The present approach is successful because it models the way in which the total set of ROIs fits an unknown recording and therefore integrates votes coming from all available ROIs.</p><p>A strong similarity is to be observed between our work and <xref rid="pone.0096936-Lasseck1" ref-type="bibr">[20]</xref>, a work developed simultaneously to but independently from ours. The difference in <xref rid="pone.0096936-Lasseck1" ref-type="bibr">[20]</xref> and the proposed work is that the author presents the idea of median clipping per frequency band and time frame that removes most of the background noise. The spectrum is enhanced which results into deriving a much lower number of ROIs. Moreover he does not apply a one vs. all multi-label framework but rather isolates the ROIs that belong to each species as in <xref rid="pone.0096936-Fodor1" ref-type="bibr">[19]</xref>.</p></sec></sec><sec id="s3"><title>Results</title><sec id="s3a"><title>Recognition of Species</title><p>The features described in section 3 are all stacked with respect to columns forming a large training set of features <bold>S</bold>, where, <bold>S</bold>&#x0200a;=&#x0200a;[<bold>S<sub>1</sub></bold>|<bold>S<sub>2</sub></bold>|<bold>S<sub>3</sub></bold>] and <bold>S</bold>
<sup>687&#x000d7;16988</sup>. Therefore an initial random forest of 80 trees is built in order to use the out of bag error to select the best performing features and reduce the dimensionality of <bold>S</bold> to <bold>S</bold>
<sup>687&#x000d7;5669</sup>. Once the features are finalized by the selection procedure the final random forest of 250 trees is constructed by training on the <bold>S</bold>
<sup>687&#x000d7;5669</sup> and the associated binary multi-label matrix <bold>Y</bold>
<sup>687&#x000d7;88</sup> and then applied to the test data represented by the associated matrix <bold>T</bold>
<sup>1000&#x000d7;5669</sup>.</p><p>The parameters of the random forest were tuned using grid-search and 10-fold cross-validation. The number of trees was set to 250 using the entropy criterion with min_samples_split&#x0200a;=&#x0200a;4, min_samples_leaf&#x0200a;=&#x0200a;3. The predictions submitted are averaged over 10 random forests initialized from different random seeds. The evaluation metric used was the area under the Receiver&#x02019;s Operating Characteristic Curve (also known as Area Under the Curve or &#x02018;AUC&#x02019;).</p><p>Many feature combinations were tried out. We refer to the most important ones in <xref rid="pone-0096936-t002" ref-type="table">Table 2</xref>.</p><table-wrap id="pone-0096936-t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.t002</object-id><label>Table 2</label><caption><title>AUC public refers to accuracy based on the 1/3 of the test data and AUC public to the accuracy achieved using the remaining 2/3 of the data.</title></caption><alternatives><graphic id="pone-0096936-t002-2" xlink:href="pone.0096936.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1">AUC_Public</td><td align="left" rowspan="1" colspan="1">AUC_Private</td><td align="left" rowspan="1" colspan="1">Description</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">0.87836</td><td align="left" rowspan="1" colspan="1">0.86368</td><td align="left" rowspan="1" colspan="1">Global Features</td></tr><tr><td align="left" rowspan="1" colspan="1">0.87648</td><td align="left" rowspan="1" colspan="1">0.87424</td><td align="left" rowspan="1" colspan="1">Histogram of ROIs</td></tr><tr><td align="left" rowspan="1" colspan="1">0.87422</td><td align="left" rowspan="1" colspan="1">0.86843</td><td align="left" rowspan="1" colspan="1">Global Features + Histogram of ROIs</td></tr><tr><td align="left" rowspan="1" colspan="1">0.90776</td><td align="left" rowspan="1" colspan="1">0.91310</td><td align="left" rowspan="1" colspan="1">Cross-correlation of ROIs</td></tr><tr><td align="left" rowspan="1" colspan="1">0.91202</td><td align="left" rowspan="1" colspan="1">
<bold>0.91689</bold>
<xref ref-type="table-fn" rid="nt101">*</xref>
</td><td align="left" rowspan="1" colspan="1">Global Features + Histogram of ROIs + Cross-corr. of ROIs</td></tr><tr><td align="left" rowspan="1" colspan="1">0.91850</td><td align="left" rowspan="1" colspan="1">0.91752</td><td align="left" rowspan="1" colspan="1">Winning Entry</td></tr><tr><td align="left" rowspan="1" colspan="1">0.92251</td><td align="left" rowspan="1" colspan="1">0.91578</td><td align="left" rowspan="1" colspan="1">2<sup>nd</sup> Best solution</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>*0.91252 achieved during contest.</p></fn></table-wrap-foot></table-wrap><p>The experiments demonstrate that our approach accurately predicts a large set of species present in an unattended acoustic monitoring scenario. An example of recognizing a single audio scene is depicted in <xref ref-type="fig" rid="pone-0096936-g006">Fig. 6</xref> where we can see a typical recognition output of the recording in <xref ref-type="fig" rid="pone-0096936-g001">Fig. 1</xref>.</p><fig id="pone-0096936-g006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.g006</object-id><label>Figure 6</label><caption><title>Probability output for 87 classes of the recording in <xref ref-type="fig" rid="pone-0096936-g001">Fig. 1</xref>.</title><p>One can clearly discern 3 classes corresponding to the probability peaks in locations 53, 74, 81. From the file of NIPS20134B database annotations the locations 53, 74, 81 indeed correspond to Cicada, Bush Cricket and Eurasian Blackcap.</p></caption><graphic xlink:href="pone.0096936.g006"/></fig></sec><sec id="s3b"><title>Detector of Recordings Void of Biotic Sounds</title><p>A useful by-product of building a classifier of multiple species is to make a binary classifier that decides whether there is any animal sound in a recording. The binary classifier uses the same features as for species classification (i.e. <bold>S</bold>
<sup>687&#x000d7;16988</sup>) but employs only the first column of the labels (i.e. if there is animal vocalization in the recording or not represented by the matrix <bold>Y</bold>
<sup>687&#x000d7;1</sup>). This detector can be used to derive statistics concerning activity patterns of animal vocalisations vs abiotic or anthropogenic noise sources in general. This is useful as a statistical measure of the healthiness of a habitat (which is associated to species&#x02019; vocalisation activity) and also as a means to screen out recordings that do not include animal sounds. Recordings void of any animal vocalisation are common in long-term continuous acoustical monitoring of habitats and discarding empty recordings helps to reduce the time of further processing of events as well as the storage capacity required. The test set of 1000 recordings was inspected manually and categorized into 209 recordings void of biotic sounds and 791 with at least one call per animal. The machine learning technique we used after a small set of comparisons with randomized trees and random forests was a large Gradient Boosting Classifier <xref rid="pone.0096936-Breiman1" ref-type="bibr">[22]</xref>.The performance was 97.05% over a 10-fold cross-validation of the training set and 96.3% over the test set. The analytic results are depicted in <xref rid="pone-0096936-t003" ref-type="table">Table 3</xref>.</p><table-wrap id="pone-0096936-t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0096936.t003</object-id><label>Table 3</label><caption><title>A detector of animal vocalizations vs. records containing exclusively abiotic sounds.</title></caption><alternatives><graphic id="pone-0096936-t003-3" xlink:href="pone.0096936.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">precision</td><td align="left" rowspan="1" colspan="1">recall</td><td align="left" rowspan="1" colspan="1">f-score</td><td align="left" rowspan="1" colspan="1">Support</td></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<bold>Animal Voc.</bold>
</td><td align="left" rowspan="1" colspan="1">0.86</td><td align="left" rowspan="1" colspan="1">0.99</td><td align="left" rowspan="1" colspan="1">0.92</td><td align="left" rowspan="1" colspan="1">791</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Abiotic sounds</bold>
</td><td align="left" rowspan="1" colspan="1">0.94</td><td align="left" rowspan="1" colspan="1">0.38</td><td align="left" rowspan="1" colspan="1">0.54</td><td align="left" rowspan="1" colspan="1">209</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Avg/total</bold>
</td><td align="left" rowspan="1" colspan="1">0.88</td><td align="left" rowspan="1" colspan="1">0.86</td><td align="left" rowspan="1" colspan="1">0.84</td><td align="left" rowspan="1" colspan="1">1000</td></tr></tbody></table></alternatives></table-wrap></sec></sec><sec id="s4"><title>Discussion</title><p>The probabilistic framework of random forests is very suitable for animal vocalisations classification tasks. Assuming that there are <italic>N<sub>tr</sub></italic> annotated recordings available for training then a feature matrix <bold>S</bold>&#x0200a;=&#x0200a;<italic>N<sub>tr</sub></italic>x<italic>N<sub>ft</sub></italic> can be constructed where <italic>N<sub>ft</sub></italic> represents different kinds of features stacked by column corresponding to different perspectives on how to engineer features out of spectrograms. Random forests do not need feature normalization among these heterogeneous features and can deal with dimensionality of features that can reach many thousands of dimensions. Their embedded feature selection ability during their growing effectively removes features that are not useful enough for the classification task.</p><p>As regards the computational cost deriving all features from raw recordings but the cross-correlation of ROIs (that is <bold>S<sub>1</sub></bold> and <bold>S<sub>2</sub></bold>) takes 13 minute on an I7 3.4 GHz PC. The training on 687 recordings and recognition of 1000 recordings takes another 10 minutes. The calculation of <bold>S<sub>3</sub></bold> takes almost 1 day and the training and testing of the whole set about 20 minutes on the same machine. One should note that the employment of Graphical Processing Units (GPUs) is possible to reduce this computational effort to a couple of hours and we are currently working to this direction.</p><p>Another feature that in <xref rid="pone.0096936-Briggs2" ref-type="bibr">[23]</xref> was demonstrated to be highly beneficial and can potentially be integrated naturally in the statistical framework of random forests is the location of the recording device and the time-stamp of the recording. This kind of information can bias the classifier towards certain species. Location information can be inserted into ARUs during installation or through a Global Positioning System and this information can be passed automatically to the header of the sound files or to their filenames. The location of the ARUs can offer some information as regards the micro-biodiversity of the local habitat. Certain species are always found near lakes (e.g. the common kingfisher <italic>Alcedo Atthis</italic> or around the sea-coast etc.), while others only in rocky areas far away from lakes/coast. Species information provided by the expert can be combined with location information to provide the <italic>a-priori</italic> probability of a species being at this location. This probability is calculated as the frequency of a species appearing there to the total number of species tagged by the human expert during training. This probability matrix of all species serves as a feature that can be appended to the <bold>S</bold> matrix. The same matrix type can be calculated for the test recordings and the classifier is responsible to resolve the situation. Time information can be drawn from the internal clock of the ARU and also passed to the header of the recording or to the filename. Time information is also important as some birds show strong preference towards the time they call or sing (e.g. night-birds). The NIPS database did not include such information but; it is information often ignored while it can be obtained easily directly from the hardware of the ARUs. We deem that this information will be vital when the number of classes will reach many hundreds of species which however neither appear altogether in a certain location not they sing/call all day long.</p><p>One would then naturally ask: what are the main difficulties in recognizing taxon-rich communities in the wild using only the acoustic modality? If one could concentrate erroneous cases they would cluster due to the following reasons:</p><list list-type="alpha-lower"><list-item><p>Noise in low frequencies appears quite often and quite strong because wind is low-pass and many abiotic sounds (e.g. motors, planes etc.) are also of low-pass character and can seriously mask night-birds and other species that vocalise in very low frequencies.</p></list-item><list-item><p>Cicadas have a noise-like spectrum that resembles the spectrum of wind when they are at large distance from the recorder and their acoustic emissions are affected by reverberation. If one applies enhancement algorithms to reduce the noise and the ROIs that will be associated with noise then one can also wipe out insect species, especially when the gain of their signal is small. If one does not apply any kind of enhancement then one will face the problem of having a large number of ROIs due to noise. Cicadas can sing for the whole duration of recordings and therefore no restriction can be applied on the size of the ROIs as in other species.</p></list-item><list-item><p>Call repertoire of one species can be very similar at least partially to that of another to the point that even a trained ear may be in doubt (e.g., different taxa of <italic>Parus-</italic>tits or the notable case of the common kingfisher and the Dunnock - <italic>Prunella modularis</italic>). In such cases the human observer seeks other queues of information to resolve the situation (e.g. repetition patterns) that are currently not taken into account in this work.</p></list-item></list><p>This work assessed the potential of bioacoustic monitoring of a taxon-rich community. We have developed an approach that can monitor 78 bird species, 8 insects and 1 amphibian (a total number of 87 species under quite challenging environmental conditions). The classification accuracy was assessed by independent observers and found 91.252%.</p><p>We propose that our method is a contribution towards monitoring biodiversity at large scales through the recording of vocal fauna such as birds, insects, amphibians and mammals. We claim that the approach presented will be very robust when the training and operational data are matched and we plan to apply them at diverse environments such us underwater acoustics mainly for the recognition of cetaceans and also on bats. The classification of the immense number of recordings stemming from the deployment of a large number of ARUs will allow the following:</p><list list-type="alpha-lower"><list-item><p>Collection of relevant data to support decisions concerning the presence, absence and distribution of species.</p></list-item><list-item><p>Data analysis in fully automated fashion that can function in degraded audio environments.</p></list-item><list-item><p>Humans and their activities can be tracked in protected areas provided they leave an audio imprint.</p></list-item></list></sec></body><back><ack><p>The approach presented in this work integrates ideas from many contributors especially as encountered in competitions. We acknowledge that the idea of using multiple templates to represent a single vocalisation was first presented by Nick Kriedler in &#x02018;The Marine explore and Cornell University Whale Detection Challenge&#x02019;. The idea of multiple templates was further developed by Fodor Gabor from Budapest University of Technology and Economics in the MLSP 2013 bird recognition challenge <xref rid="pone.0096936-Fodor1" ref-type="bibr">[19]</xref>. Last, we acknowledge that all competitions took place in the data hosting platform of Kaggle (<ext-link ext-link-type="uri" xlink:href="http://www.kaggle.com/c/multilabel-bird-species-classification-nips2013">http://www.kaggle.com/c/multilabel-bird-species-classification-nips2013</ext-link>). The author wishes to thank the editor and the reviewers who kindly helped him to totally revise the manuscript.</p></ack><ref-list><title>References</title><ref id="pone.0096936-Fitzpatrick1"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Fitzpatrick</surname><given-names>M</given-names></name>, <name><surname>Preisser</surname><given-names>E</given-names></name>, <name><surname>Ellison</surname><given-names>A</given-names></name>, <name><surname>Elkinton</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Observer bias and the detection of low-density populations</article-title>. <source>Ecological Applications</source>
<volume>19</volume>: <fpage>1673</fpage>&#x02013;<lpage>1679</lpage>
<pub-id pub-id-type="doi">10.1890/09-0265.1</pub-id>.<pub-id pub-id-type="pmid">19831062</pub-id></mixed-citation></ref><ref id="pone.0096936-Blumstein1"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Blumstein</surname><given-names>D</given-names></name>, <name><surname>Mennill</surname><given-names>D</given-names></name>, <name><surname>Clemins</surname><given-names>P</given-names></name>, <name><surname>Girod</surname><given-names>L</given-names></name>, <name><surname>Yao</surname><given-names>K</given-names></name>, <name><surname>Patricelli</surname><given-names>G</given-names></name>, <etal>et al</etal> (<year>2011</year>) <article-title>Acoustic monitoring in terrestrial environments using microphone arrays: applications, technological considerations and prospectus</article-title>. <source>Journal of Applied Ecology</source>
<volume>48</volume>: <fpage>758</fpage>&#x02013;<lpage>767</lpage>.</mixed-citation></ref><ref id="pone.0096936-Sueur1"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Sueur</surname><given-names>J</given-names></name>, <name><surname>Pavoine</surname><given-names>S</given-names></name>, <name><surname>Hamerlynck</surname><given-names>O</given-names></name>, <name><surname>Duvail</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Rapid Acoustic Survey for Biodiversity Appraisal</article-title>. <source><italic toggle="yes">PLoS ONE</italic> 3</source>
<volume>(12)</volume>: <fpage>e4065</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0004065</pub-id>
</mixed-citation></ref><ref id="pone.0096936-Riede1"><label>4</label><mixed-citation publication-type="other">Riede K (1997) Bioacoustic monitoring of insect communities in a Bornean rainforest canopy. In: N.E. Stork, J. Adis, R.K. Didham (eds). Canopy Arthropods. London: Chapman &#x00026; Hall. pp. 442&#x02013;452.</mixed-citation></ref><ref id="pone.0096936-Briggs1"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Briggs</surname><given-names>F</given-names></name>, <name><surname>Lakshminarayanan</surname><given-names>B</given-names></name>, <name><surname>Neal</surname><given-names>L</given-names></name>, <name><surname>Fern</surname><given-names>X</given-names></name>, <name><surname>Raich</surname><given-names>R</given-names></name>, <etal>et al</etal> (<year>2012</year>) <article-title>Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach</article-title>. <source>The Journal of the Acoustical Society of America</source>
<volume>131</volume>: <fpage>4640</fpage>.<pub-id pub-id-type="pmid">22712937</pub-id></mixed-citation></ref><ref id="pone.0096936-Bardeli1"><label>6</label><mixed-citation publication-type="other">Bardeli D, Wolff F, Kurth M, Koch K, Tauchert, et al. (2009) Detecting bird sounds in a complex acoustic environment and application to bioacoustic monitoring. Pattern Recognition Letter: 31, 23, 1524&#x02013;1534.</mixed-citation></ref><ref id="pone.0096936-Potamitis1"><label>7</label><mixed-citation publication-type="other">Potamitis I, Ntalampiras S, Jahn O, Riede K (2014) Automatic bird sound detection in long real-field recordings: Applications and tools, Applied Acoustics, Volume 80, pp. 1&#x02013;9, ISSN 0003-682X, <pub-id pub-id-type="doi">10.1016/j.apacoust.2014.01.001</pub-id>.</mixed-citation></ref><ref id="pone.0096936-Slimani1"><label>8</label><mixed-citation publication-type="other">Slimani T, Beltr&#x000e1;n GJ, Mouden E, Radi M, Marquez R (2010) Recent discoveries and monitoring populations of the Moroccan Toad. (Alytesmaurus Pasteur &#x00026; Bons, 1962). Proceedings of the XI Congresso Luso-Espanhol de Herpetolog&#x000ed;a/Sevilha: 148.</mixed-citation></ref><ref id="pone.0096936-Halkias1"><label>9</label><mixed-citation publication-type="other">Halkias X, Glotin H (2013) Classification of Mysticete sounds using machine learning techniques. Journal of the Acoustical Society of America, Vol. 134 (5), pp. 3496, Nov. 2013.</mixed-citation></ref><ref id="pone.0096936-Riede2"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Riede</surname><given-names>K</given-names></name> (<year>1993</year>) <article-title>Monitoring biodiversity: analysis of Amazonian rainforest sounds</article-title>. <source><italic toggle="yes">Ambio</italic></source>
<volume>22</volume>: <fpage>546</fpage>&#x02013;<lpage>548</lpage>.</mixed-citation></ref><ref id="pone.0096936-Kottege1"><label>11</label><mixed-citation publication-type="other">Kottege N, Kroon F, Jurdak R, Jones D (2012) Classification of underwater broadband bio-acoustics using spectro-temporal features. WUWNet &#x02018;12 Proceedings of the Seventh ACM International Conference on Underwater Networks and Systems.</mixed-citation></ref><ref id="pone.0096936-Skowronski1"><label>12</label><mixed-citation publication-type="other">Skowronski M, Harris J (2006) Acoustic detection and classification of microchiroptera using machine learning: Lessons learned from automatic speech recognition. The Journal of the Acoustical Society of America 119: 1817, 2006.</mixed-citation></ref><ref id="pone.0096936-Grimsley1"><label>13</label><mixed-citation publication-type="other">Grimsley J, Gadziola M, Wenstrup J (2013). Automated classification of mouse pup isolation syllables: from cluster analysis to an Excel-based &#x0201c;mouse pup syllable classification calculator&#x0201d;. Frontiers in Behavioral Neuroscience. doi:10.3389/fnbeh.2012.00089.</mixed-citation></ref><ref id="pone.0096936-Glotin1"><label>14</label><mixed-citation publication-type="other">Glotin H, LeCun Y, Arti&#x000e8;res T, Mallat S, Tchernichovski O, <etal>et al</etal>. (2013) Proc. of &#x02018;Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data - NIP4B&#x02019;, joint to NIPS, <ext-link ext-link-type="uri" xlink:href="http://sabiod.org">http://sabiod.org</ext-link>, ISSN 979-10-90821-04-0.</mixed-citation></ref><ref id="pone.0096936-Glotin2"><label>15</label><mixed-citation publication-type="other">Glotin H, Clark C, LeCun Y, Dugan P, Halkias X, <etal>et al</etal>. (2013) Proc. of the &#x02018;1st international workshop on Machine Learning for Bioacoustics -ICML4B&#x02019;, joint to ICML, <ext-link ext-link-type="uri" xlink:href="http://sabiod.org">http://sabiod.org</ext-link>, ISSN 979-10-90821-02-6.</mixed-citation></ref><ref id="pone.0096936-Brandes1"><label>16</label><mixed-citation publication-type="other">Brandes T, Naskrecki P, Figueroa H (2006) Using image processing to detect and classify narrow-band cricket and frog calls, Journal of the Acoustical Society of America, pp. 2950&#x02013;2957, v.120.</mixed-citation></ref><ref id="pone.0096936-Lundy1"><label>17</label><mixed-citation publication-type="other">Lundy M, Teeling E, Boston E, Scott D, Buckley D, <etal>et al</etal>.. (2013) The shape of sound: elliptic Fourier descriptors (EFD) discriminate the echolocation calls of Myotis bast (M. daubentonii, M. nattereri and M. mystacinus), Bioacoustics, pp. 101&#x02013;115, 20, 2.</mixed-citation></ref><ref id="pone.0096936-Bas1"><label>18</label><mixed-citation publication-type="other">Bas Y, Dufour O, Glotin H (2013) &#x02018;Bird Challenge Overview&#x02019;, in Proc. of &#x02018;Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data- NIP4B&#x02019;, joint to NIPS Conf., ISSN 979-10-90821-04-0.</mixed-citation></ref><ref id="pone.0096936-Fodor1"><label>19</label><mixed-citation publication-type="other">Fodor G (2013) The Ninth Annual MLSP Competition: First place. Machine Learning for Signal Processing (MLSP), 2013 IEEE Int. Workshop on, Digital Object Identifier: 10.1109/MLSP.2013.6661932.</mixed-citation></ref><ref id="pone.0096936-Lasseck1"><label>20</label><mixed-citation publication-type="other">Lasseck M (2013) Bird Song Classification in Field Recordings: Winning Solution for NIPS4B 2013 Competition, Chap. in Proc. of &#x02018;Neural Information Processing Scaled for Bioacoustics: from Neurons to Big Data - NIP4B&#x02019;, joint to NIPS Conf., ISSN 979-10-90821-04-0.</mixed-citation></ref><ref id="pone.0096936-Shih1"><label>21</label><mixed-citation publication-type="other">Shih F (2009) Image Processing and Mathematical Morphology. Taylor &#x00026; Francis.</mixed-citation></ref><ref id="pone.0096936-Breiman1"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Breiman</surname><given-names>L</given-names></name> (<year>2001</year>) <article-title>Random Forests</article-title>. <source>Machine Learning</source>
<volume>45(1)</volume>: <fpage>5</fpage>&#x02013;<lpage>32</lpage>
<pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
</mixed-citation></ref><ref id="pone.0096936-Briggs2"><label>23</label><mixed-citation publication-type="other">Briggs F, Raich R, Eftaxias K, Lei Z, Huang Y (2013) The ninth annual MLSP competition: overview, IEEE International workshop on machine learning for signal processing, Southampton, United Kingdom., Sept. 22&#x02013;25.</mixed-citation></ref><ref id="pone.0096936-Hastie1"><label>24</label><mixed-citation publication-type="other">Hastie T, Tibshirani R, Friedman J (2001) Elements of Statistical Learning. Springer.</mixed-citation></ref><ref id="pone.0096936-Keen1"><label>25</label><mixed-citation publication-type="other">Keen S, Ross J, Griffiths E, Lanzone M, Farnsworth A (2014), A comparison of similarity-based approaches in the classification of flight calls of four species of North American wood-warblers (Parulidae), Ecological Informatics, ISSN 1574&#x02013;9541, <pub-id pub-id-type="doi">10.1016/j.ecoinf.2014.01.001</pub-id>.</mixed-citation></ref></ref-list></back></article>