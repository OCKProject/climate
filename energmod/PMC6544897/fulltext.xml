<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">J R Soc Interface</journal-id><journal-id journal-id-type="iso-abbrev">J R Soc Interface</journal-id><journal-id journal-id-type="publisher-id">RSIF</journal-id><journal-id journal-id-type="hwp">royinterface</journal-id><journal-title-group><journal-title>Journal of the Royal Society Interface</journal-title></journal-title-group><issn pub-type="ppub">1742-5689</issn><issn pub-type="epub">1742-5662</issn><publisher><publisher-name>The Royal Society</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6544897</article-id><article-id pub-id-type="pmid">31039693</article-id><article-id pub-id-type="doi">10.1098/rsif.2018.0344</article-id><article-id pub-id-type="publisher-id">rsif20180344</article-id><article-categories><subj-group subj-group-type="hwp-journal-coll"><subject>1004</subject><subject>44</subject><subject>181</subject><subject>22</subject></subj-group><subj-group subj-group-type="heading"><subject>Life Sciences&#x02013;Mathematics interface</subject></subj-group><subj-group subj-group-type="leader"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Excitatory versus inhibitory feedback in Bayesian formulations of scene construction</article-title><alt-title alt-title-type="short">Excitatory versus inhibitory feedback in Bayesian formulations of scene construction</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Abadi</surname><given-names>Alireza Khatoon</given-names></name><xref ref-type="aff" rid="af1">1</xref></contrib><contrib contrib-type="author"><name><surname>Yahya</surname><given-names>Keyvan</given-names></name><xref ref-type="aff" rid="af2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9253-5859</contrib-id><name><surname>Amini</surname><given-names>Massoud</given-names></name><xref ref-type="aff" rid="af1">1</xref></contrib><contrib contrib-type="author"><name><surname>Friston</surname><given-names>Karl</given-names></name><xref ref-type="aff" rid="af3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-3632-7569</contrib-id><name><surname>Heinke</surname><given-names>Dietmar</given-names></name><xref ref-type="aff" rid="af4">4</xref><xref ref-type="corresp" rid="cor1"/></contrib></contrib-group><aff id="af1"><label>1</label><institution>Department of Mathematics, Faculty of Mathematical Sciences, Tarbiat Modares University</institution>, <addr-line>Tehran 14115-134</addr-line>, <country>Iran</country></aff><aff id="af2"><label>2</label><institution>Faculty of Informatics, Chemnitz University of Technology</institution>, <addr-line>Stra&#x000df;e der Nationen 62, R. B216, 09111 Chemnitz</addr-line>, <country>Germany</country></aff><aff id="af3"><label>3</label><institution>Wellcome Trust Centre for Neuroimaging, Institute of Neurology, University College London</institution>, <addr-line>12 Queen Square, London WC1N 3BG</addr-line>, <country>UK</country></aff><aff id="af4"><label>4</label><institution>Centre for Computational Neuroscience and Cognitive Robotics, School of Psychology, University of Birmingham</institution>, <addr-line>Edgbaston, Birmingham B15 2TT</addr-line>, <country>UK</country></aff><author-notes><corresp id="cor1">e-mail: <email>d.g.heinke@bham.ac.uk</email></corresp></author-notes><pub-date pub-type="ppub"><month>5</month><year>2019</year></pub-date><pub-date pub-type="epub"><day>1</day><month>5</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>1</day><month>5</month><year>2019</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>16</volume><issue>154</issue><elocation-id>20180344</elocation-id><history><date date-type="received"><day>15</day><month>5</month><year>2018</year></date><date date-type="accepted"><day>2</day><month>4</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 The Authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>Published by the Royal Society under the terms of the Creative Commons Attribution License <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>, which permits unrestricted use, provided the original author and source are credited.</license-p></license><?release-delay 0|0?></permissions><self-uri content-type="pdf" xlink:href="rsif20180344.pdf"/><abstract><p>The selective attention for identification model (SAIM) is an established model of selective visual attention. SAIM implements translation-invariant object recognition, in scenes with multiple objects, using the parallel distributed processing (PDP) paradigm. Here, we show that SAIM can be formulated as Bayesian inference. Crucially, SAIM uses excitatory feedback to combine top-down information (i.e. object knowledge) with bottom-up sensory information. By contrast, predictive coding implementations of Bayesian inference use inhibitory feedback. By formulating SAIM as a predictive coding scheme, we created a new version of SAIM that uses inhibitory feedback. Simulation studies showed that both types of architectures can reproduce the response time costs induced by multiple objects&#x02014;as found in visual search experiments. However, due to the different nature of the feedback, the two SAIM schemes make distinct predictions about the motifs of microcircuits mediating the effects of top-down afferents. We discuss empirical (neuroimaging) methods to test the predictions of the two inference architectures.</p></abstract><kwd-group><kwd>selective visual attention</kwd><kwd>computational modelling</kwd><kwd>active inference</kwd><kwd>parallel distributed processing</kwd><kwd>neuroimaging</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>Wellcome Trust</institution><institution-id>http://dx.doi.org/10.13039/100004440</institution-id></institution-wrap></funding-source><award-id>088130/Z/09/Z</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>cover-date</meta-name><meta-value>May, 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><label>1.</label><title>Introduction</title><p>In 2003, Heinke &#x00026; Humphreys [<xref rid="RSIF20180344C1" ref-type="bibr">1</xref>] introduced the selective attention for identification model (SAIM) to model translation-invariant object identification in multiple object scenes. A foundational assumption of SAIM is that the brain implements a soft constraint satisfaction as implemented by the parallel distributed processing (PDP) paradigm [<xref rid="RSIF20180344C2" ref-type="bibr">2</xref>]. This led to a neural network architecture with feedback loops that enable an interaction between top-down information (i.e. knowledge about objects stored in an object identification stage) and bottom-up information (i.e. sensory information). Heinke and Humphreys demonstrated that SAIM could explain a broad range of empirical phenomena typically associated with selective visual attention, such as the effects of spatial cuing, object-based selection and the response time costs of recognizing multiple objects. Furthermore, SAIM could account for deficits in selective visual attention, such as visual neglect, visual extinction and the influence of knowledge on visual neglect.</p><p>In short, SAIM suggests that many &#x02018;attentional&#x02019; phenomena can be explained as an emergent property of object identification (i.e. perceptual inference) in multiple object scenes. As far as we know, this level of success remains unrivalled by any other model. Subsequent work by Heinke and colleagues [<xref rid="RSIF20180344C3" ref-type="bibr">3</xref>&#x02013;<xref rid="RSIF20180344C5" ref-type="bibr">5</xref>] demonstrated that extensions of SAIM could reproduce findings from visual search experiments, deal with natural colour images [<xref rid="RSIF20180344C6" ref-type="bibr">6</xref>] and perceptual grouping [<xref rid="RSIF20180344C7" ref-type="bibr">7</xref>]. Finally, by modifying the constraints to reflect action possibilities (i.e. affordances), it was possible to incorporate affordances in multiple object scenes [<xref rid="RSIF20180344C8" ref-type="bibr">8</xref>]. It is also worth noting that SAIM's mechanisms are based on nonlinear dynamics that are formally similar to those used in dynamic neural fields (e.g. [<xref rid="RSIF20180344C9" ref-type="bibr">9</xref>&#x02013;<xref rid="RSIF20180344C13" ref-type="bibr">13</xref>]). The latter reference is particularly relevant in the current context, because it considers the use of lateral interactions to engineer neurodynamic architectures for one-shot learning of visual objects using bottom-up recognition under top-down predictions. The common theme here is a dynamical implementation of a universal prior in object recognition; namely, that only one object (i.e. the winning or selected hypothesis) can cause sensory input at any one time. This fundamental prior is generally mediated by lateral interactions in neuronal schemes. The winner-take-all (WTA) interactions&#x02014;implicit in SAIM&#x02014;play the same role as lateral connections in neural field formulations.</p><p>The aim of this paper is to relate SAIM to a predictive processing framework for modelling action and perception; namely, the free-energy principle of Friston <italic>et al</italic>. (e.g. [<xref rid="RSIF20180344C14" ref-type="bibr">14</xref>&#x02013;<xref rid="RSIF20180344C17" ref-type="bibr">17</xref>]; see [<xref rid="RSIF20180344C18" ref-type="bibr">18</xref>,<xref rid="RSIF20180344C19" ref-type="bibr">19</xref>]). A <italic>prima facie</italic> inspection suggests that Bayesian principles advocate a similar computational architecture to that employed by SAIM: both architectures are hierarchical, and both contain feedback loops. This paper offers a mathematical analysis of how these two architectures are related. In brief, we show that SAIM can be derived from first principles (i.e. the free-energy principle). However, SAIM assumes a different &#x02018;generative model&#x02019; compared to those typically used in schemes like predictive coding. A crucial consequence of this difference is that SAIM's feedback loops are excitatory, while predictive coding schemes lead to inhibitory feedback loops (i.e. subtracting predictions from sensory input to form prediction errors). To facilitate a direct comparison between these two architectures, we derived a new version of SAIM&#x02014;error prediction (EP)-SAIM&#x02014;which uses the generative model usually adopted in predictive coding. We then present stimulation studies comparing the two models and produce (quantitative) predictions for future (EEG or fMRI) studies. In short, this work develops a formalism to address an important and long-standing systems neuroscience question: does the brain combine sensory information with prior knowledge using excitatory or inhibitory feedback?</p><p>To clarify the arguments, especially for those unfamiliar with SAIM, we first present a slightly revised version of SAIM. To highlight the contrasting assumptions about the feedback loops, we will call this version excitatory matching (EM)-SAIM. We then replicate a key finding from the foundational paper that introduced SAIM. Using simulations, we illustrate EM-SAIM's ability to perform object identification in multiple object scenes. Moreover, these simulations show that EM-SAIM reproduces the well-known multiple object cost; i.e. the increased time it takes to detect a target object with increasing numbers of non-target objects. This ubiquitous empirical finding is an emergent property of SAIM's WTA mechanism. The evidence for multiple object cost comes from visual search experiments (e.g. [<xref rid="RSIF20180344C20" ref-type="bibr">20</xref>]; see [<xref rid="RSIF20180344C21" ref-type="bibr">21</xref>] for a review). Here, we reproduce these results using the EM version of SAIM. Having established the validity of this EM scheme, we then reformulated the soft constraints in SAIM as free-energy minimization&#x02014;to produce a prediction error (PE)-SAIM. We then repeated the simulation studies using the same (synthetic) stimuli to establish its construct validity, in relation to EM-SAIM. Finally, we compare and contrast the simulation results to identify key aspects of belief updating that may enable the two versions to be disambiguated, using empirical measures of neuronal evidence accumulation (e.g. EEG or fMRI). The MatLab code for the simulation studies reported in this paper can be found in the Github repository <uri xlink:href="https://github.com/SAIM-models/EMvPE">https://github.com/SAIM-models/EMvPE</uri>.</p><p>This paper does not aim to advance our understanding of selective visual attention <italic>per se</italic>; e.g. by comparing predictive coding and SAIM formulations of attention (e.g. [<xref rid="RSIF20180344C22" ref-type="bibr">22</xref>,<xref rid="RSIF20180344C23" ref-type="bibr">23</xref>]). Rather, we hope to lay the foundations for empirical work that will disambiguate between these convergent formulations (see General discussion). Finally, we have tried to keep the mathematics accessible for readers without a mathematical background.</p></sec><sec id="s2"><label>2.</label><title>The excitatory matching (EM)-SAIM</title><p>Before presenting the mathematical derivation of EM-SAIM, we provide an overview of the EM-SAIM architecture (<xref ref-type="fig" rid="RSIF20180344F1">figure&#x000a0;1</xref>; for an illustration). After considering the mathematical details, we then highlight how an EM-SAIM differs from the original SAIM. We conclude this section by demonstrating that EM-SAIM can reproduce multiple object costs.
<fig id="RSIF20180344F1" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>EM-SAIM's architecture. The three networks, Knowledge Network, Contents Network and Selection Network, have different functions: the Knowledge Network identifies the contents of the FOA by activating the best-matching template unit. The Contents Network maps a section of the input image into the FOA. The Selection Network determines the location of this section (see details in the main text). The arrows between the modules indicate the direction of message passing between the networks. (Online version in colour.)</p></caption><graphic xlink:href="rsif20180344-g1"/></fig></p><sec id="s2a"><label>2.1.</label><title>Overview</title><p>EM-SAIM selects an object by mapping a region in the input image into a &#x02018;focus of attention&#x02019; (FOA) (<xref ref-type="fig" rid="RSIF20180344F1">figure&#x000a0;1</xref>). The mapping is implemented through the <italic>Contents Network</italic> and is translation invariant. This means that no matter where an object appears in the input scene, it can be mapped into the FOA. The <italic>Selection Network</italic> determines which region in the input image is mapped into the FOA. The <italic>Selection Network</italic> identifies this region by activating units in a layer that corresponds to locations in the input image (<xref ref-type="fig" rid="RSIF20180344F1">figure&#x000a0;1</xref>). The output of the <italic>Contents Network</italic> is passed onto the <italic>Knowledge Network</italic>. The <italic>Knowledge Network</italic> is equipped with template units that store templates of known (i.e. recognizable) objects. This network compares the templates and the input from the <italic>Contents Network</italic> with a simple template matching. Given the results of this template matching, the <italic>Knowledge Network</italic> activates the best-matching template unit. This reflects the identity of the selected object&#x02014;the object in the <italic>Contents Network</italic>.</p><p>In addition to these bottom-up pathways, EM-SAIM also possesses top-down pathways. Note these top-down pathways are mandated by the soft constraint satisfaction approach described below. The top-down pathway from the <italic>Knowledge Network</italic> to the <italic>Contents Network</italic> adds a weighted sum of the templates to the activation in the FOA (excitatory feedback). The weighting is determined by the activation of the template units. In other words, the feedback directs the FOA to focus on the content of the <italic>Contents Network</italic>. The top-down connections from the <italic>Contents Network</italic> to the <italic>Selection Network</italic> underwrite a correlation of the <italic>Contents Network</italic> with the input image. The result of the correlation is feed into the <italic>Selection Network</italic>. Again&#x02014;as with the feedback from <italic>Knowledge Network <italic>to</italic> Contents Network&#x02014;</italic>this correlation rests on excitatory feedback. Since the <italic>Selection Network</italic> implements a WTA mechanism, this input directs the <italic>Selection Network's</italic> attention to the location in the input image that best matches the content of the <italic>Contents Network</italic>.</p><p>It is important to note that EM-SAIM does not achieve object identification instantaneously. Rather, object identification evolves over time. Initially (if we assume that there is no foreknowledge about the objects in the scene), the template units have same activation; the <italic>Contents Network</italic> is set to an equally weighted summation of template units and the <italic>Selection Network</italic> has equal activation across all image locations (i.e. no spatial bias). Subsequently, EM-SAIM begins the selection process and identification process in parallel, eventually converging to a point attractor, in which no unit changes its activation. At that point, EM-SAIM is said to have selected and identified an object.</p></sec><sec id="s2b"><label>2.2.</label><title>Mathematical derivation</title><p>Our implementation of EM-SAIM is based on the energy function minimization scheme introduced by Hopfield &#x00026; Tank [<xref rid="RSIF20180344C24" ref-type="bibr">24</xref>]. In this scheme, the desired outputs of a network are expressed in terms of constraints; e.g. template matching as a constraint on the object identification in the knowledge network. Network dynamics can then be expressed as a gradient descent on an energy function <inline-formula><mml:math id="IM1"><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> of the output activity <inline-formula><mml:math id="IM2"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:math></inline-formula> of the neurons. The energy function comprises a mixture of distinct energy functions, where the minimum of each component satisfies a particular constraint. This ensures the network dynamics implement a form of soft constraint satisfaction. The general form of EM-SAIM uses the gradient descent described by Hopfield &#x00026; Tank [<xref rid="RSIF20180344C24" ref-type="bibr">24</xref>]<disp-formula id="RSIF20180344M2.1"><label>2.1</label><mml:math id="DM1"><mml:mi>&#x003c4;</mml:mi><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="IM3"><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo>&#x002d9;</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the transmembrane potential of the <italic>i</italic>th neuron (or neural population), <inline-formula><mml:math id="IM4"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is their firing rate activation and <italic>&#x003c4;</italic> is the membrane time constant. The activation and depolarization are linked through a well-known sigmoid (activation) function: <inline-formula><mml:math id="IM5"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>.</p><p>To ensure a level of biological plausibility, SAIM's energy function includes an energy component for every neuron or unit<disp-formula id="RSIF20180344M2.2"><label>2.2</label><mml:math id="DM2"><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">mem</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>&#x003c4;</mml:mi></mml:mfrac></mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mo>&#x0222b;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula></p><p>The gradient descent on this term leads to neuronal dynamics that emulate a leaky postsynaptic membrane.<sup><xref ref-type="fn" rid="FN1">1</xref></sup> Another energy component, that is central to SAIM, is the WTA energy function<disp-formula id="RSIF20180344M2.3"><label>2.3</label><mml:math id="DM3"><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">WTA</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02212;</mml:mo><mml:mi>b</mml:mi><mml:munder><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:munder><mml:mo>&#x02061;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="IM6"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are the inputs to the <italic>i</italic>th neuron or neuronal population. This WTA energy function produces competition among neurons, in which the neuron with the largest input becomes activated&#x02014;to nearly one (i.e. the winning unit), while all remaining neurons tend to zero. The first term corresponds to the constraint that the sum of all neuronal activities is equal to one; while the second term (i.e. input term) implies the constraint that the response of the neuron with the greatest input is maximal. The addition of the two ensures a WTA behaviour, where <italic>a</italic> and <italic>b</italic> weight the two constraints; allowing either constraint to dominate. The ensuing WTA behaviour is a nice illustration of <italic>soft</italic> constraint satisfaction. This energy function is important for the <italic>Knowledge Network</italic>, where the best-matching template is indicated by the highest input&#x02014;and for <italic>Selection Network</italic>, as we will see later. It is also important to note that a change of the sign of the input term turns the WTA into a loser-take-all where the neuron with the smallest input wins the competition. This mechanism is important for PE-SAIM.</p><p>To ensure that EM-SAIM satisfies all constraints imposed by its constituent networks, the energy functions for each network are combined to provide an objective function for the entire network<disp-formula id="RSIF20180344M2.4"><label>2.4</label><mml:math id="DM4"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace"><mml:mtr><mml:mtd><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">mem</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In other words, each network implements a constraint that is specified in terms of its unique energy function, while every neuron tries to minimize the total energy function: <inline-formula><mml:math id="IM7"><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>. Here, <inline-formula><mml:math id="IM8"><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> is the energy function for the <italic>Selection Network</italic>, <inline-formula><mml:math id="IM9"><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x02009;</mml:mo></mml:math></inline-formula> is the energy function for the <italic>Contents Network</italic> and <inline-formula><mml:math id="IM10"><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> is the energy function for the <italic>Knowledge Network</italic> (i.e. superscripts SN, CN and KN stand for <italic>Selection Network</italic>, <italic>Contents Network</italic> and <italic>Knowledge Network,</italic> respectively).</p><p>The arguments of the energy functions, <inline-formula><mml:math id="IM11"><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="IM12"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>,&#x000a0;are the outputs of the <italic>Selection Network</italic> and the <italic>Knowledge Network</italic>, respectively, and <inline-formula><mml:math id="IM13"><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> is the output of the <italic>Contents Network</italic>. The use of <inline-formula><mml:math id="IM14"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow></mml:math></inline-formula> here indicates that&#x02014;in contrast with the <italic>Knowledge Network</italic> and the <italic>Selection Network</italic>&#x02014;we drop the sigmoid function in the <italic>Contents Network</italic>. This follows because the <italic>Contents Network</italic> represents continuous valued sensory signals. Also note the use of matrix notation for the <italic>Contents Network</italic> and the <italic>Selection Network</italic> outputs, which are two-dimensional matrices. By contrast, the <italic>Knowledge Network</italic> output is a one-dimensional vector. In the following, we will consider each individual energy function and the constraints it satisfies in detail.</p><sec id="s2b1"><label>2.2.1.</label><title>Knowledge network</title><p>The Knowledge Network implements template-based object identification through a scalar product<disp-formula id="RSIF20180344M2.5"><label>2.5</label><mml:math id="DM5"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">temp</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Here, <italic>M</italic> is the size of the FOA and <inline-formula><mml:math id="IM15"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> is the template of the <italic>k</italic>th template neuron or unit. The size of each template is the same as the size of the FOA. Examples of templates can be found in the simulations below (<xref ref-type="fig" rid="RSIF20180344F2">figure&#x000a0;2</xref>). The Knowledge Network constraint ensures that the best-matching template unit is activated, while the remaining units are suppressed. The WTA energy function implements this constraint<disp-formula id="RSIF20180344M2.6"><label>2.6</label><mml:math id="DM6"><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>x</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">temp</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula>
<fig id="RSIF20180344F2" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>Input images and templates. The simulations used three input images and two templates in the Knowledge Network. The three input images were two single-object images (+ and 2) and one two-object image (+/2). The two templates perfectly matched the two objects.</p></caption><graphic xlink:href="rsif20180344-g2"/></fig></p></sec><sec id="s2b2"><label>2.2.2.</label><title>Contents network</title><p>The Contents Network receives an input from Sigma-pi units (i.e. modulatory synaptic interactions) which combine the activation in the selection network and the visual field to realize a translation-invariant mapping<disp-formula id="RSIF20180344M2.7"><label>2.7</label><mml:math id="DM7"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>Here, <italic>N</italic> is the size of the input image and <inline-formula><mml:math id="IM16"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> is the input image. Contents Network constraint ensures that the output units of the <italic>Contents Network</italic> reflect the output of the Sigma-pi units<disp-formula id="RSIF20180344M2.8"><label>2.8</label><mml:math id="DM8"><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x02009;</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula></p></sec><sec id="s2b3"><label>2.2.3.</label><title>Selection network</title><p>The Selection Network implements one constraint, which ensures that only one location is selected. Here, we used the first term of the WTA energy function<disp-formula id="RSIF20180344M2.9"><label>2.9</label><mml:math id="DM9"><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula></p><p>This concludes our description of the network-specific energy components that constitute the total energy.</p><p>To simulate the processing of visual input, the total energy is minimized using a gradient descent scheme with the form of equation (2.1). In detail, we used an Euler approximation, with the addition of biological noise, of the sort implied by drift diffusion models (e.g. [<xref rid="RSIF20180344C25" ref-type="bibr">25</xref>])<disp-formula id="RSIF20180344M2.10"><label>2.10</label><mml:math id="DM10"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02009;</mml:mo><mml:mo>;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="IM17"><mml:msub><mml:mi>&#x003be;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the noise term with variance <inline-formula><mml:math id="IM18"><mml:mi>&#x003c3;</mml:mi></mml:math></inline-formula>. The resulting energy gradients for each network can then be expressed as follows (using direct calculation):</p><p><italic>Selection Network</italic><disp-formula id="RSIF20180344M2.11"><label>2.11</label><mml:math id="DM11"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace"><mml:mtr><mml:mtd><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi>j</mml:mi><mml:mo>&#x02009;</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mi mathvariant="bold">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><italic>Contents Network</italic><disp-formula id="RSIF20180344M2.12"><label>2.12</label><mml:math id="DM12"><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo><mml:mrow><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">K</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mi mathvariant="bold">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula></p><p><italic>Knowledge Network</italic><disp-formula id="RSIF20180344M2.13"><label>2.13</label><mml:math id="DM13"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace"><mml:mtr><mml:mtd><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:msubsup><mml:mi>x</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The terms in bold font (i.e. input terms in equation (2.3)) represent feedback from higher networks to lower networks; i.e. from the Knowledge Network to the Contents Network and from Contents Network to Selection Network. These terms follow from the gradient descent and show that feedback connections are required for soft constraint satisfaction. Crucially, these feedback connections constitute a positive (i.e. excitatory) feedback (see <xref rid="RSIF20180344TB1" ref-type="table">table&#x000a0;1</xref> for the circuit diagram of the implicit message passing and connections). For example, responses in the Contents Network <inline-formula><mml:math id="IM19"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow></mml:math></inline-formula> will descend the gradient in equation (2.12), and will therefore increase with the activity of units in the higher Knowledge Network <inline-formula><mml:math id="IM20"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula>. Similarly, unit responses in the Selection Network <inline-formula><mml:math id="IM21"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow></mml:math></inline-formula> increase with the source of descending projections from the Contents Network <inline-formula><mml:math id="IM22"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mrow><mml:mo>&#x02009;</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula>.
<table-wrap id="RSIF20180344TB1" orientation="portrait" position="float"><label>Table&#x000a0;1.</label><caption><p>Graphical illustration of feedback connections. These circuit diagrams illustrate how equations (2.11) and (2.12) for EM-SAIM and equations (3.3) and (3.4) for PE-SAIM map onto neural message passing and circuitry. Circles denote hypothetical neuronal populations, while the arrows correspond to connections. Excitatory connections are shown in black and inhibitory connections are shown in red. The small blue (crossed) circles denote a modulatory synaptic interaction (Sigma-pi units). These graphical illustrations illustrate why EM-SAIM can be seen as being mediated by excitatory feedback while PE-SAIM uses inhibitory feedback to implement a disinhibition via prediction error units. (Online version in colour.)</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/></colgroup><tbody><tr><td rowspan="1" colspan="1"><inline-graphic xlink:href="rsif20180344-i1.jpg"/></td></tr></tbody></table></table-wrap></p></sec></sec><sec id="s2c"><label>2.3.</label><title>Comparing EM-SAIM with the original SAIM</title><p>EM-SAIM incorporates two changes that lend it a greater biological plausibility than the original implementation. The first is the inclusion of Brownian noise. This not only makes EM-SAIM more biological plausible but enables it to simulate variations in response time commonly found in behavioural experiments. The second change concerns the feedback connections. In the original SAIM, the feedback from the <italic>Knowledge Network</italic> was conveyed directly to the <italic>Selection Network</italic>. In EM-SAIM, the <italic>Knowledge Network</italic> now projects to the <italic>Contents Network</italic> and the <italic>Contents Network</italic> projects to the <italic>Selection Network</italic>. This change creates a more plausible architecture, given that feedback tends to target input brain region (e.g. [<xref rid="RSIF20180344C26" ref-type="bibr">26</xref>]).</p><p>This revised feedback architecture retains the top-down modulation of the selection process, albeit in a more indirect way. To fully understand neurobiological premise of this argument, it is worth noting that SAIM's networks can be related to the <italic>what</italic>-pathway and the <italic>where</italic>-pathway (see [<xref rid="RSIF20180344C1" ref-type="bibr">1</xref>] for a more detailed discussion). According to this interpretation, the <italic>Knowledge Network</italic> and the <italic>Contents Network</italic> correspond to brain regions in the <italic>what</italic>-pathway (ventral pathway), while the <italic>Selection Network</italic> corresponds to areas in the <italic>where</italic>-pathway (dorsal pathway), the posterior parietal cortex. Hence, if the <italic>Knowledge Network</italic> and the <italic>Contents Network</italic> are in the ventral pathway, feedback connections between these two networks better reflect known anatomical connections (as opposed to feedback connections to the <italic>Selection Network</italic> as in the original SAIM).</p></sec><sec id="s2d"><label>2.4.</label><title>Simulation results</title><p>We first performed validation simulations to ensure EM-SAIM can replicate the simulations of multiple object cost in terms of reaction times, as reported in Study 2 of Heinke &#x00026; Humphreys [<xref rid="RSIF20180344C1" ref-type="bibr">1</xref>]. As in the original study, we used two objects, 2 and + (cross) (<xref ref-type="fig" rid="RSIF20180344F2">figure&#x000a0;2</xref>). These objects also formed the templates in the <italic>Knowledge Network</italic>. The reaction times were simulated by measuring the number of time steps it takes for a template unit to pass a threshold (see appendix A for parameters). The multiple object cost was simulated by contrasting the reaction times for input images with one object (+ or 2) with input images with two objects, + and 2. In empirical experiments, such as visual search tasks, multiple object costs are demonstrated with more objects (e.g. [<xref rid="RSIF20180344C20" ref-type="bibr">20</xref>]; see [<xref rid="RSIF20180344C3" ref-type="bibr">3</xref>] for a simulation study). However, for the purpose of this work, a simple set-up is sufficient to establish that EM-SAIM reproduces SAIM's cardinal behaviour. <xref ref-type="fig" rid="RSIF20180344F3">Figure&#x000a0;3</xref> shows an example of a typical simulation for three input images: +/2, single 2 and single +.
<fig id="RSIF20180344F3" orientation="portrait" position="float"><label>Figure 3.</label><caption><p>Three exemplar simulation results for multiple object costs with EM-SAIM. The graphs show the time course of the activation for the FOA and the two template units in the Knowledge Network. The reaction times were measured by determining the number of iterations it takes for a template unit to pass a threshold (0.9). As expected, the results show that EM-SAIM's reaction times were slower for the two-objects image (1013 iterations) than for the two single-object images: + (687 iterations) and 2 (777 iterations).</p></caption><graphic xlink:href="rsif20180344-g3"/></fig></p><p>These examples show that EM-SAIM can reproduce the multiple object cost. Also, as in the original SAIM, EM-SAIM exhibits a top-down bias towards the +, as the combined templates match better with the+than the 2. We also conducted a study with 20 simulations for each input image, to establish there was a statistically significant difference between the three conditions (<xref ref-type="fig" rid="RSIF20180344F4">figure&#x000a0;4</xref>). We applied a <italic>t</italic>-test to the simulation results and found a significant difference between +/2 and single + (<italic>t</italic><sub>38</sub> = 11.40; <italic>p</italic> &#x0003c; 0.001) and between +/2 and single 2 (<italic>t</italic><sub>38</sub> = 5.34; <italic>p</italic> &#x0003c; 0.001) (and between 2 and single + (<italic>t</italic><sub>38</sub> = &#x02212;7.85; <italic>p</italic> &#x0003c; 0.001)). Crucially, the reaction time for +/2 was slower than for single + and single 2.
<fig id="RSIF20180344F4" orientation="portrait" position="float"><label>Figure 4.</label><caption><p>Results for 20 simulation runs for each input image. There was significant difference between +/2 and single +; and between +/2 and single 2. Hence, EM-SAIM can replicate the findings with the original SAIM (see main text for details).</p></caption><graphic xlink:href="rsif20180344-g4"/></fig></p><p>In summary, these simulation results suggest that EM-SAIM reproduces the key result from the original SAIM simulations. In addition to the original SAIM simulations, the new (EM) version can also reproduce the natural variation of reaction times found in experiments with humans. Also, despite the addition of neuronal noise, none of the 40 single stimuli simulations showed an error and the +/2 simulations always identified the cross. Note that the exact numerical outcome of the simulations, such as the variation of reaction times, depends on the parameter settings. Nevertheless, a broad range of parameter settings produce the findings present here. We will return to the issue of numerical evaluation of the model in the discussion section of PE-SAIM.</p></sec><sec id="s2e"><label>2.5.</label><title>Interpreting selective attention for identification model within the active inference framework</title><p>In this section, we consider the links between the above formulation of visual processing within the PDP framework and current formulations based upon predictive coding and the Bayesian brain. In brief, we will see that both SAIM and approximate Bayesian inference can be described in terms of minimizing an energy function. The particular energy function used in Bayesian formulations corresponds to variational free energy (also known as an &#x02018;evidence bound&#x02019; in machine learning). Variational free energy is a function of data and a generative model (i.e. a probabilistic model of how data are generated from causes, such as visual objects). In what follows, we show that the energy function used by SAIM can be interpreted as a variational free energy under a particular generative model. This means SAIM can be formulated in terms of Bayesian inference under a particular model of how visual data were generated. Furthermore, it means the computational architecture described in the previous section can be compared in a formal way to the architectures used in Bayesian schemes.</p><p>Casting SAIM in terms of variational free-energy minimization is much simpler than one might suppose. The free-energy principle considers how the Bayesian brain hypothesis (see [<xref rid="RSIF20180344C27" ref-type="bibr">27</xref>] for a review) may be implemented in the brain. According to the free-energy principle (and in line with the Bayesian brain hypothesis), the brain is thought to use a generative model to infer the hidden (i.e. latent) causes of sensory signals. These models are characterized as &#x02018;generative&#x02019; in the sense that they describe how the latent causes generate signals. In the course of the inference process, the brain is assumed to update representations (as encoded by a posterior probability density) of the latent causes via a minimization of &#x02018;free energy&#x02019;. This belief updating, evidence accumulation or inference process can be illustrated using SAIM's object identification.</p><p>Let us assume the generative model of object identification comprised the templates used in SAIM. Hence, for each physical object (e.g. two, crosses, etc.), the templates represent the latent causes of sensory signals in the input image. Given these sensory signals, the minimization of the free energy produces a posterior probability density for each template&#x02014;reflecting the probability that the sensory signals are caused by the corresponding object. On this view, the templates correspond to prior beliefs about the latent causes of sensory signals that are recovered from sensory data through Bayesian belief updating. This belief updating can be expressed as a gradient descent on variational free energy.</p><p>An important point to note here is that the free energy minimized during inference is a single quantity (i.e. a functional of the posterior probability density and sensory input) that is specified by the generative model. In other words, the free energy is a global objective function analogous to SAIM's total energy function&#x02014;and in both approaches, the energy has to be minimized. Hence, SAIM is, in effect, an instantiation of the free-energy principle. Moreover, a gradient descent on the free-energy functional implements the inference by optimizing the posterior distribution (e.g. [<xref rid="RSIF20180344C16" ref-type="bibr">16</xref>]). In short, SAIM's gradient descent is formally consistent with the free-energy principle. In addition, one can regard SAIM's soft constraint satisfaction as equivalent to probabilistic inference under certain prior beliefs (i.e. constraints on the way visual data are generated).</p><p>Note that SAIM's inference process does not yield a representation of uncertainty, but simply a point estimate of the posterior. In Bayesian terms, this corresponds to a <italic>maximum a posteriori</italic> estimate. In terms of the free-energy principle, SAIM inverts a hierarchical Bayesian model, where the <italic>Contents Network</italic>, <italic>Selection Network</italic> and <italic>Knowledge Network</italic> encode the posterior expectations and hierarchical (also known as empirical) priors. Interestingly, the WTA constraints in SAIM can be regarded as implementing the prior belief that only one object can be in one place at a time.</p><p>Having noted a formal equivalence between SAIM's energy minimization approach and the free-energy principle, one can now ask: what is SAIM's underlying generative model? In the free-energy approach, the probabilistic generative model is linked and energy through a Gibbs measure<disp-formula id="RSIF20180344M2.14"><label>2.14</label><mml:math id="DM14"><mml:mi>ln</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mi>&#x003bc;</mml:mi><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula><mml:math id="IM23"><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> denotes sensory signals and <inline-formula><mml:math id="IM24"><mml:mi>&#x003bc;</mml:mi></mml:math></inline-formula> are the expected causes of sensory signals under a generative model <italic>m</italic>. To reverse engineer the probabilistic representation in EM-SAIM, consider the energy function of EM-SAIM<disp-formula id="RSIF20180344M2.15"><label>2.15</label><mml:math id="DM15"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace"><mml:mtr><mml:mtd><mml:mi>ln</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>&#x02212;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This equation can be separated into network-specific components, which correspond to the empirical and full priors of the generative model<sup><xref ref-type="fn" rid="FN2">2</xref></sup><disp-formula id="RSIF20180344M2.16"><label>2.16</label><mml:math id="DM16"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace"><mml:mtr><mml:mtd/><mml:mtd><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>with the likelihood and prior from the <italic>Selection Network</italic> becoming<disp-formula id="RSIF20180344M2.17"><label>2.17</label><mml:math id="DM17"><mml:mi>ln</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x02009;</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></disp-formula>and<disp-formula id="RSIF20180344M2.18"><label>2.18</label><mml:math id="DM18"><mml:mi>ln</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>and the empirical prior from the <italic>Content Network</italic> becoming<disp-formula id="RSIF20180344M2.19"><label>2.19</label><mml:math id="DM19"><mml:mi>ln</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02009;</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:math></disp-formula>and<disp-formula id="RSIF20180344M2.20"><label>2.20</label><mml:math id="DM20"><mml:mi>ln</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>where the prior from the <italic>Knowledge Network</italic>
<inline-formula><mml:math id="IM25"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is a full prior.</p><p>These equations show that SAIM's generative model is formally distinct from those used in predictive coding, which uses Gaussian priors to ensure the priors are conjugate with the approximate (Gaussian) posterior (this is known as the Laplace assumption in Bayesian statistics). Under Gaussian assumptions, the likelihood and empirical priors above would have quadratic forms. However, it is immediately evident that the generative model implicit in SAIM has a much richer form. For example, the full priors in equations (2.18) and (2.20) show that EM-SAIM's model assumes a sparse probability density over the causes in the <italic>Selection</italic> and <italic>Knowledge Networks</italic>. This follows because these prior energies are minimized when one of the latent (non-negative) causes are one and the rest are zero. This sort of non-Gaussian prior is commonly employed in LASSO (least absolute shrinkage and selection operator) regression analyses (see Discussion). We will now look more closely at this form and elaborate a variant of SAIM whose empirical priors can be expressed in terms of squared prediction errors.</p></sec></sec><sec id="s3"><label>3.</label><title>The PE-SAIM</title><p>In the previous section, we formulated SAIM in terms of free-energy minimization under a particular generative model that entails non-Gaussian empirical priors, in contrast with predictive coding models that usually assume Gaussian forms. In this section, we modify EM-SAIM by adopting Gaussian assumptions in the generative model (called PE-SAIM) and examine whether this new version can replicate the multiple object cost findings above. Under Gaussian assumptions, the free-energy components can be expressed as squared <italic>prediction errors</italic>. In SAIM, this applies to two levels: the <italic>Contents Network</italic>, which predicts the activation in the input image modulated by the <italic>Selection Network</italic> via Sigma-pi units<disp-formula id="RSIF20180344M3.1"><label>3.1</label><mml:math id="DM21"><mml:mi>ln</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:math></disp-formula><disp-formula id="RSIF20180344UM1"><mml:math id="DM22"><mml:mrow><mml:mi mathvariant="normal">and</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:msubsup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math></disp-formula>and the <italic>Knowledge Network</italic> which predicts the content of the FOA<disp-formula id="RSIF20180344M3.2"><label>3.2</label><mml:math id="DM23"><mml:mi>ln</mml:mi><mml:mo>&#x02061;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo fence="false" stretchy="false">|</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mstyle></mml:math></disp-formula><disp-formula id="RSIF20180344UM2"><mml:math id="DM24"><mml:mrow><mml:mi mathvariant="normal">and</mml:mi></mml:mrow><mml:mspace width="1em"/><mml:msubsup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02212;</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>As noted earlier, the use of <inline-formula><mml:math id="IM26"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> (rather than <inline-formula><mml:math id="IM27"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula>) reflects the fact that the <italic>Contents Network</italic> uses a linear output function. Finally, note that in PE-SAIM, the two WTA priors (i.e. softmax) becomes a loser-take-all (i.e. softmin)&#x02014;as the <italic>Selection Network</italic> and <italic>Knowledge Network</italic> need to select the best predictors; i.e. minimize prediction error. To minimize free energy, we again used an Euler scheme for gradient descent, retaining biological noise as in EM-SAIM. The requisite gradients for each network or hierarchical level can be derived by direct calculation from the above expressions:</p><p><italic>Selection Network</italic><disp-formula id="RSIF20180344M3.3"><label>3.3</label><mml:math id="DM25"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace"><mml:mtr><mml:mtd><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>&#x02009;</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">C</mml:mi><mml:mi mathvariant="bold">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">n</mml:mi></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">F</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><italic>Contents Network</italic><disp-formula id="RSIF20180344M3.4"><label>3.4</label><mml:math id="DM26"><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>k</mml:mi><mml:mi>K</mml:mi></mml:munderover><mml:msubsup><mml:mrow><mml:mi>&#x003f5;</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">K</mml:mi><mml:mi mathvariant="bold">N</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula></p><p><italic>Knowledge Network</italic><disp-formula id="RSIF20180344M3.5"><label>3.5</label><mml:math id="DM27"><mml:mtable columnalign="right left" rowspacing=".5em" columnspacing="thickmathspace"><mml:mtr><mml:mtd><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msup><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">total</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">VF</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x02202;</mml:mi><mml:msubsup><mml:mi>y</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:munder><mml:mo>&#x02061;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mspace width="1em"/><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>&#x02009;</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>&#x02061;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>These equations map onto a neural architecture as illustrated in <xref rid="RSIF20180344TB1" ref-type="table">table&#x000a0;1</xref>. The summaries of neuronal message passing in <xref rid="RSIF20180344TB1" ref-type="table">table&#x000a0;1</xref> illustrate why EM-SAIM can be seen as being mediated by excitatory feedback, while PE-SAIM uses inhibitory feedback to implement a disinhibition via prediction error units. For example, the influence of <inline-formula><mml:math id="IM28"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>&#x02009;</mml:mo></mml:math></inline-formula> on <inline-formula><mml:math id="IM29"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> is mediated by two inhibitory connections (via <inline-formula><mml:math id="IM30"><mml:msubsup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula>); namely, an inhibition of inhibition. As in the equations for EM-SAIM, we used bold to indicate the feedback terms between networks. However, in contrast with EM-SAIM, the feedback terms are mediated by prediction errors (i.e. the <inline-formula><mml:math id="IM31"><mml:mi>&#x003f5;</mml:mi></mml:math></inline-formula> terms in equations (3.1) and (3.2)) that implement an <italic>inhibitory</italic> (i.e. negative) influence of higher levels on the low levels. This inhibitory feedback is mandated by the formation of prediction errors. For example, the gradient descent implied by equation (3.4) means that units in the content network <inline-formula><mml:math id="IM32"><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> increase when prediction errors <inline-formula><mml:math id="IM33"><mml:msubsup><mml:mi>&#x003f5;</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> decrease. In short, by introducing prediction errors, we effectively reverse the sign of the coupling between successive levels in the hierarchy.</p><p>This architecture is consistent with generic predictive coding schemes, in which the prediction errors at any level in a predictive coding hierarchy are formed by subtracting predictions to create a prediction error or mismatch. Before considering the implications for neuronal message passing in the brain, we need to first establish the construct validity of the PE-SAIM in relation to the multiple object cost.</p><sec id="s3a"><label>3.1.</label><title>Simulation results and discussion</title><p>Figures&#x000a0;<xref ref-type="fig" rid="RSIF20180344F5">5</xref> and <xref ref-type="fig" rid="RSIF20180344F6">6</xref> show simulation results that demonstrate PE-SAIM can also replicate the two-object cost. The <italic>t</italic>-test confirmed a significant difference between +/2 and single + (<italic>t</italic><sub>38</sub> = 17.09; <italic>p</italic> &#x0003c; 0.001) and between +/2 and single 2 (<italic>t</italic><sub>38</sub> = 16.52; <italic>p</italic> &#x0003c; 0.001) (and between 2 and single + (<italic>t</italic><sub>38</sub> = &#x02212;4.00; <italic>p</italic> &#x0003c; 0.001)). Furthermore, none of the 40 single stimuli simulations showed an error and the +/2 simulations always selected the cross. Hence, both variants of SAIM can reproduce the qualitative multiple object costs. This is pleasing in the sense that it establishes a construct validity of the two schemes. In other words, both EM-SAIM and PE-SAIM can reproduce the finer (psychophysical) details of perceptual synthesis in recognizing multiple objects in visual scenes in a biologically plausible fashion. However, this presents an interesting challenge if we wanted to establish which offers the best account of neuronal message passing in real visual hierarchies. Recall from above that a key architectural difference between the two schemes is the use of top-down predictions to select the most likely explanation for sensory input in fundamentally different ways. The EM scheme uses <italic>excitatory</italic> feedback to ensure top-down constraints are satisfied in lower levels, while the PE scheme employs top-down predictions to form prediction errors using <italic>inhibitory</italic> feedback.
<fig id="RSIF20180344F5" orientation="portrait" position="float"><label>Figure 5.</label><caption><p>Three exemplar simulation results for the multiple object costs with PE-SAIM. The graphs show the time course of the activation for the FOA and the two template units in the Knowledge Network. The reaction times were measured by determining the number of iterations it takes for a template unit to pass a set threshold (0.56). As expected, the results show that PE-SAIM's reaction times were slower for the two-objects image (1159 iterations) than for the two single-object images:+(271 iterations) and 2 (267 iterations).</p></caption><graphic xlink:href="rsif20180344-g5"/></fig>
<fig id="RSIF20180344F6" orientation="portrait" position="float"><label>Figure 6.</label><caption><p>Simulation results for PE-SAIM from 20 runs for stimulus. There was significant difference between +/2 and single +; and between +/2 and single 2. Hence, PE-SAIM can produce the same results as EM-SAIM (see main text for details).</p></caption><graphic xlink:href="rsif20180344-g6"/></fig></p></sec></sec><sec id="s4"><label>4.</label><title>Comparing PE-SAIM with EM-SAIM</title><p>It is important to note that these particular simulation results depend on our particular choice of parameters.<sup><xref ref-type="fn" rid="FN3">3</xref></sup> For both networks, the parameters were chosen to ensure significant reaction time cost effects in the absence of recognition errors. On the other hand, it would have been possible to generate simulation results where reaction costs are paired with recognition errors. Even though this observation is not crucial to make the point that, in principle, both models can replicate the two-object cost, it suggests the choice of parameters can modify the performance of object recognition in a measurable way. In turn, this affords the opportunity to compare the ability of the two schemes to explain empirical (e.g. psychophysical) data. This sort of comparison usually uses Bayesian model comparison. Bayesian model comparison has been used to disambiguate different models of choice behaviour and generally rests upon computing Bayes factors that score the evidence for one model over another, given the same data [<xref rid="RSIF20180344C28" ref-type="bibr">28</xref>] (see [<xref rid="RSIF20180344C29" ref-type="bibr">29</xref>] for a review). In brief, the Bayes factor assesses which model is better at generating a given dataset, considering all plausible parameter settings (under some generally uninformative prior over the parameters).</p><p>For the purpose of evaluating the two implementations of SAIM, Bayesian model comparison could leverage trade-offs between recognition accuracy and reaction time costs (similar to the effects observed in our simulations) by varying the number of objects and the discriminability of the stimuli. In this setting, it might be possible to use the two models to fit behavioural accuracy and response times, by optimizing model parameters. In principle, it would then be possible to compare the evidence for both schemes in empirical response data.</p><p>The simulations also illustrate an interesting point about the representation of the selected object in FOA. Despite the fact that there are no perfect representations of the selected object, both SAIMs can make correct decisions. This is the case because the &#x02018;two&#x02019; can be easily discriminated from the &#x02018;cross&#x02019;. Note a perfect representation is not necessary as the task does not require it. Moreover, EM-SAIM's representation is less accurate than PE-SAIM's representation. This difference has the potential to distinguish between the two models. For instance, in an empirical study, participants could be required not only to find a certain object, but also to identify specific features of that object. Our simulations predict that inference under EM-SAIM would produce more errors than PE-SAIM. However, as noted above, this may depend the parameter settings, which would have to be optimized for any given choice behaviour, thereby enabling Bayesian model comparison to ascertain which model is the best account of empirical data.</p><p>Apart from these behavioural assessments, PE-SAIM and EM-SAIM can also generate neuronal responses of the sort measured by EEG or fMRI. Most current methods of measuring neuronal activity are indirect and depend on which physiological process (e.g. dendrite depolarization, axonal firing, haemodynamics, etc.) the respective method (EEG, fMRI, etc.) can measure. To simulate neuronal responses, we omitted the <italic>Contents Network</italic>&#x02014;as its activation depends on &#x02018;pixilated inputs&#x02019;. We summed the output activation and the input activation (as defined by equations (2.11), (2.13), (3.3) and (3.5)) for the <italic>Selection Network</italic> and the <italic>Knowledge Network</italic>. We excluded the activation from the softmax/softmin equations in these calculations. The resulting neuronal response reflects activation in dendritic trees and axons, while ignoring activation of inhibitory interneurons.</p><p><xref ref-type="fig" rid="RSIF20180344F7">Figure&#x000a0;7</xref> shows the resulting time courses of activations for both models. They suggest that it may be possible to distinguish between the two models: for EM-SAIM, the results suggest a reduction in activity in both areas, while for PE-SAIM, they evince an increase. These results may come as a surprise for some readers: given that PE-SAIM tries to minimize prediction error, a reduction in activity might have been expected; while for EM-SAIM, the opposite effect might have been expected. The counterintuitive results with EM-SAIM can be explained relatively easily. The initial state of EM-SAIM uses a weighted combination of templates in the <italic>Knowledge</italic>
<italic>Network <italic>and</italic> Contents Network</italic>. This combined template matches with the two objects in the input image (but the match is better for &#x02018;cross&#x02019; than for &#x02018;two&#x02019;). As the selection process proceeds, this match declines as only the &#x02018;cross&#x02019; in the input is matched&#x02014;and the &#x02018;two&#x02019; template in the <italic>Knowledge Network</italic> ceases to match. The increase in activation in PE-SAIM needs some more detailed unpacking. Initially, the combined template produces a top-down prediction that generates a better match for the &#x02018;cross&#x02019; than the &#x02018;two&#x02019;. The <italic>Selection Network</italic> starts to bias the FOA towards the &#x02018;cross&#x02019;. Subsequently, this bias leads to a mismatch with the top-down prediction leading to an increased activation (i.e. prediction error). As the <italic>Knowledge Network</italic> starts generating the improved prediction&#x02014;by selecting the cross&#x02014;the increase in the prediction error declines in the input of the <italic>Knowledge Network</italic>. However, as the &#x02018;two&#x02019; template produces a non-matching prediction, the overall error does not fall back to zero. A similar effect can be observed for the <italic>Selection Network</italic>. Even though the FOA generates a prediction matching the &#x02018;cross&#x02019; in the input, the mismatch with the &#x02018;two&#x02019; leads to higher activation. These results highlight the complicated nature of evoked responses when both prediction error and attentional selection are in play (see [<xref rid="RSIF20180344C30" ref-type="bibr">30</xref>&#x02013;<xref rid="RSIF20180344C32" ref-type="bibr">32</xref>] for empirical examples in fMRI and EEG).
<fig id="RSIF20180344F7" orientation="portrait" position="float"><label>Figure 7.</label><caption><p>Sum of input and output activation. These results show that the two models predict a qualitatively different time course of neuronal activation (see main text for details).</p></caption><graphic xlink:href="rsif20180344-g7"/></fig></p><p>Other neuroimaging methods to exploit these sorts of simulations empirically could focus on disambiguating between excitatory and disinhibitory responses to top-down afferents. There are a number of candidates that one could consider. First, one could use the laminar specificity of forward and backward (bottom-up and top-down) connections in conjunction with laminar-specific fMRI to make predictions about the neuronal correlates of attentional effects [<xref rid="RSIF20180344C33" ref-type="bibr">33</xref>]. Another approach would be to use frequency tagging to measure attentional effects on steady-state electrophysiological responses (e.g. [<xref rid="RSIF20180344C34" ref-type="bibr">34</xref>]). There are also several examples in the literature that use <italic>dynamic causal modelling</italic> to disambiguate between inhibitory and excitatory connections in cortical hierarchies [<xref rid="RSIF20180344C35" ref-type="bibr">35</xref>&#x02013;<xref rid="RSIF20180344C41" ref-type="bibr">41</xref>]. In brief, dynamic causal modelling entails fitting empirical (usually EEG&#x02014;but see [<xref rid="RSIF20180344C42" ref-type="bibr">42</xref>], for example, using fMRI) data&#x02014;in the form of evoked responses&#x02014;using a neural mass model with lamina-specific coupling [<xref rid="RSIF20180344C35" ref-type="bibr">35</xref>,<xref rid="RSIF20180344C43" ref-type="bibr">43</xref>]. One can then evaluate the evidence for competing architectures by specifying different patterns of connectivity within and between the neural masses that constitute electromagnetic sources (i.e. equivalent current dipoles). After the models have been fitted, the model evidence (i.e. the probability of the empirical data under each model) can be evaluated and used to adjudicate among different architectures. In principle, one could use exactly the same technology to test models that had different time constants&#x02014;as well and different inhibitory or excitatory effects (e.g. [<xref rid="RSIF20180344C35" ref-type="bibr">35</xref>]). This would involve comparing equivalent models with different priors over the synaptic time constants or effective connectivity in question (i.e. the influence of descending or feedback afferents to a primary visual source). In this setting, dynamic causal modelling will also have to consider that PE-SAIM assumes not only feedback loops between regions but also within layers (see the error terms in equations (3.4) and (3.5)). Recent invasive data, addressing the alternative architectures for predictive coding, also offer the intriguing possibility of testing the alternative predictions about the nature of feedback (see [<xref rid="RSIF20180344C44" ref-type="bibr">44</xref>] for an example).</p></sec><sec id="s5"><label>5.</label><title>General discussion</title><p>The aim of the paper was to examine how SAIM's soft constraint satisfaction&#x02014;using energy minimization&#x02014;relates to the free-energy minimization of approximate Bayesian inference. To facilitate this comparison, we first created a new version of SAIM: EM-SAIM includes slightly more biologically plausible features than the original SAIM but crucially, for the purpose of this paper, is based on the same architecture and a formally similar energy function. We then ensured that EM-SAIM can reproduce the multiple object cost. Subsequently, we showed that SAIM's energy minimization can be interpreted in terms of Bayesian inference to a point estimator (i.e. maximum <italic>a posteriori</italic> estimate). We also noted that the ensuing probabilistic inference implements a soft constraint satisfaction, whereby empirical and full priors furnish the requisite constraints. By reverse engineering EM-SAIM's energy function, we showed that EM-SAIM's generative model uses a sparse prior of the sort commonly found in sparse regression models. It is worth noting that this type of prior is employed in methods such as the LASSO regression (e.g. [<xref rid="RSIF20180344C45" ref-type="bibr">45</xref>]) and independent component analysis (e.g. [<xref rid="RSIF20180344C46" ref-type="bibr">46</xref>]). The upshot of using this sort of prior is that it favours sparse representations of data. Furthermore, in EM-SAIM, the WTA forces the representation to become a local representation. Crucially, this generative model differs from the generative models used in predictive coding and related Bayesian filtering formulations of visual processing. These formulations normally employ a generative model based on Gaussian assumptions. Therefore, we replaced the empirical priors in EM-SAIM's architecture with a Gaussian form (i.e. log probabilities that are proportional to squared prediction errors) to show that PE-SAIM is also able to simulate the multiple object cost.</p><p>Our simulations suggest that EM-SAIM and PE-SAIM are quantitatively indistinguishable, in terms of their predictions of behavioural (psychophysical) responses. However, with suitable experimental designs, the two models can be used to model empirical data quantitatively. If this is feasible, Bayesian model comparison should be able to disambiguate the two schemes using recognition accuracy and reaction times (e.g. [<xref rid="RSIF20180344C28" ref-type="bibr">28</xref>,<xref rid="RSIF20180344C29" ref-type="bibr">29</xref>]). We further observed that EM-SAIM and PE-SAIM make quite different predictions about neuronal responses in terms of belief updating. EM-SAIM suggests that excitatory feedback loops mediate the behavioural effects we have illustrated, while PE-SAIM implies inhibitory feedback loops. Hence, these models seem to make distinct predictions about the physiology of feedback connections.</p><p>At first glance, EM-SAIM appears to be more consistent with the well-known physiology of excitatory (glutamatergic) feedback connections in the cortex (e.g. [<xref rid="RSIF20180344C47" ref-type="bibr">47</xref>]). However, these feedback connections target inhibitory interneurons. Hence, it is possible that feedback connections can also mediate the construction of prediction error (see [<xref rid="RSIF20180344C16" ref-type="bibr">16</xref>,<xref rid="RSIF20180344C43" ref-type="bibr">43</xref>,<xref rid="RSIF20180344C48" ref-type="bibr">48</xref>,<xref rid="RSIF20180344C49" ref-type="bibr">49</xref>] for detailed arguments). Therefore, our current knowledge of physiology does not definitively disambiguate the two architectures. On the other hand&#x02014;and as discussed above&#x02014;it may be possible to distinguish between the two architectures empirically; leveraging the fact that the two models make different predictions for excitatory or inhibitory nature of top-down afferents. The two types of feedback motifs may generate different dynamics (with different time constants). It is therefore conceivable that laminar-specific fMRI, dynamic causal modelling or frequency-tagged EEG, in conjunction with Bayesian model comparison, might allow us to disambiguate the two architectures using non-invasive techniques in humans (see [<xref rid="RSIF20180344C50" ref-type="bibr">50</xref>] for a contemporary discussion of empirical predictions for invasive studies). Finally, it is worth noting that both models make different predictions in terms of their preference for familiar versus novel stimuli.<sup><xref ref-type="fn" rid="FN4">4</xref></sup> EM-SAIM would prefer familiar stimuli, while PE-SAIM would prefer novel stimuli (that elicit greater prediction errors). Interestingly, a recent study by Park <italic>et al</italic>. [<xref rid="RSIF20180344C51" ref-type="bibr">51</xref>] found a category-specific (i.e. faces versus natural scenes) preference that could provide an interesting paradigm within which to test the two models.</p><p>The microcircuits for predictive coding motifs in <xref rid="RSIF20180344TB1" ref-type="table">table&#x000a0;1</xref> speak to disinhibition as the physiological mechanism for the effect of descending or backward connections (indicated by the double red lines in <xref rid="RSIF20180344TB1" ref-type="table">table&#x000a0;1</xref>). There is growing interest and evidence for disinhibitory mechanisms of this sort (reviewed in [<xref rid="RSIF20180344C32" ref-type="bibr">32</xref>,<xref rid="RSIF20180344C48" ref-type="bibr">48</xref>,<xref rid="RSIF20180344C50" ref-type="bibr">50</xref>]). This evidence comes in part from recent invasive studies using optogenetic characterizations of inhibitory interneurons. Microcircuit motifs that use disinhibition have been found in several cortical regions [<xref rid="RSIF20180344C52" ref-type="bibr">52</xref>]: in brief, vasoactive intestinal peptide positive (VIP+) interneurons are thought to provide disinhibitory control, by targeting parvalbumin positive (PV+) and somatostatin positive (SOM+) interneurons that otherwise inhibit target excitatory neurons [<xref rid="RSIF20180344C53" ref-type="bibr">53</xref>]. This synaptic architecture is supported by evidence from rodent studies, showing that optogenetic inhibition of SOM+ and PV+ interneurons reduces the inhibitory effect of descending projections to V1 from cingulate cortex. Conversely, optogenetic inhibition of VIP+ interneurons enhances the effect of projections from cingulate cortex [<xref rid="RSIF20180344C54" ref-type="bibr">54</xref>]. In humans, disinhibitory effects can be observed when neocortical GABA is reduced using brain stimulation, both physiologically and functionally [<xref rid="RSIF20180344C55" ref-type="bibr">55</xref>]. In short, the balance of empirical evidence points to the disinhibitory motifs that implied by a PE-SAIM like architecture.</p><p>The dialectic between excitatory and inhibitory feedback has been discussed in the literature at length (see [<xref rid="RSIF20180344C56" ref-type="bibr">56</xref>&#x02013;<xref rid="RSIF20180344C58" ref-type="bibr">58</xref>]). For example, Kersten <italic>et al.</italic> [<xref rid="RSIF20180344C57" ref-type="bibr">57</xref>] have formulated the dichotomy in terms of the &#x02018;shut up&#x02019; versus &#x02018;stop gossiping&#x02019; interpretations of Bayesian object perception. Intuitively, the shut up version corresponds to inhibitory top-down influences that &#x02018;explain away&#x02019; any representations at lower levels to reduce the level of prediction error activity. Conversely, the suppression of activity in lower levels when something can be predicted may be better explained by top-down augmentation of the best representation that suppresses all competing expectations. Sometimes, the dichotomy is motivated by contrasting predictive coding with Grossberg adaptive resonance theory (ART) (e.g. [<xref rid="RSIF20180344C59" ref-type="bibr">59</xref>]; see also Kay &#x00026; Phillips's [<xref rid="RSIF20180344C60" ref-type="bibr">60</xref>] coherence INFOMAX for a similar point; or Bowman <italic>et al</italic>.&#x02019;s [<xref rid="RSIF20180344C61" ref-type="bibr">61</xref>] salience detector). According to ART, the excitatory feedback loop is particularly important in the induction of strong &#x02018;resonance&#x02019; to foster learning. Hence, the ART resembles EM-SAIM's architecture in terms of excitatory feedback.</p><p>Having established how SAIM is related to hierarchical Bayesian inference under the free-energy principle, it is worth returning to SAIM's domain of enquiry, modelling phenomena typically associated with selective visual attention. Predictive coding like formulations of attention introduce an additional variable that has to be optimized; namely, the amplitude of random fluctuations in sensory input&#x02014;or its inverse called &#x02018;precision&#x02019;. This is a key quantity in engineering formulations of predictive coding (e.g. Kalman filtering). In this context, precision corresponds to the Kalman gain; namely, the gain or weight afforded prediction errors during belief updating. Crucially, the precision itself can be predicted. According to Feldman &#x00026; Friston [<xref rid="RSIF20180344C22" ref-type="bibr">22</xref>] and Kanai <italic>et al</italic>. [<xref rid="RSIF20180344C23" ref-type="bibr">23</xref>], attention is realized as optimizing precision. In brief, top-down predictions of precision can select which prediction errors are effectively boosted, such that they have a greater influence on belief updating at higher levels of the hierarchy. This is thought to be the computational homologue of attention in predictive coding. Crucially, the top-down predictions of precision have an excitatory effect&#x02014;in contrast with the inhibitory top-down feedback used to form prediction errors <italic>per se</italic>. When one considers predictions of precision, in the context of predictive coding formulations of attention, one has to consider both excitatory and inhibitory top-down feedback. Crucially, the excitatory top-down influences that mediate precision are modulatory or nonlinear in nature&#x02014;in virtue of the fact that they modulate prediction errors. Interestingly, this speaks to the nonlinearities inherent in PE-SAIM.</p><p>In conclusion, attention is intricately linked with perceptual inference. Interestingly, this assumption is strikingly similar to the influence of SAIM's <italic>Selection Network</italic> using Sigma-pi units. Hence, it should be relatively straightforward to modify PE-SAIM and let the <italic>Selection Network</italic> modulate prediction error rather than the sensory information. We cannot foresee any problems in terms of functionality of this new PE-SAIM and anticipate it should behave in a similar way to the PE-SAIM described above. We will consider the formal relationship between precision and the role of the <italic>Selection Network</italic> in SAIM in a subsequent paper&#x02014;and pursue the implications for the functional anatomy of visual attention.</p></sec></body><back><ack><title>Acknowledgements</title><p>The authors would like to thank Howard Bowman, University of Kent and Ulrik Beierholm, University of Durham for the insightful discussions during the preparation of this paper. We would also like to thank the reviewers for invaluable help with several conceptual and technical issues.</p></ack><fn-group><title>Endnotes</title><fn id="FN1"><label>1</label><p>An intuitive explanation of this component is that its partial derivative &#x02018;removes&#x02019; the integral leaving only the term <inline-formula><mml:math id="IM34"><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. The ensuing link between <italic>x</italic> and <italic>y</italic> turns this term into a leak term: <inline-formula><mml:math id="IM35"><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>.</p></fn><fn id="FN2"><label>2</label><p>Empirical priors are priors that are themselves parametrized by random variables. Empirical priors are part of any hierarchical generative model, with full priors at the highest level.</p></fn><fn id="FN3"><label>3</label><p>This is also true for the fact that EM-SAIM exhibits lower levels of noise than PE-SAIM.</p></fn><fn id="FN4"><label>4</label><p>We would like to thank the second reviewer for this idea.</p></fn></fn-group><sec id="s6"><title>Data accessibility</title><p>The MatLab scripts for the publication can found on Github: <uri xlink:href="https://github.com/saim-models/EMvPE.git">https://github.com/saim-models/EMvPE.git</uri>.</p></sec><sec id="s7"><title>Authors' contributions</title><p>A.K.A. and M.A. helped with the mathematics and commented on drafts. K.Y. and D.H. conducted the simulation studies. K.F. and D.H. conceived the study and drafted the manuscript. All authors gave final approval for publication.</p></sec><sec id="s8" sec-type="COI-statement"><title>Competing interests</title><p>We declare we have no competing interests.</p></sec><sec id="s9"><title>Funding</title><p>K.F. is funded by a Wellcome Trust Principal Research Fellowship (no. 088130/Z/09/Z).</p></sec><app-group><app><title>Appendix A. Parameter values</title><p><table-wrap id="RSIF20180344ATB1" orientation="portrait" position="anchor"><caption><p>EM-SAIM</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">network</th><th align="left" rowspan="1" colspan="1">parameter name</th><th align="left" rowspan="1" colspan="1">value</th></tr></thead><tbody><tr><td rowspan="2" colspan="1"/><td rowspan="1" colspan="1">threshold for reaction time</td><td rowspan="1" colspan="1">0.7</td></tr><tr><td rowspan="1" colspan="1">maximal duration of simulation</td><td rowspan="1" colspan="1">1500</td></tr><tr><td rowspan="6" colspan="1"><italic>Knowledge Network</italic></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM36"><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">1000</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM37"><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">10</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM38"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.1</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM39"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">3.0</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM40"><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">30</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM41"><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">6&#x0200a;&#x000d7;&#x0200a;10<sup>&#x02212;4</sup></td></tr><tr><td rowspan="3" colspan="1"><italic>Contents Network</italic></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM42"><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">600</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM43"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.5</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM44"><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">8&#x0200a;&#x000d7;&#x0200a;10<sup>&#x02212;4</sup></td></tr><tr><td rowspan="5" colspan="1"><italic>Selection Network</italic></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM45"><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">200</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM46"><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">15</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM47"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM48"><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">5</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM49"><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">0.0014</td></tr></tbody></table></table-wrap>
<table-wrap id="RSIF20180344ATB2" orientation="portrait" position="anchor"><caption><p>PE-SAIM</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">network</th><th align="left" rowspan="1" colspan="1">parameter name</th><th align="left" rowspan="1" colspan="1">value</th></tr></thead><tbody><tr><td rowspan="2" colspan="1"/><td rowspan="1" colspan="1">threshold for reaction time</td><td rowspan="1" colspan="1">0.56</td></tr><tr><td rowspan="1" colspan="1">maximal duration of simulation</td><td rowspan="1" colspan="1">2300</td></tr><tr><td rowspan="6" colspan="1"><italic>Knowledge Network</italic></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM50"><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">2000</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM51"><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">20</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM52"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">1.5</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM53"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">8</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM54"><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">50</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM55"><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">KN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">7&#x0200a;&#x000d7;&#x0200a;10<sup>&#x02212;4</sup></td></tr><tr><td rowspan="3" colspan="1"><italic>Contents Network</italic></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM56"><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">500</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM57"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">4</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM58"><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">5 &#x000d7;&#x0200a;10<sup>&#x02212;4</sup></td></tr><tr><td rowspan="5" colspan="1"><italic>Selection Network</italic></td><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM59"><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">5000</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM60"><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM61"><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">5</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM62"><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">100</td></tr><tr><td rowspan="1" colspan="1"><inline-formula><mml:math id="IM63"><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SN</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula></td><td rowspan="1" colspan="1">2.86&#x0200a;&#x000d7;&#x0200a;10<sup>&#x02212;4</sup></td></tr></tbody></table></table-wrap></p></app></app-group><ref-list><title>References</title><ref id="RSIF20180344C1"><label>1</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Humphreys</surname><given-names>GW</given-names></name></person-group>
<year>2003</year>
<article-title>Computational models of visual selective attention: a review</article-title>. In <source>Connectionist models in cognitive psychology (studies in cognition)</source> (ed. <person-group person-group-type="editor"><name name-style="western"><surname>Houghton</surname><given-names>G</given-names></name></person-group>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Psychology Press</publisher-name>.</mixed-citation></ref><ref id="RSIF20180344C2"><label>2</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Rumelhart</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name></person-group>
<year>1988</year>
<source>Parallel distributed processing: explorations in the microstructure of cognition, Vol. 1 foundations</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref><ref id="RSIF20180344C3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Backhaus</surname><given-names>A</given-names></name></person-group>
<year>2011</year>
<article-title>Modelling visual search with the selective attention for identification model (VS-SAIM): a novel explanation for visual search asymmetries</article-title>. <source>Cogn. Comput.</source>
<volume>1</volume>, <fpage>185</fpage>&#x02013;<lpage>205</lpage>. (<pub-id pub-id-type="doi">10.1007/s12559-010-9076-x</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mavritsaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Allen</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Deco</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Humphreys</surname><given-names>GW</given-names></name></person-group>
<year>2011</year>
<article-title>Bridging the gap between physiology and behavior: evidence from the sSoTS model of human visual attention</article-title>. <source>Psychol. Rev.</source>
<volume>118</volume>, <fpage>3</fpage>&#x02013;<lpage>41</lpage>. (<pub-id pub-id-type="doi">10.1037/a0021868</pub-id>)<pub-id pub-id-type="pmid">21244184</pub-id></mixed-citation></ref><ref id="RSIF20180344C5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Narbutas</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>Y-S</given-names></name>, <name name-style="western"><surname>Kristan</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name></person-group>
<year>2017</year>
<article-title>Serial versus parallel search: a model comparison approach based on reaction time distributions</article-title>. <source>Vis. Cogn.</source>
<volume>25</volume>, <fpage>306</fpage>&#x02013;<lpage>325</lpage>. (<pub-id pub-id-type="doi">10.1080/13506285.2017.1352055</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C6"><label>6</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Backhaus</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Humphreys</surname><given-names>GW</given-names></name></person-group>
<year>2007</year>
<article-title>The selective attention for identification model (SAIM): simulating visual search in natural colour images</article-title>. In <source>Attention in cognitive systems. Theories and systems from an interdisciplinary viewpoint</source> (eds <person-group person-group-type="editor"><name name-style="western"><surname>Paletta</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Rome</surname><given-names>E</given-names></name></person-group>). <comment>Lecture Notes in Computer Science, vol. 4840, pp. 141&#x02013;154.</comment>
<publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name> (<pub-id pub-id-type="doi">10.1007/978-3-540-77343-6_9</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C7"><label>7</label><mixed-citation publication-type="conf-paper"><person-group person-group-type="author"><name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Humphreys</surname><given-names>GW</given-names></name></person-group>
<year>2004</year>
<article-title>Modelling grouping through interactions between top-down and bottom-up processes: the grouping and selective attention for identification model (G-SAIM)</article-title>. In <comment><italic>Attention and performance in computational vision</italic> (eds L Paletta, JK Tsotsos, E Rome). Lecture Notes in Computer Science, vol. 3368, pp. 148&#x02013;158. Berlin, Germany: Springer.</comment> (<pub-id pub-id-type="doi">10.1007/978-3-540-30572-9_11</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C8"><label>8</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>B&#x000f6;hme</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name></person-group>
<year>2009</year>
<article-title>Modeling visual affordances: the selective attention for action model (SAAM)</article-title>. In <source>Connectionist models of behaviour and cognition II</source> (eds <person-group person-group-type="editor"><name name-style="western"><surname>Mayor</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ruh</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Plunkett</surname><given-names>K</given-names></name></person-group>), pp. <fpage>325</fpage>&#x02013;<lpage>336</lpage>. <publisher-loc>Singapore</publisher-loc>: <publisher-name>World Scientific</publisher-name>.</mixed-citation></ref><ref id="RSIF20180344C9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zibner</surname><given-names>SKU</given-names></name>, <name name-style="western"><surname>Faubel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Iossifidis</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Sch&#x000f6;ner</surname><given-names>G</given-names></name></person-group>
<year>2011</year>
<article-title>Dynamic neural fields as building blocks of a cortex-inspired architecture for robotic scene representation</article-title>. <source>IEEE Trans. Auton. Ment. Dev.</source>
<volume>3</volume>, <fpage>74</fpage>&#x02013;<lpage>91</lpage>. (<pub-id pub-id-type="doi">10.1109/TAMD.2011.2109714</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sandamirskaya</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Zibner</surname><given-names>SK</given-names></name>, <name name-style="western"><surname>Schneegans</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sch&#x000f6;ner</surname><given-names>G</given-names></name></person-group>
<year>2013</year>
<article-title>Using dynamic field theory to extend the embodiment stance toward higher cognition</article-title>. <source>New Ideas Psychol.</source>
<volume>31</volume>, <fpage>322</fpage>&#x02013;<lpage>339</lpage>. (<pub-id pub-id-type="doi">10.1016/j.newideapsych.2013.01.002</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Strauss</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Woodgate</surname><given-names>PJW</given-names></name>, <name name-style="western"><surname>Sami</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name></person-group>
<year>2015</year>
<article-title>Choice reaching with a LEGO arm robot (CoRLEGO): the motor system guides visual attention to movement-relevant information</article-title>. <source>Neural Netw.</source>
<volume>72</volume>, <fpage>3</fpage>&#x02013;<lpage>12</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neunet.2015.10.005</pub-id>)<pub-id pub-id-type="pmid">26667353</pub-id></mixed-citation></ref><ref id="RSIF20180344C12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Faubel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sch&#x000f6;ner</surname><given-names>G</given-names></name></person-group>
<year>2008</year>
<article-title>Learning to recognise objects on the fly: a neurally based dynamic field approach</article-title>. <source>Neural Netw.</source>
<volume>21</volume>, <fpage>562</fpage>&#x02013;<lpage>576</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neunet.2008.03.007</pub-id>)<pub-id pub-id-type="pmid">18501555</pub-id></mixed-citation></ref><ref id="RSIF20180344C13"><label>13</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Faubel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sch&#x000f6;ner</surname><given-names>G</given-names></name></person-group>
<year>2009</year>
<article-title>A neuro-dynamic architecture for one shot learning of objects that uses both bottom-up recognition and top-down prediction</article-title>. In <conf-name>2009 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, St Louis, MO, USA, 10&#x02013;15 October 2009</conf-name>, pp. <fpage>3162</fpage>&#x02013;<lpage>3169</lpage>. (<pub-id pub-id-type="doi">10.1109/IROS.2009.5354380</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>W</given-names></name></person-group>
<year>2007</year>
<article-title>Variational free energy and the Laplace approximation</article-title>. <source>Neuroimage</source>
<volume>34</volume>, <fpage>220</fpage>&#x02013;<lpage>234</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.08.035</pub-id>)<pub-id pub-id-type="pmid">17055746</pub-id></mixed-citation></ref><ref id="RSIF20180344C15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kilner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name></person-group>
<year>2006</year>
<article-title>A free energy principle for the brain</article-title>. <source>J. Physiol. Paris</source>
<volume>100</volume>, <fpage>70</fpage>&#x02013;<lpage>87</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jphysparis.2006.10.001</pub-id>)<pub-id pub-id-type="pmid">17097864</pub-id></mixed-citation></ref><ref id="RSIF20180344C16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>
<year>2008</year>
<article-title>Hierarchical models in the brain</article-title>. <source>PLoS Comput. Biol.</source>
<volume>4</volume>, <fpage>e1000211</fpage> (<pub-id pub-id-type="doi">10.1371/journal.pcbi.1000211</pub-id>)<pub-id pub-id-type="pmid">18989391</pub-id></mixed-citation></ref><ref id="RSIF20180344C17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>
<year>2010</year>
<article-title>The free-energy principle: a unified brain theory?</article-title>
<source>Nat. Rev. Neurosci.</source>
<volume>11</volume>, <fpage>127</fpage>&#x02013;<lpage>138</lpage>. (<pub-id pub-id-type="doi">10.1038/nrn2787</pub-id>)<pub-id pub-id-type="pmid">20068583</pub-id></mixed-citation></ref><ref id="RSIF20180344C18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Clark</surname><given-names>A</given-names></name></person-group>
<year>2013</year>
<article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title>. <source>Behav. Brain Sci.</source>
<volume>36</volume>, <fpage>181</fpage>&#x02013;<lpage>204</lpage>. (<pub-id pub-id-type="doi">10.1017/S0140525X12000477</pub-id>)<pub-id pub-id-type="pmid">23663408</pub-id></mixed-citation></ref><ref id="RSIF20180344C19"><label>19</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hohwy</surname><given-names>J</given-names></name></person-group>
<year>2013</year>
<source>The predictive mind</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref><ref id="RSIF20180344C20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Heinke</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Humphreys</surname><given-names>GW</given-names></name></person-group>
<year>2015</year>
<article-title>Modeling visual search using three-parameter probability functions in a hierarchical Bayesian framework</article-title>. <source>Attent. Percept. Psychophys.</source>
<volume>77</volume>, <fpage>985</fpage>&#x02013;<lpage>1010</lpage>. (<pub-id pub-id-type="doi">10.3758/s13414-014-0825-x</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Eckstein</surname><given-names>MP</given-names></name></person-group>
<year>2011</year>
<article-title>Visual search: a retrospective</article-title>. <source>J. Vis.</source>
<volume>11</volume>, <fpage>1</fpage>&#x02013;<lpage>36</lpage>.</mixed-citation></ref><ref id="RSIF20180344C22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Feldman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>
<year>2010</year>
<article-title>Attention, uncertainty, and free-energy</article-title>. <source>Front. Hum. Neurosci.</source>
<volume>4</volume>, <fpage>215</fpage> (<pub-id pub-id-type="doi">10.3389/fnhum.2010.00215</pub-id>)<pub-id pub-id-type="pmid">21160551</pub-id></mixed-citation></ref><ref id="RSIF20180344C23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kanai</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Komura</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Shipp</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>
<year>2015</year>
<article-title>Cerebral hierarchies: predictive processing, precision and the pulvinar</article-title>. <source>Phil. Trans. R. Soc. B</source>
<volume>370</volume>, <fpage>20140169</fpage> (<pub-id pub-id-type="doi">10.1098/rstb.2014.0169</pub-id>)<pub-id pub-id-type="pmid">25823866</pub-id></mixed-citation></ref><ref id="RSIF20180344C24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name></person-group>
<year>1986</year>
<article-title>Computing with neural circuits: a model</article-title>. <source>Science</source>
<volume>233</volume>, <fpage>625</fpage>&#x02013;<lpage>633</lpage>. (<pub-id pub-id-type="doi">10.1126/science.3755256</pub-id>)<pub-id pub-id-type="pmid">3755256</pub-id></mixed-citation></ref><ref id="RSIF20180344C25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ratcliff</surname><given-names>R</given-names></name>, <name name-style="western"><surname>McKoon</surname><given-names>G</given-names></name></person-group>
<year>2008</year>
<article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title>. <source>Neural Comput.</source>
<volume>20</volume>, <fpage>873</fpage>&#x02013;<lpage>922</lpage>. (<pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id>)<pub-id pub-id-type="pmid">18085991</pub-id></mixed-citation></ref><ref id="RSIF20180344C26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lamme</surname><given-names>VAF</given-names></name>, <name name-style="western"><surname>Sup&#x000e8;r</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Spekreijse</surname><given-names>H</given-names></name></person-group>
<year>1998</year>
<article-title>Feedforward, horizontal, and feedback processing in the visual cortex</article-title>. <source>Curr. Opin. Neurobiol.</source>
<volume>8</volume>, <fpage>529</fpage>&#x02013;<lpage>535</lpage>. (<pub-id pub-id-type="doi">10.1016/S0959-4388(98)80042-1</pub-id>)<pub-id pub-id-type="pmid">9751656</pub-id></mixed-citation></ref><ref id="RSIF20180344C27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Knill</surname><given-names>DC</given-names></name>, <name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>
<year>2004</year>
<article-title>The Bayesian brain: the role of uncertainty in neural coding and computation</article-title>. <source>Trends Neurosci.</source>
<volume>27</volume>, <fpage>712</fpage>&#x02013;<lpage>719</lpage>. (<pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id>)<pub-id pub-id-type="pmid">15541511</pub-id></mixed-citation></ref><ref id="RSIF20180344C28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kass</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Raftery</surname><given-names>AE</given-names></name></person-group>
<year>1995</year>
<article-title>Bayes factors</article-title>. <source>J. Am. Stat. Assoc.</source>
<volume>90</volume>, <fpage>773</fpage>&#x02013;<lpage>795</lpage>. (<pub-id pub-id-type="doi">10.1080/01621459.1995.10476572</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C29"><label>29</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bishop</surname><given-names>CM</given-names></name></person-group>
<year>2006</year>
<source>Pattern recognition and machine learning</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation></ref><ref id="RSIF20180344C30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kok</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Rahnev</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Jehee</surname><given-names>JFM</given-names></name>, <name name-style="western"><surname>Lau</surname><given-names>HC</given-names></name>, <name name-style="western"><surname>de Lange</surname><given-names>FP</given-names></name></person-group>
<year>2012</year>
<article-title>Attention reverses the effect of prediction in silencing sensory signals</article-title>. <source>Cereb. Cortex</source>
<volume>22</volume>, <fpage>2197</fpage>&#x02013;<lpage>2206</lpage>. (<pub-id pub-id-type="doi">10.1093/cercor/bhr310</pub-id>)<pub-id pub-id-type="pmid">22047964</pub-id></mixed-citation></ref><ref id="RSIF20180344C31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kok</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Jehee</surname><given-names>JFM</given-names></name>, <name name-style="western"><surname>de Lange</surname><given-names>FP</given-names></name></person-group>
<year>2012</year>
<article-title>Less is more: expectation sharpens representations in the primary visual cortex</article-title>. <source>Neuron</source>
<volume>75</volume>, <fpage>265</fpage>&#x02013;<lpage>270</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.034</pub-id>)<pub-id pub-id-type="pmid">22841311</pub-id></mixed-citation></ref><ref id="RSIF20180344C32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Auksztulewicz</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>
<year>2015</year>
<article-title>Attentional enhancement of auditory mismatch responses: a DCM/MEG study</article-title>. <source>Cereb. Cortex</source>
<volume>25</volume>, <fpage>4273</fpage>&#x02013;<lpage>4283</lpage>. (<pub-id pub-id-type="doi">10.1093/cercor/bhu323</pub-id>)<pub-id pub-id-type="pmid">25596591</pub-id></mixed-citation></ref><ref id="RSIF20180344C33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lawrence</surname><given-names>SJD</given-names></name>, <name name-style="western"><surname>Formisano</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Muckli</surname><given-names>L</given-names></name>, <name name-style="western"><surname>de Lange</surname><given-names>FP</given-names></name></person-group>
<comment>In press.</comment>
<article-title>Laminar fMRI: applications for cognitive neuroscience</article-title>. <source>Neuroimage</source>. (<pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.07.004</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Colon</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Legrain</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Mouraux</surname><given-names>A</given-names></name></person-group>
<year>2014</year>
<article-title>EEG frequency-tagging to dissociate the cortical responses to nociceptive and non-nociceptive stimuli</article-title>. <source>J. Cogn. Neurosci.</source>
<volume>26</volume>, <fpage>2262</fpage>&#x02013;<lpage>2274</lpage>. (<pub-id pub-id-type="doi">10.1162/jocn_a_00648</pub-id>)<pub-id pub-id-type="pmid">24738772</pub-id></mixed-citation></ref><ref id="RSIF20180344C35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bastos</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Litvak</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Moran</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bosman</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>
<year>2015</year>
<article-title>A DCM study of spectral asymmetries in feedforward and feedback connections between visual areas V1 and V4 in the monkey</article-title>. <source>Neuroimage</source>
<volume>108</volume>, <fpage>460</fpage>&#x02013;<lpage>475</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.12.081</pub-id>)<pub-id pub-id-type="pmid">25585017</pub-id></mixed-citation></ref><ref id="RSIF20180344C36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Boly</surname><given-names>M</given-names></name><etal>et al.</etal></person-group>
<year>2011</year>
<article-title>Preserved feedforward but impaired top-down processes in the vegetative state</article-title>. <source>Science</source>
<volume>332</volume>, <fpage>858</fpage>&#x02013;<lpage>862</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1202043</pub-id>)<pub-id pub-id-type="pmid">21566197</pub-id></mixed-citation></ref><ref id="RSIF20180344C37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>
<year>2012</year>
<article-title>Dynamic causal modelling of precision and synaptic gain in visual perception&#x02014;an EEG study</article-title>. <source>Neuroimage</source>
<volume>63</volume>, <fpage>223</fpage>&#x02013;<lpage>231</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.06.044</pub-id>)<pub-id pub-id-type="pmid">22750569</pub-id></mixed-citation></ref><ref id="RSIF20180344C38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>
<year>2012</year>
<article-title>Free-energy and illusions: the cornsweet effect</article-title>. <source>Front. Psychol.</source>
<volume>3</volume>, <fpage>43</fpage> (<pub-id pub-id-type="doi">10.3389/fpsyg.2012.00043</pub-id>)<pub-id pub-id-type="pmid">22393327</pub-id></mixed-citation></ref><ref id="RSIF20180344C39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fogelson</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Litvak</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Peled</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Fernandez-del-Olmo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>
<year>2014</year>
<article-title>The functional anatomy of schizophrenia: a dynamic causal modeling study of predictive coding</article-title>. <source>Schizophrenia Res.</source>
<volume>158</volume>, <fpage>204</fpage>&#x02013;<lpage>212</lpage>. (<pub-id pub-id-type="doi">10.1016/j.schres.2014.06.011</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moran</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Blockeel</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Adams</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>
<year>2015</year>
<article-title>Losing control under ketamine: suppressed cortico-hippocampal drive following acute ketamine in rats</article-title>. <source>Neuropsychopharmacology</source>
<volume>40</volume>, <fpage>268</fpage>&#x02013;<lpage>277</lpage>. (<pub-id pub-id-type="doi">10.1038/npp.2014.184</pub-id>)<pub-id pub-id-type="pmid">25053181</pub-id></mixed-citation></ref><ref id="RSIF20180344C41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pinotsis</surname><given-names>DA</given-names></name><etal>et al.</etal></person-group>
<year>2014</year>
<article-title>Contrast gain control and horizontal interactions in V1: a DCM study</article-title>. <source>Neuroimage</source>
<volume>92</volume>, <fpage>143</fpage>&#x02013;<lpage>155</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.01.047</pub-id>)<pub-id pub-id-type="pmid">24495812</pub-id></mixed-citation></ref><ref id="RSIF20180344C42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>K</given-names></name></person-group>
<year>2011</year>
<article-title>Network discovery with DCM</article-title>. <source>Neuroimage</source>
<volume>56</volume>, <fpage>1202</fpage>&#x02013;<lpage>1221</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.12.039</pub-id>)<pub-id pub-id-type="pmid">21182971</pub-id></mixed-citation></ref><ref id="RSIF20180344C43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bastos</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Usrey</surname><given-names>WM</given-names></name>, <name name-style="western"><surname>Adams</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Mangun</surname><given-names>GR</given-names></name>, <name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>
<year>2012</year>
<article-title>Canonical microcircuits for predictive coding</article-title>. <source>Neuron</source>
<volume>76</volume>, <fpage>695</fpage>&#x02013;<lpage>711</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id>)<pub-id pub-id-type="pmid">23177956</pub-id></mixed-citation></ref><ref id="RSIF20180344C44"><label>44</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schwiedrzik</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Freiwald</surname><given-names>WA</given-names></name></person-group>
<year>2017</year>
<article-title>High-level prediction signals in a low-level area of the macaque face-processing hierarchy</article-title>. <source>Neuron</source>
<volume>96</volume>, <fpage>89</fpage>&#x02013;<lpage>97</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.007</pub-id>)<pub-id pub-id-type="pmid">28957679</pub-id></mixed-citation></ref><ref id="RSIF20180344C45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name></person-group>
<year>1996</year>
<article-title>Regression shrinkage and selection via the lasso</article-title>. <source>J. R. Stat. Soc. Ser. B</source>
<volume>58</volume>, <fpage>267</fpage>&#x02013;<lpage>288</lpage>.</mixed-citation></ref><ref id="RSIF20180344C46"><label>46</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bell</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>
<year>1995</year>
<article-title>An information maximisation approach to blind separation and blind de-convolution</article-title>. <source>Neural Comput.</source>
<volume>7</volume>, <fpage>1129</fpage>&#x02013;<lpage>1159</lpage>. (<pub-id pub-id-type="doi">10.1162/neco.1995.7.6.1129</pub-id>)<pub-id pub-id-type="pmid">7584893</pub-id></mixed-citation></ref><ref id="RSIF20180344C47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sherman</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Guillery</surname><given-names>RW</given-names></name></person-group>
<year>1998</year>
<article-title>On the actions that one nerve cell can have on another: distinguishing &#x02018;drivers&#x02019; from &#x02018;modulators&#x02019;</article-title>. <source>Proc. Natl Acad. Sci. USA</source>
<volume>95</volume>, <fpage>7121</fpage>&#x02013;<lpage>7126</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.95.12.7121</pub-id>)<pub-id pub-id-type="pmid">9618549</pub-id></mixed-citation></ref><ref id="RSIF20180344C48"><label>48</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shipp</surname><given-names>S</given-names></name></person-group>
<year>2016</year>
<article-title>Neural elements for predictive coding</article-title>. <source>Front. Psychol.</source>
<volume>7</volume>, <fpage>1792</fpage> (<pub-id pub-id-type="doi">10.3389/fpsyg.2016.01792</pub-id>)<pub-id pub-id-type="pmid">27917138</pub-id></mixed-citation></ref><ref id="RSIF20180344C49"><label>49</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shipp</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Adams</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>
<year>2013</year>
<article-title>Reflections on agranular architecture: predictive coding in the motor cortex</article-title>. <source>Trends Neurosci.</source>
<volume>36</volume>, <fpage>706</fpage>&#x02013;<lpage>716</lpage>. (<pub-id pub-id-type="doi">10.1016/j.tins.2013.09.004</pub-id>)<pub-id pub-id-type="pmid">24157198</pub-id></mixed-citation></ref><ref id="RSIF20180344C50"><label>50</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Keller</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group>
<year>2018</year>
<article-title>Predictive processing: a canonical cortical computation</article-title>. <source>Neuron</source>
<volume>100</volume>, <fpage>424</fpage>&#x02013;<lpage>435</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.003</pub-id>)<pub-id pub-id-type="pmid">30359606</pub-id></mixed-citation></ref><ref id="RSIF20180344C51"><label>51</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shimojo</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Shimojo</surname><given-names>S</given-names></name></person-group>
<year>2010</year>
<article-title>Roles of familiarity and novelty in visual preference judgments are segregated across object categories</article-title>. <source>Proc. Natl Acad. Sci. USA</source>
<volume>107</volume>, <fpage>14 552</fpage>&#x02013;<lpage>14 555</lpage>. (<pub-id pub-id-type="doi">10.1073/pnas.1004374107</pub-id>)</mixed-citation></ref><ref id="RSIF20180344C52"><label>52</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Letzkus</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Wolff</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Luthi</surname><given-names>A</given-names></name></person-group>
<year>2015</year>
<article-title>Disinhibition, a circuit mechanism for associative learning and memory</article-title>. <source>Neuron</source>
<volume>88</volume>, <fpage>264</fpage>&#x02013;<lpage>276</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.024</pub-id>)<pub-id pub-id-type="pmid">26494276</pub-id></mixed-citation></ref><ref id="RSIF20180344C53"><label>53</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pi</surname><given-names>HJ</given-names></name>, <name name-style="western"><surname>Hangya</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Kvitsiani</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Sanders</surname><given-names>JI</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>ZJ</given-names></name>, <name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name></person-group>
<year>2013</year>
<article-title>Cortical interneurons that specialize in disinhibitory control</article-title>. <source>Nature</source>
<volume>503</volume>, <fpage>521</fpage>&#x02013;<lpage>524</lpage>. (<pub-id pub-id-type="doi">10.1038/nature12676</pub-id>)<pub-id pub-id-type="pmid">24097352</pub-id></mixed-citation></ref><ref id="RSIF20180344C54"><label>54</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kamigaki</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hoang Do</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>WC</given-names></name>, <name name-style="western"><surname>Jenvay</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Miyamichi</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Luo</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>
<year>2014</year>
<article-title>Selective attention. Long-range and local circuits for top-down modulation of visual cortex processing</article-title>. <source>Science</source>
<volume>345</volume>, <fpage>660</fpage>&#x02013;<lpage>665</lpage>. (<pub-id pub-id-type="doi">10.1126/science.1254126</pub-id>)<pub-id pub-id-type="pmid">25104383</pub-id></mixed-citation></ref><ref id="RSIF20180344C55"><label>55</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koolschijn</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Emir</surname><given-names>UE</given-names></name>, <name name-style="western"><surname>Pantelides</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Nili</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Behrens</surname><given-names>TEJ</given-names></name>, <name name-style="western"><surname>Barron</surname><given-names>HC</given-names></name></person-group>
<year>2019</year>
<article-title>The hippocampus and neocortical inhibitory engrams protect against memory interference</article-title>. <source>Neuron</source>
<volume>101</volume>, <fpage>528</fpage>&#x02013;<lpage>541</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neuron.2018.11.042</pub-id>)<pub-id pub-id-type="pmid">30581011</pub-id></mixed-citation></ref><ref id="RSIF20180344C56"><label>56</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petro</surname><given-names>LS</given-names></name>, <name name-style="western"><surname>Muckli</surname><given-names>L</given-names></name></person-group>
<year>2017</year>
<article-title>The laminar integration of sensory inputs with feedback signals in human cortex</article-title>. <source>Brain Cogn.</source>
<volume>112</volume>, <fpage>54</fpage>&#x02013;<lpage>57</lpage>. (<pub-id pub-id-type="doi">10.1016/j.bandc.2016.06.007</pub-id>)<pub-id pub-id-type="pmid">27814926</pub-id></mixed-citation></ref><ref id="RSIF20180344C57"><label>57</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kersten</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Mamassian</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Yuille</surname><given-names>A</given-names></name></person-group>
<year>2004</year>
<article-title>Object perception as Bayesian inference</article-title>. <source>Annu. Rev. Psychol.</source>
<volume>55</volume>, <fpage>271</fpage>&#x02013;<lpage>304</lpage>. (<pub-id pub-id-type="doi">10.1146/annurev.psych.55.090902.142005</pub-id>)<pub-id pub-id-type="pmid">14744217</pub-id></mixed-citation></ref><ref id="RSIF20180344C58"><label>58</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kogo</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Trengove</surname><given-names>C</given-names></name></person-group>
<year>2015</year>
<article-title>Is predictive coding theory articulated enough to be testable?</article-title>
<source>Front. Comput. Neurosci.</source>
<volume>9</volume>, <fpage>111</fpage> (<pub-id pub-id-type="doi">10.3389/fncom.2015.00111</pub-id>)<pub-id pub-id-type="pmid">26441621</pub-id></mixed-citation></ref><ref id="RSIF20180344C59"><label>59</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name></person-group>
<year>2013</year>
<article-title>Adaptive resonance theory: how a brain learns to consciously attend, learn, and recognize a changing world</article-title>. <source>Neural Netw.</source>
<volume>37</volume>, <fpage>1</fpage>&#x02013;<lpage>47</lpage>. (<pub-id pub-id-type="doi">10.1016/j.neunet.2012.09.017</pub-id>)<pub-id pub-id-type="pmid">23149242</pub-id></mixed-citation></ref><ref id="RSIF20180344C60"><label>60</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kay</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Phillips</surname><given-names>WA</given-names></name></person-group>
<year>2011</year>
<article-title>Coherent infomax as a computational goal for neural systems</article-title>. <source>Bull. Math. Biol.</source>
<volume>73</volume>, <fpage>344</fpage> (<pub-id pub-id-type="doi">10.1007/s11538-010-9564-x</pub-id>)<pub-id pub-id-type="pmid">20821064</pub-id></mixed-citation></ref><ref id="RSIF20180344C61"><label>61</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bowman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Filetti</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wyble</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Olivers</surname><given-names>C</given-names></name></person-group>
<year>2013</year>
<article-title>Attention is more than prediction precision</article-title>. <source>Behav. Brain Sci.</source>
<volume>36</volume>, <fpage>206</fpage>&#x02013;<lpage>208</lpage>. (<pub-id pub-id-type="doi">10.1017/S0140525X12002324</pub-id>)<pub-id pub-id-type="pmid">23663435</pub-id></mixed-citation></ref></ref-list></back></article>