<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="review-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Philos Trans A Math Phys Eng Sci</journal-id><journal-id journal-id-type="iso-abbrev">Philos Trans A Math Phys Eng Sci</journal-id><journal-id journal-id-type="publisher-id">RSTA</journal-id><journal-id journal-id-type="hwp">roypta</journal-id><journal-title-group><journal-title>Philosophical transactions. Series A, Mathematical, physical, and engineering sciences</journal-title></journal-title-group><issn pub-type="ppub">1364-503X</issn><issn pub-type="epub">1471-2962</issn><publisher><publisher-name>The Royal Society Publishing</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6388008</article-id><article-id pub-id-type="doi">10.1098/rsta.2018.0144</article-id><article-id pub-id-type="publisher-id">rsta20180144</article-id><article-categories><subj-group subj-group-type="hwp-journal-coll"><subject>1003</subject><subject>50</subject></subj-group><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group><subj-group subj-group-type="leader"><subject>Opinion Piece</subject></subj-group></article-categories><title-group><article-title>Multiscale computing for science and engineering in the era of exascale performance</article-title><alt-title alt-title-type="short">Multiscale Computing on the Exascale</alt-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-3955-2449</contrib-id><name><surname>Hoekstra</surname><given-names>Alfons G.</given-names></name><xref ref-type="aff" rid="af1">1</xref><xref ref-type="aff" rid="af2">2</xref><xref ref-type="corresp" rid="cor1"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-6638-0945</contrib-id><name><surname>Chopard</surname><given-names>Bastien</given-names></name><xref ref-type="aff" rid="af3">3</xref></contrib><contrib contrib-type="author"><name><surname>Coster</surname><given-names>David</given-names></name><xref ref-type="aff" rid="af4">4</xref></contrib><contrib contrib-type="author"><name><surname>Portegies Zwart</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="af5">5</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8787-7256</contrib-id><name><surname>Coveney</surname><given-names>Peter V.</given-names></name><xref ref-type="aff" rid="af6">6</xref></contrib></contrib-group><aff id="af1"><label>1</label><addr-line>Computational Science Laboratory</addr-line>, <addr-line>Institute for Informatics</addr-line>, <addr-line>Faculty of Science</addr-line>, <institution>University of Amsterdam</institution>, <country>The Netherlands</country></aff><aff id="af2"><label>2</label><addr-line>High Performance Computing Department</addr-line>, <institution>ITMO University</institution>, <addr-line>St Petersburg</addr-line>, <country>Russia</country></aff><aff id="af3"><label>3</label><addr-line>Department of Computer Science</addr-line>, <institution>University of Geneva</institution>, <country>Switzerland</country></aff><aff id="af4"><label>4</label><institution>Institute for Plasma Physics</institution>, <addr-line>Garching</addr-line>, <country>Germany</country></aff><aff id="af5"><label>5</label><addr-line>Leiden Observatory</addr-line>, <institution>Leiden University</institution>, <country>The Netherlands</country></aff><aff id="af6"><label>6</label><addr-line>The Centre for Computational Science</addr-line>, <addr-line>Department of Chemistry</addr-line>, <institution>University College London</institution>, <country>UK</country></aff><author-notes><corresp id="cor1">e-mail: <email>a.g.hoekstra@uva.nl</email></corresp><fn fn-type="other"><p>One contribution of 11 to a theme issue &#x02018;<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsta/377/2142">Multiscale modelling, simulation and computing: from the desktop to the exascale</ext-link>&#x02019;.</p></fn></author-notes><pub-date pub-type="ppub"><day>8</day><month>4</month><year>2019</year></pub-date><pub-date pub-type="epub"><day>18</day><month>2</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>18</day><month>2</month><year>2019</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>377</volume><issue>2142</issue><issue-title>Theme issue &#x02018;Multiscale modelling, simulation and computing: from the desktop to the exascale&#x02019; compiled and edited by Alfons G. Hoekstra, Simon Portegies Zwart and Peter Coveney</issue-title><elocation-id>20180144</elocation-id><history><date date-type="accepted"><day>9</day><month>10</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; 2019 The Authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>Published by the Royal Society under the terms of the Creative Commons Attribution License <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>, which permits unrestricted use, provided the original author and source are credited.</license-p></license><?release-delay 0|0?></permissions><self-uri content-type="pdf" xlink:href="rsta20180144.pdf"/><abstract><p>In this position paper, we discuss two relevant topics: (i) generic multiscale computing on emerging exascale high-performing computing environments, and (ii) the scaling of such applications towards the exascale. We will introduce the different phases when developing a multiscale model and simulating it on available computing infrastructure, and argue that we could rely on it both on the conceptual modelling level and also when actually executing the multiscale simulation, and maybe should further develop generic frameworks and software tools to facilitate multiscale computing. Next, we focus on simulating multiscale models on high-end computing resources in the face of emerging exascale performance levels. We will argue that although applications could scale to exascale performance relying on weak scaling and maybe even on strong scaling, there are also clear arguments that such scaling may no longer apply for many applications on these emerging exascale machines and that we need to resort to what we would call <italic>multi-scaling</italic>.</p><p>This article is part of the theme issue &#x02018;Multiscale modelling, simulation and computing: from the desktop to the exascale&#x02019;.</p></abstract><kwd-group><kwd>multiscale modelling and simulation</kwd><kwd>multiscale computing</kwd><kwd>exascale</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>European Union Horizon 2020 research and innovation programme</institution></institution-wrap></funding-source><award-id>671564, 671351 and 800925</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>cover-date</meta-name><meta-value>April 8, 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><label>1.</label><title>Introduction</title><p>In science, our goal is to convincingly explain the processes at work in phenomena that we observe, as well as to predict what will occur before it does so. Predictions of real-world events all need substantial quantities of data and validated computational models together with the execution of many high-fidelity simulations. In many cases, the models that describe the phenomena are multiscale, as their accuracy and reliability depend on the correct representation of processes taking place on several length and time scales. Multiscale phenomena are everywhere around us [<xref rid="RSTA20180144C1" ref-type="bibr">1</xref>&#x02013;<xref rid="RSTA20180144C7" ref-type="bibr">7</xref>]. If we study the origin and evolution of the Universe [<xref rid="RSTA20180144C8" ref-type="bibr">8</xref>,<xref rid="RSTA20180144C9" ref-type="bibr">9</xref>] or properties of materials [<xref rid="RSTA20180144C10" ref-type="bibr">10</xref>&#x02013;<xref rid="RSTA20180144C14" ref-type="bibr">14</xref>], if we try to understand health and disease [<xref rid="RSTA20180144C3" ref-type="bibr">3</xref>,<xref rid="RSTA20180144C15" ref-type="bibr">15</xref>&#x02013;<xref rid="RSTA20180144C22" ref-type="bibr">22</xref>] or develop fusion as a potential energy source of the future [<xref rid="RSTA20180144C23" ref-type="bibr">23</xref>], in all these cases and many more we find that processes on quite disparate length and time scales interact in strong and nonlinear ways. In short, multiscale modelling is ubiquitous and progress in most of these cases is determined by our ability to design and implement multiscale models of the particular systems under study [<xref rid="RSTA20180144C1" ref-type="bibr">1</xref>,<xref rid="RSTA20180144C6" ref-type="bibr">6</xref>,<xref rid="RSTA20180144C24" ref-type="bibr">24</xref>,<xref rid="RSTA20180144C25" ref-type="bibr">25</xref>].</p><p>The increasing importance of multiscale modelling in many domains of science and engineering is clearly demonstrated in numerous publications (e.g. [<xref rid="RSTA20180144C1" ref-type="bibr">1</xref>,<xref rid="RSTA20180144C26" ref-type="bibr">26</xref>]). Therefore, we must anticipate that multiscale simulations will become an ever-more important form of scientific application on high-end computing resources, necessitating the development of sustainable and reusable solutions for such applications. That is, we expect to need generic algorithms for multiscale computing.</p><p>We therefore require innovative new ways of computing to face the challenges posed both by multiscale modelling and simulation and by the emerging high-end computing ecosystem [<xref rid="RSTA20180144C27" ref-type="bibr">27</xref>]. This will contribute to our ability to solve multiscale problems and, as we will argue, can <italic>also</italic> offer an avenue for new ways to efficiently exploit exascale resources. Multiscale computing could face these challenging by deploying its various single-scale components across heterogeneous architectures of exascale resources, mapped to produce optimal performance and designed to bridge both temporal and spatial scales [<xref rid="RSTA20180144C28" ref-type="bibr">28</xref>&#x02013;<xref rid="RSTA20180144C31" ref-type="bibr">31</xref>]. Therefore, we should embark upon a programme to efficiently deploy multiscale codes on today's and future high-performance computers and, thereby, establish a new and more effective paradigm for exploiting current and emerging computing resources.</p><p>In this position paper, we explore and discuss generic multiscale computing on emerging exascale high-performing computing (HPC) environments. We will first discuss the different phases when developing a multiscale model and simulating it on available computing infrastructure, and analyse where in our view we could, and maybe should, continue to further develop generic frameworks and software tools to facilitate multiscale computing. Next, we will focus on simulating multiscale models on high-end computing resources, which we call High Performance Multiscale Computing (HPMC), in the face of emerging exascale performance levels. We will argue that strong <italic>and</italic> weak parallel scaling of monolithic applications often may reach its limits at the exascale and that we need to invoke what we would call <italic>multi-scaling</italic>. Note that although our analysis is driven by the needs of modelling multiscale systems, our arguments with respect to the scalability challenge for multiscale systems to the exascale also point to the necessity to consider new approaches to increase concurrency within complex (single-scale) models through new algorithms and corresponding implementations.</p></sec><sec id="s2"><label>2.</label><title>Generic multiscale modelling and simulation</title><sec id="s2a"><label>(a)</label><title>Simulation-based science</title><p>Simulation-based science is all about formulating computational models of phenomena that we observe, and performing computer simulations in order to deepen our understanding of the systems that underpin these phenomena. The aim is to predict their future behaviour or to find adaptations that would change their behaviour in some desired way. Simulation-based science is sometimes called the third pillar of science and complements theory and experiments. Together they underpin the scientific method and strongly interact. Theory provides the necessary framework for computational models, experiments provide the data against which the computational models need to be validated, and numerical simulations may lead to new insights and theory or new hypotheses that are tested experimentally.</p><p>In performing simulation-based science, we usually go through a generic modelling and simulation cycle (<xref ref-type="fig" rid="RSTA20180144F1">figure 1</xref>). Based on available (observational) data and knowledge and theory of the phenomenon under study, a conceptual model is formulated, which is then turned into a computational model. This is then implemented on a computer after which we perform numerical experiments with the computational model. These simulations provide results that are used to validate our models and, once validated, to predict the behaviour of the system we study.
<fig id="RSTA20180144F1" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>The modelling and simulation cycle. (Online version in colour.)</p></caption><graphic xlink:href="rsta20180144-g1"/></fig></p><p>The power of this approach lies in the fact that the computational sciences have developed a large collection of well-established generic modelling paradigms (such as ODE or PDE-based methods, particle-based methods, agent-based methods, fully discrete methods, etc.) and a large collection of well-established (numerical) methods to discretize these computational models and implement them very efficiently on the full range of available computers (from the laptop via cloud to petaflop/s supercomputers). We argue that also in multiscale modelling and simulation such generic approaches are possible and have been demonstrated in a number of successful projects. Having been exposed to such solutions over the last decade, we will discuss the potential of generic multiscale modelling and simulation, with emphasis on high-end performance levels, projecting forward to the era of exascale computing.</p></sec><sec id="s2b"><label>(b)</label><title>The multiscale modelling and simulation framework</title><p>Many applications in computational science involve large ranges of spatial and temporal scales. Multiscale modelling amounts to splitting the spatio-temporal scales into what are often called single-scale submodels. These submodels are then coupled through various scale-bridging techniques. Such a multiscale model, that is, a collection of single-scale submodels coupled via scale bridging algorithms, should then be a sufficiently accurate representation of the behaviour of the problem where all the scales are present, yet with a substantial reduction of computing needs. <xref ref-type="fig" rid="RSTA20180144F2">Figure 2</xref> illustrates the multiscale approach as defined in the Multiscale Modelling and Simulation Framework (MMSF) [<xref rid="RSTA20180144C28" ref-type="bibr">28</xref>,<xref rid="RSTA20180144C32" ref-type="bibr">32</xref>].
<fig id="RSTA20180144F2" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>Decomposition of a monolythic application covering many spatial and temporal scales into several coupled single-scale submodels. (Online version in colour.)</p></caption><graphic xlink:href="rsta20180144-g2"/></fig></p><p>The MMSF is a theoretical and practical method for modelling, characterizing and simulating multiscale phenomena. The MMSF has been developed and applied over the past 10 years [<xref rid="RSTA20180144C1" ref-type="bibr">1</xref>,<xref rid="RSTA20180144C28" ref-type="bibr">28</xref>,<xref rid="RSTA20180144C29" ref-type="bibr">29</xref>,<xref rid="RSTA20180144C31" ref-type="bibr">31</xref>,<xref rid="RSTA20180144C33" ref-type="bibr">33</xref>&#x02013;<xref rid="RSTA20180144C35" ref-type="bibr">35</xref>], and comprises a four-stage pipeline from developing a multiscale model to executing the multiscale simulation. This is shown in <xref ref-type="fig" rid="RSTA20180144F3">figure 3</xref>. First, we model a phenomenon by identifying relevant processes that are well described by single-scale submodels, and their relevant scales, using the Scale Separation Map (SSM, the right panel in <xref ref-type="fig" rid="RSTA20180144F2">figure 2</xref> is an example of an SSM). The architecture of the multiscale model, that is the communication between the single-scale models and details of the scale-bridging methods, are then specified in the Multiscale Modelling Language (MML) [<xref rid="RSTA20180144C36" ref-type="bibr">36</xref>]. This specification is then used to (semi) automatically glue the single scale components (software implementations of the single-scale models) and the scale bridging components together using some dedicated coupling toolkit (such as, for instance, Muscle [<xref rid="RSTA20180144C33" ref-type="bibr">33</xref>,<xref rid="RSTA20180144C37" ref-type="bibr">37</xref>], MaMiCo [<xref rid="RSTA20180144C38" ref-type="bibr">38</xref>] or AMUSE [<xref rid="RSTA20180144C9" ref-type="bibr">9</xref>,<xref rid="RSTA20180144C39" ref-type="bibr">39</xref>]). Finally, the multiscale simulation can be executed, by using dedicated environments such as, e.g. QCG [<xref rid="RSTA20180144C40" ref-type="bibr">40</xref>]. This can in principle be done in a highly automated, optimized way, certainly when targeting HPC infrastructure [<xref rid="RSTA20180144C31" ref-type="bibr">31</xref>,<xref rid="RSTA20180144C41" ref-type="bibr">41</xref>].
<fig id="RSTA20180144F3" orientation="portrait" position="float"><label>Figure 3.</label><caption><p>The multiscale modelling and simulation framework. (Online version in colour.)</p></caption><graphic xlink:href="rsta20180144-g3"/></fig></p></sec><sec id="s2c"><label>(c)</label><title>Generic frameworks</title><p>From the start of the development of the MMSF and comparable frameworks for multiscale modelling and simulation (for an overview, see the paper by Groen <italic>et al.</italic> [<xref rid="RSTA20180144C42" ref-type="bibr">42</xref>] in this special issue), the ambition has always been to be as generic as possible, in the sense that such environments should be applicable for a large range of applications from different domains, that software components (implementing single-scale models or e.g. scale bridging) be reusable and interchangeable, and relieve the users as much as possible of the intricacies of launching and executing complex multiscale models on equally complex distributed HPC infrastructure.</p><p>We argue, based on a range of projects in multiscale modelling and computing, that the four phases of the MMSF (<xref ref-type="fig" rid="RSTA20180144F3">figure 3</xref>) are generic and follow the simulation-based science cycle from <xref ref-type="fig" rid="RSTA20180144F1">figure 1</xref>. The Scale Separation Map and its more detailed description encoded in the MML together form the Conceptual Multiscale Model, whereas the full implementation of the single-scale models, the scale-briding algorithms, glued together according to the MML architecture using coupling frameworks, forms the Computational Multiscale Model. The final execution phase in the MMSF is then the multiscale simulation.</p><p>This observation would warrant generic all-encompassing software environments to define a conceptual model, which is then translated by this environment into a computational model, and is finally executed on a range of computing infrastructures, again facilitated by such generic environments. Although this is certainly possible, as demonstrated, e.g. by our own developments [<xref rid="RSTA20180144C32" ref-type="bibr">32</xref>], it is however not surprising that a range of different environments have been developed to, e.g. connect single codes into a full multiscale simulation. Usually, this is within specific communities, e.g. weather/climate [<xref rid="RSTA20180144C43" ref-type="bibr">43</xref>], fusion [<xref rid="RSTA20180144C23" ref-type="bibr">23</xref>] or astrophysics [<xref rid="RSTA20180144C44" ref-type="bibr">44</xref>]. These environments run in production and are tailored to the needs of their communities.</p><p>This then triggers the question as to what the benefits of generic frameworks could be, if any, and where they could be most useful. We believe that such frameworks should lead to a separation of concerns, where the user can focus on the modelling and simulation itself. A main ingredient in multiscale modelling is coupling single-scale models using scale bridging to convert information from one scale to another. Such coupling increases the complexity of multiscale modelling and simulation, and software should help mitigate that, both on the conceptual level and the simulation level. Moreover, generic frameworks should allow the reuse of validated and tested modules (submodels) and this could help in effectively standardizing data formats.</p><p>In our opinion, generic tools could be beneficial to the users both on the level of developing a conceptual multiscale model and on the level of executing the multiscale model. We call this the multiscale computing hourglass (see <xref ref-type="fig" rid="RSTA20180144F4">figure 4</xref>). It is important to continue to develop frameworks for multiscale computing on both levels, as we will argue below in more detail.
<fig id="RSTA20180144F4" orientation="portrait" position="float"><label>Figure 4.</label><caption><p>The multiscale computing hourglass. (Online version in colour.)</p></caption><graphic xlink:href="rsta20180144-g4"/></fig></p><p>In MMSF, a conceptual multiscale model is defined as a collection of single-scale models, their connection including scale-bridging methods, and the details of the connection scheme [<xref rid="RSTA20180144C28" ref-type="bibr">28</xref>]. The MML provides a formal way to specify the conceptual multiscale model [<xref rid="RSTA20180144C28" ref-type="bibr">28</xref>,<xref rid="RSTA20180144C36" ref-type="bibr">36</xref>], and using its xml version called xMML, it can be fully specified in a machine-readable text file. Such formal specification, in the form of an xMML file, could then be input to the neck of the hourglass in <xref ref-type="fig" rid="RSTA20180144F4">figure 4</xref>, where actual &#x02018;glue&#x02019; code needs to be implemented. The benefits of agreeing on a formal specification of a multiscale model, relying, e.g. on MML, should not be underestimated and will bring many benefits. First, it allows clear and well-defined communication with colleagues, as well as the sharing of multiscale models. One could, for instance, imagine repositories of xMML files, very much like existing repositories of SBML [<xref rid="RSTA20180144C45" ref-type="bibr">45</xref>] or CellML [<xref rid="RSTA20180144C46" ref-type="bibr">46</xref>&#x02013;<xref rid="RSTA20180144C48" ref-type="bibr">48</xref>] models. It could also form the starting point for a mathematical framework to reason about properties of a multiscale model, ranging from proving that it does not contain a deadlock [<xref rid="RSTA20180144C28" ref-type="bibr">28</xref>], via using it as input for multiscale uncertainty quantification methods [<xref rid="RSTA20180144C49" ref-type="bibr">49</xref>], to estimating and optimizing its computational footprint [<xref rid="RSTA20180144C31" ref-type="bibr">31</xref>].</p><p>The neck of the hourglass contains the actual creation of the multiscale computational model, where all the application-specific details need to be established (<xref ref-type="fig" rid="RSTA20180144F4">figure 4</xref>). One important component is software to glue together the single-scale components using some coupling toolkit. Although some of these toolkits have been developed to be again generic and suitable across scientific domains (e.g. Muscle [<xref rid="RSTA20180144C33" ref-type="bibr">33</xref>,<xref rid="RSTA20180144C37" ref-type="bibr">37</xref>]), most coupling libraries used in production today are developed and maintained in specific scientific communities, e.g. astrophysics [<xref rid="RSTA20180144C39" ref-type="bibr">39</xref>], complex fluids [<xref rid="RSTA20180144C38" ref-type="bibr">38</xref>,<xref rid="RSTA20180144C50" ref-type="bibr">50</xref>], quantum chemistry [<xref rid="RSTA20180144C51" ref-type="bibr">51</xref>] or climate modelling [<xref rid="RSTA20180144C43" ref-type="bibr">43</xref>]. Although we would very much welcome an exchange of ideas and technologies between these communities, to avoid re-inventing the wheel, we also acknowledge that given the current state of the art, it would be unrealistic to push for generic coupling toolkits and environments, comparable to, e.g. MPI in parallel computing.</p><p>What would be profitable is to link generic concepts for conceptual multiscale modelling, as laid down in xMML files, to these coupling environments. For instance, one could compile xMML files into &#x02018;glue code&#x02019; and &#x02018;wrapper code&#x02019; for a specific environment, as we have demonstrated with Muscle [<xref rid="RSTA20180144C28" ref-type="bibr">28</xref>,<xref rid="RSTA20180144C33" ref-type="bibr">33</xref>]. Such technology would mitigate much of the aforementioned complexity in relation to coupling together single-scale components and scale-bridging code snippets. In our experience, such high level inter-operability does not necessarily induce overheads (e.g. [<xref rid="RSTA20180144C29" ref-type="bibr">29</xref>]), and even if so, we believe that the benefits in terms of maintainability and extensibility of complex multiscale models would outweigh such trade-offs. Moreover, this approach also allows for straightforward &#x02018;plug-and-play&#x02019;, where, for instance, different implementations of a single-scale model, that may perform optimally on different architectures, are quickly interchanged.</p><p>Finally, at the bottom of the hourglass, when simulating the multiscale model as efficiently as possible on available computing resources, we find again ample room for generic solutions that would alleviate the burden of mapping a complex multiscale simulation in the most efficient way to hardware. Certainly when the targeted hardware consists of HPC machines, distributed systems, cloud environments or a combination of all these, and when issues like load balancing, advanced reservations, fault tolerance and energy efficiency need to be taken into account and optimized, we believe that generic solutions, to which the coupling libraries discussed above could interface, would be highly beneficial.</p><p>To do so we have proposed the concept of Multiscale Computing Patterns (MCPs), which are generic and recurring call sequences at the level of the single-scale components of a multiscale simulation [<xref rid="RSTA20180144C31" ref-type="bibr">31</xref>]. It is beyond the scope of this manuscript to discuss the details of MCPs. It suffices to note that we proposed three such patterns, which capture most, if not all, multiscale simulations. We have proposed and implemented a pilot of MCPs software that takes as input the generic xMML description of a multiscale model, in combination with performance measurements of the components that make up the multiscale model (both in terms of computational performance and energy usage on massively parallel machines), and then interfaces with middleware, such as in our case QCG [<xref rid="RSTA20180144C40" ref-type="bibr">40</xref>], to propose an optimal mapping of the multiscale simulation to available hardware, and to schedule and finally execute the simulation [<xref rid="RSTA20180144C41" ref-type="bibr">41</xref>]. Note that such a generic deployment of the multiscale simulation only requires knowledge of the control structure of the multiscale model, as available via the xMML, and detailed knowledge of the performance of the single-scale models (both in terms of execution time, but also in terms of e.g. energy usage). If this is available for a range of (heterogeneous) architectures, the MCPs software can select, in a generic way and for each type of pattern, an optimal way to perform the multiscale simulation, given available hardware and even considering expected queuing times for that hardware [<xref rid="RSTA20180144C41" ref-type="bibr">41</xref>]. Although these are very recent developments, in our view such generic solutions for scheduling and executing multiscale simulations on complex computing ecosystems consisting of large HPC machines, distributed environments and clouds, and interfacing these solutions to the most popular multiscale coupling libraries and to the most relevant scheduling environments and queuing systems, could be extremely beneficial to users, who no longer need to worry about this time-consuming and complex phase of multiscale computing.</p></sec></sec><sec id="s3"><label>3.</label><title>Towards the exascale</title><sec id="s3a"><label>(a)</label><title>Strong and weak scaling</title><p>As the clock speeds of individual cores are no longer increasing, emerging exascale machines can only reach their anticipated exaflop/s computational speeds by aggregating larger and larger numbers of nodes and cores. As a result, these machines are becoming &#x02018;fatter&#x02019;, not faster. We have argued before that as a result of this we are approaching the limits of what is achievable using monolithic codes [<xref rid="RSTA20180144C31" ref-type="bibr">31</xref>]. Our argument was that &#x02018;because the parallelism is usually applied to the spatial domain, we are increasingly simulating larger slabs of matter and bigger chunks of the Universe, applying weak scaling by using more particles, a higher grid resolution or more finite elements. Yet it often is the temporal behaviour that one is really interested in, and that behaviour is not extended by adopting larger computers of this nature, or by making the problem physically larger. Since the scientific problems of interest usually have time scales which scale as a nonlinear function of the volume of the system under investigation, each temporal update requires more wall clock time for larger physical problems. This is in fact a recipe for disaster: we are not getting closer to studying large space and long time behaviour with monolithic codes&#x02019;. We consider this to be a cogent argument to suggest that monolithic codes could be the exception on exascale machines, as weak scaling may not produce the desired results. If so, we should invest in developing other scenarios, including high-performance multiscale computing, or scenarios where monolithic codes (or coupled multiscale codes for that matter) are repeated over and over again in ensembles, e.g. in parameter sweeping scenarios or for uncertainty quantification (see also Portegies Zwart, who recently made a similar argument [<xref rid="RSTA20180144C27" ref-type="bibr">27</xref>]).</p><p>We will now further analyse this argument by looking in detail at different scaling scenarios, relying on the Scale Separation Map as introduction in <xref ref-type="fig" rid="RSTA20180144F2">figure 2</xref> and high-level modelling of parallel performance.</p><p>If we would assume that we can continue using emerging exascale machines efficiently by relying on weak scaling, we basically hope that much larger systems can be simulated. And that this is possible without new additional algorithmic strategies, but by just relying on the huge additional computing power offered by such exascale machines. <xref ref-type="fig" rid="RSTA20180144F5">Figure 5</xref> shows different scaling scenarios on a scale map. For instance, in climate modelling, the system size <italic>L</italic> is likely to stay the same (the full Earth), as well as the time span of the simulation <italic>T</italic>. But accuracy may be improved, which requires reductions in &#x00394;<italic>t</italic> and &#x00394;<italic>x</italic>. Decreasing &#x00394;<italic>t</italic> would imply more iterations to reach the time span <italic>T</italic>, and therefore our argument holds.
<fig id="RSTA20180144F5" orientation="portrait" position="float"><label>Figure 5.</label><caption><p>Weak scaling scenarios. (<italic>a</italic>) By increasing system size and simulated time and (<italic>b</italic>) by increasing temporal and spatial resolution. A combination of both is also possible. (Online version in colour.)</p></caption><graphic xlink:href="rsta20180144-g5"/></fig></p><p>For other applications, <italic>T</italic> might already be sufficient, but <italic>L</italic> needs to be increased. As an example, one may consider the transport of sediments in a river over a given time period. The capability to consider a longer stretch of the river is important. Also, when computing blood flow, we may want to add more vessels in the simulation, thus increasing <italic>L</italic>. In this case, the argument above would not hold and weak scaling remains a good option.</p><p>It is likely, however, that both <italic>L</italic> and <italic>T</italic> need to be increased in proportion because phenomena at a larger time scale usually also imply a larger spatial scale. It is in such scaling scenarios, which we believe are most common, where we may be reaching the limits of what is achievable by just increasing the number of processors, as we need to do in order to reach exascale performance levels.</p><p>We will now explore the limits of increasing <italic>L</italic> and/or <italic>T</italic>, assuming that a much larger computer becomes available. We will show that for some applications exascale machines can bring a substantial gain, but for others scaling is limited, and new multiscale techniques need to be developed to exploit the additional computing power. Note that here we mainly focus the discussion on increasing <italic>L</italic> or <italic>T</italic>, but the same approach can be used to study the possibility to decrease &#x00394;<italic>t</italic> or &#x00394;<italic>x</italic> to improve accuracy.</p><p>Let us consider a parallel code whose execution time <italic>T</italic><sub>par</sub> reads
<disp-formula id="RSTA20180144M3x1"><label>3.1</label><mml:math id="DM1"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">par</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi mathvariant="italic">&#x003a9;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi mathvariant="italic">&#x003a9;</mml:mi></mml:mrow><mml:mi>W</mml:mi></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic>W</italic> is the total work (e.g. the number of operations required to solve the problem), <italic>p</italic> the number of cores, <italic>R</italic> the speed of the core (number of operations per second) and <italic>&#x003a9;</italic> the overhead resulting from the parallelization. The efficiency <italic>E</italic> is then
<disp-formula id="RSTA20180144M3x2"><label>3.2</label><mml:math id="DM2"><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">seq</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">par</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>W</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mi mathvariant="italic">&#x003a9;</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi mathvariant="italic">&#x003a9;</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula>Note that the quantity <italic>pR&#x003a9;</italic>/<italic>W</italic> is the total fractional overhead [<xref rid="RSTA20180144C52" ref-type="bibr">52</xref>]. In all generality, we can write
<disp-formula id="RSTA20180144M3x3"><label>3.3</label><mml:math id="DM3"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">par</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math></disp-formula>showing that the effective power of the processors is reduced by a factor equal to the efficiency of the parallel implementation.</p><p>The relation <italic>W</italic>&#x02009;=&#x02009;<italic>W</italic>(<italic>p</italic>) that guarantees that <italic>E</italic> stays constant is called the isoefficiency function. Weak scaling is achieved by increasing <italic>W</italic> as <italic>p</italic> increases, typically according to the isoefficiency relation. Strong scaling is achieved by increasing <italic>p</italic> while keeping <italic>W</italic> constant. In the former case, one solves a larger problem on more processors, within about the same time <italic>T</italic><sub>par</sub>. In the latter case, one expects to solve a given problem faster by using more processors.</p><p>For instance, for an iterative stencil-based calculation on a three-dimensional mesh, one has a communication overhead between each subdomain, at each iteration. Let us assume that we have <italic>n</italic> iterations and that <italic>w</italic>&#x02009;=&#x02009;<italic>W</italic>/<italic>n</italic> is the work per iteration, which is proportional to the sum of the volumes of the subdomains. The communication overhead is then proportional to the boundary of one subdomain, namely to (<italic>W</italic>/(<italic>np</italic>))<sup>2/3</sup>&#x02009;=&#x02009;(<italic>w</italic>/<italic>p</italic>)<sup>2/3</sup>. Thus,
<disp-formula id="RSTA20180144M3x4"><label>3.4</label><mml:math id="DM4"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">par</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>W</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>C</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>w</mml:mi><mml:mi>p</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic>C</italic> is a constant depending on the speed of the interconnection network. Thus <italic>&#x003a9;</italic>&#x02009;=&#x02009;<italic>nC</italic>(<italic>w</italic>/<italic>p</italic>)<sup>2/3</sup> and
<disp-formula id="RSTA20180144M3x5"><label>3.5</label><mml:math id="DM5"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mi mathvariant="italic">&#x003a9;</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>C</mml:mi><mml:mi>p</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>C</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula>The isoefficiency function is then <italic>W</italic>(<italic>p</italic>)&#x0223c;<italic>p</italic>.</p><p>Many scientific applications correspond to the time evolution of a spatial quantity. The typical spatial and temporal scales that can be resolved depend on the space and time discretization &#x00394;<italic>x</italic> and &#x00394;<italic>t</italic>, as well as <italic>L</italic>, the spatial extension of the system, and <italic>T</italic>, the duration of the simulation. Spatial and temporal scales outside this interval will not be resolved. One can estimate the CPU time required for such a simulation. The total work <italic>W</italic> is typically
<disp-formula id="RSTA20180144M3x6"><label>3.6</label><mml:math id="DM6"><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <italic>&#x003b2;</italic> is a proportionality factor and <italic>d</italic> is the spatial dimension. The number of iterations is <italic>n</italic>&#x02009;=&#x02009;<italic>T</italic>/&#x00394;<italic>t</italic> and the work per iteration is
<disp-formula id="RSTA20180144M3x7"><label>3.7</label><mml:math id="DM7"><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>If we again assume a stencil-based calculation on a three-dimensional mesh, combining equations (<xref ref-type="disp-formula" rid="RSTA20180144M3x3">3.3</xref>), (<xref ref-type="disp-formula" rid="RSTA20180144M3x5">3.5</xref>)&#x02013;(<xref ref-type="disp-formula" rid="RSTA20180144M3x7">3.7</xref>) results in
<disp-formula id="RSTA20180144UM1"><mml:math id="DM8"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">par</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mi>&#x003b2;</mml:mi><mml:mi>R</mml:mi></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:msup><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>We can write this as
<disp-formula id="RSTA20180144M3x8"><label>3.8</label><mml:math id="DM9"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">par</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#x003b2;</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mi>C</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula>The above equation relates the physical time <italic>T</italic> that can be simulated within a parallel execution time <italic>T</italic><sub>par</sub> as a function of the spatial extension <italic>L</italic> of the system and the number of processors <italic>p</italic>. This equation <italic>T</italic>&#x02009;=&#x02009;<italic>T</italic>(<italic>L</italic>) can be seen as an iso-performance relation, or an iso-<italic>T</italic><sub>par</sub> curve.</p><p>Let us now take specific values for the performance model given in equation (<xref ref-type="disp-formula" rid="RSTA20180144M3x8">3.8</xref>) (<xref rid="RSTA20180144TB1" ref-type="table">table 1</xref>). Here we assume a Lattice Boltzmann (LB) simulation<sup><xref ref-type="fn" rid="FN0001">1</xref></sup> of blood flow in an aneurysm of centimetre size, resolved at 10&#x02009;&#x003bc;m, for two heart beats. Time resolution is 10<sup>&#x02212;5</sup>&#x02009;s, and we assume cores able to perform 0.4&#x02009;&#x000d7;&#x02009;10<sup>9</sup> double precision operations per second. A typical LB iteration requires <italic>&#x003b2;</italic>&#x02009;=&#x02009;200 arithmetic operations and the exchange of typically 20 populations of 8 bytes. Thus, we set <italic>C</italic>&#x02009;=&#x02009;1&#x02009;GB&#x02009;s<sup>&#x02212;1</sup>&#x02009;&#x000d7;&#x02009;20&#x02009;&#x000d7;&#x02009;8&#x02009;=&#x02009;1.5&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;7</sup>.
<table-wrap id="RSTA20180144TB1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Specific values for LB example.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/><col align="left" span="1"/></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1"><italic>R</italic> (s<sup>&#x02212;1</sup>)</th><th align="left" rowspan="1" colspan="1"><italic>C</italic> (s)</th><th align="left" rowspan="1" colspan="1"><italic>&#x003b2;</italic></th><th align="left" rowspan="1" colspan="1"><italic>p</italic></th><th align="left" rowspan="1" colspan="1"><italic>T</italic><sub>par</sub> (day)</th><th align="left" rowspan="1" colspan="1"><italic>L</italic> (m)</th><th align="left" rowspan="1" colspan="1">&#x00394;<italic>x</italic> (m)</th><th align="left" rowspan="1" colspan="1">&#x00394;<italic>t</italic> (s)</th><th align="left" rowspan="1" colspan="1"><italic>T</italic> (s)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">0.4 10<sup>9</sup></td><td rowspan="1" colspan="1">1.5 10<sup>&#x02212;7</sup></td><td rowspan="1" colspan="1">200</td><td rowspan="1" colspan="1">1000</td><td rowspan="1" colspan="1">1.5</td><td rowspan="1" colspan="1">0.01</td><td rowspan="1" colspan="1">10<sup>&#x02212;5</sup></td><td rowspan="1" colspan="1">10<sup>&#x02212;5</sup></td><td rowspan="1" colspan="1">2</td></tr></tbody></table></table-wrap>
</p><p><xref ref-type="fig" rid="RSTA20180144F6">Figure 6</xref> show the iso-performance curve (<xref ref-type="disp-formula" rid="RSTA20180144M3x8">3.8</xref>), assuming that we consider a computation of <italic>T</italic><sub>par</sub>&#x02009;=&#x02009;1.5 days. These curves indicate how <italic>L</italic> and <italic>T</italic> can be varied for the same <italic>T</italic><sub>par</sub>, while keeping &#x00394;<italic>x</italic> and &#x00394;<italic>t</italic> fixed.
<fig id="RSTA20180144F6" orientation="portrait" position="float"><label>Figure 6.</label><caption><p>Scaling of a lattice Boltzmann code at different performance levels. See text for a detailed account of the behaviour shown here.</p></caption><graphic xlink:href="rsta20180144-g6"/></fig></p><p>When getting a faster machine, with <italic>m</italic> times more processors, we see that we can consider the same physical time <italic>T</italic>, and increase the system size by the expected factor <italic>m</italic><sup>1/3</sup>. This is weak scaling. On the other hand, one can also keep the same system size and increase the simulation time <italic>T</italic>. This corresponds to a strong scaling situation, except that here we keep <italic>T</italic><sub>par</sub>&#x02009;=&#x02009;1.5 days constant and compute a larger time scale. As we can see from <xref ref-type="fig" rid="RSTA20180144F6">figure 6</xref>, the potential is less significant, although still interesting. For instance, taking <italic>p</italic>&#x02009;=&#x02009;1&#x02009;000&#x02009;000 and the same <italic>L</italic> allows us to reach <italic>T</italic>&#x02009;=&#x02009;10<sup>3</sup>&#x02009;s, that is an increase of a factor 500, with respect to 1000 times more processors. With <italic>p</italic>&#x02009;=&#x02009;1&#x02009;000&#x02009;000&#x02009;000, the value of <italic>T</italic> becomes 2&#x02009;&#x000d7;&#x02009;10<sup>5</sup>, hence an increase of a factor 10<sup>5</sup> with 10<sup>6</sup> additional processors. We also notice that these iso-curves stop when <italic>L</italic> becomes too small, corresponding to the situation where each processor would hold less than one data element.</p><p>It is, however, usually more likely that both <italic>L</italic> and <italic>T</italic> need to be increased. Typically, when analysing convective processes, <italic>L</italic> and <italic>T</italic> must grow proportionally (for a diffusive process, <italic>L</italic> would grow as <inline-formula><mml:math id="IM1"><mml:msqrt><mml:mi>T</mml:mi></mml:msqrt></mml:math></inline-formula>). <xref ref-type="fig" rid="RSTA20180144F6">Figure 6</xref> shows with a dotted line an increase of <italic>L</italic> and <italic>T</italic> by the same factor (up to a 1000). Its intersection with the exascale iso-performance line gives <italic>T</italic>&#x02009;=&#x02009;8&#x02009;&#x000d7;&#x02009;10<sup>1</sup>&#x02009;s and <italic>L</italic>&#x02009;=&#x02009;4&#x02009;&#x000d7;&#x02009;10<sup>&#x02212;1</sup>&#x02009;m. This is a 40 times longer simulation time, for a system 40 times larger, thus with 40<sup>3</sup>&#x02009;=&#x02009;64&#x02009;000 more mesh points. Therefore, if such a constraint holds between <italic>L</italic> and <italic>T</italic>, the possibility to reach larger physical time scales is very limited, even with a one million-fold performance increase. This is in line with our original qualitative argument that weak scaling starts to break down if we only increase processor numbers without increasing processor speeds.</p><p>Next we consider another application that scales less well than LB models. We assume an N-body problem with long-range forces. We consider a <inline-formula><mml:math id="IM2"><mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> implementation, assuming that the Barnes&#x02013;Hut algorithm does not apply here because the particles are on average too close to each other. The sequential execution time is
<disp-formula id="RSTA20180144M3x9"><label>3.9</label><mml:math id="DM10"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">seq</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mn>11</mml:mn><mml:mi>R</mml:mi></mml:mfrac><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>6</mml:mn><mml:mi>R</mml:mi></mml:mfrac><mml:mi>N</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>where the first term corresponds to the computation of the pairs interaction forces (e.g. gravity needs 11 operations) and the second is the time integration (e.g. Verlet requires six operations). <italic>R</italic> is the speed of a core, <italic>T</italic>/&#x00394;<italic>t</italic> is the number of time steps.<sup><xref ref-type="fn" rid="FN0002">2</xref></sup></p><p>With <italic>p</italic> cores and <italic>N</italic>/<italic>p</italic> particles per core, the force computation requires an <italic>all-to-all</italic> communication, which can be realized with <italic>p</italic>&#x02009;&#x02212;&#x02009;1 communications involving all cores <italic>P</italic><sub><italic>i</italic></sub> in parallel. At stage <italic>k</italic>, <italic>k</italic>&#x02009;=&#x02009;1,&#x02009;&#x02026;,&#x02009;<italic>p</italic>&#x02009;&#x02212;&#x02009;1, <italic>P</italic><sub><italic>i</italic></sub> sends 3<italic>N</italic>/<italic>p</italic> coordinates to <italic>P</italic><sub><italic>i</italic>+<italic>k</italic></sub> modulo <italic>p</italic>. The resulting communication time is
<disp-formula id="RSTA20180144M3x10"><label>3.10</label><mml:math id="DM11"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">comm</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>8</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mi>C</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x000d7;</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>because there are three double precision spatial coordinates per particle. <italic>C</italic> is the time to exchange a byte between two cores.</p><p>Thus, the parallel time <italic>T</italic><sub>par</sub> is
<disp-formula id="RSTA20180144M3x11"><label>3.11</label><mml:math id="DM12"><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">par</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mfrac><mml:mrow><mml:mn>11</mml:mn><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>6</mml:mn><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>24</mml:mn><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:mfrac><mml:mo>]</mml:mo></mml:mrow><mml:mfrac><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>and, from this relation, we obtain
<disp-formula id="RSTA20180144M3x12"><label>3.12</label><mml:math id="DM13"><mml:mfrac><mml:mi>T</mml:mi><mml:mrow><mml:mi mathvariant="normal">&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">par</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>6</mml:mn><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>24</mml:mn><mml:mi>C</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:math></disp-formula>This relation links the number of iterations <italic>T</italic>/&#x00394;<italic>t</italic> that are possible with <italic>N</italic> particles within a computational time <italic>T</italic><sub>par</sub>. We call this relation the <italic>T</italic><sub>par</sub> iso-line. Following the same approach as in the previous case, we show in <xref ref-type="fig" rid="RSTA20180144F7">figure 7</xref> this iso-line for different scales of computing resources. Here, we took <italic>R</italic>&#x02009;=&#x02009;10<sup>9</sup>&#x02009;s<sup>&#x02212;1</sup>, <italic>C</italic>&#x02009;=&#x02009;10<sup>9</sup>&#x02009;s. The <italic>T</italic><sub>par</sub> iso-lines stops when the number of particles per core reaches 1. From <xref ref-type="fig" rid="RSTA20180144F7">figure 7</xref>, we see first that adding more cores allows one to consider more particles, but not in proportion to the increase of computing power. Second, the possibility of running the simulation to larger physical time is rather limited. And worse, the maximum possible number of iterations may even decrease as the number of cores increases, as is the case for the reference simulation (<italic>N</italic>,&#x02009;<italic>T</italic>/&#x00394;<italic>t</italic>). This behaviour is obviously due to the poor scaling of an <italic>all-to-all</italic> communication and suggests that the exascale may not allow one to solve larger N-body problems with a simple brute force approach.
<fig id="RSTA20180144F7" orientation="portrait" position="float"><label>Figure 7.</label><caption><p>The relationship between the number (<italic>N</italic>) of particles and the number (<italic>T</italic>/&#x00394;<italic>t</italic>) of iterations that are possible within <italic>T</italic><sub>par</sub>&#x02009;=&#x02009;1.5 days, and different numbers of processors (<italic>p</italic>).</p></caption><graphic xlink:href="rsta20180144-g7"/></fig></p><p>These results indicate that some single-scale monolithic applications (e.g. lattice Boltzmann solvers) still have good potential to exploit exascale computers, but for others (e.g. N-body simulations) exascale may not bring any benefit to simulate larger systems for a longer physical time. Smarter than brute force approaches are thus needed, such as for instance multiscale simulation where scales are separated within different sub-models.</p></sec><sec id="s3b"><label>(b)</label><title>Multi-scaling</title><p>One way out of this dilemma is by taking highly efficient monolithic codes that perform well on the petascale, and combining them in loosely coupled ways to reach exascale performance. We call this <italic>multi-scaling</italic>, where &#x02018;scale&#x02019; in this case does not refer to spatial or temporal scales, but to processor scales. We can identify a few scenarios for multi-scaling (<xref ref-type="fig" rid="RSTA20180144F8">figure 8</xref>).
<fig id="RSTA20180144F8" orientation="portrait" position="float"><label>Figure 8.</label><caption><p>&#x02018;Multi-scaling&#x02019; for parallel performance, by adding more processes (upper right, multi-process), by executing a single-scale process multiple times in parallel (lower left, multiple instances) or by building a multiscale model (lower right), or any combination of these three. (Online version in colour.)</p></caption><graphic xlink:href="rsta20180144-g8"/></fig></p><p>One could run multiple instances of the code, e.g. in replica computing (parameter sweeping) applications or uncertainty quantification. One could also create a multi-process application, effectively coupling different processes together that overlap in spatio-temporal scales. Finally, one could create multiscale applications, coupling single-scale monolithic codes on different spatio-temporal scales. And, of course, combinations of these scenarios are possible, e.g. performing uncertainty quantification on a multiscale model [<xref rid="RSTA20180144C49" ref-type="bibr">49</xref>].</p><p>It would be wise to explore further these types of applications and better understand how <italic>multi-scaling</italic> could help unleash the power of future exascale machines, but at the same time also face the challenges in relation to energy consumption and fault tolerance. We have already seen convincing examples, e.g. using the multiple instances approach, in replica computing for calculating binding affinaties [<xref rid="RSTA20180144C53" ref-type="bibr">53</xref>,<xref rid="RSTA20180144C54" ref-type="bibr">54</xref>].</p></sec></sec><sec id="s4"><label>4.</label><title>Conclusion</title><p>Multiscale computing has turned into a mature paradigm, supported by many communities that have invested in production software. Yet, we see room for further development of generic solutions in conceptual multiscale modelling as well as in executing multiscale simulations. When seen in the light of the anticipated need for <italic>multi-scaling</italic> as opposed to weak scaling in order to reach exaflop/s performance, we conclude that such developments are very much needed for the optimal exploitation of future exascale HPC systems.</p></sec></body><back><ack><title>Acknowledgements</title><p>We acknowledge all participants of the workshop, <italic>Multiscale Computing, from the Desktop to the Exascale</italic>, that was held from 16 to 20 April 2018 in the Lorentz Center in Leiden, the Netherlands,<sup><xref ref-type="fn" rid="FN0003">3</xref></sup> for helping to shape the opinions expressed in this manuscript.</p></ack><fn-group><fn id="FN0001"><label>1</label><p>The values of the parameters chosen here correspond to an actual run with Palabos, see www.palabos.org and www.thrombus-vph.eu.</p></fn><fn id="FN0002"><label>2</label><p>Note that we do not assume advanced individual time-stepping algorithms in this example.</p></fn><fn id="FN0003"><label>3</label><p>For a list of participants, see <uri xlink:href="https://www.lorentzcenter.nl/lc/web/2018/993/info.php3?wsid=993&#x00026;venue=Snellius">https://www.lorentzcenter.nl/lc/web/2018/993/info.php3?wsid=993&#x00026;venue=Snellius</uri>.</p></fn></fn-group><sec id="s5"><title>Data accessibility</title><p>This article has no additional data.</p></sec><sec id="s6"><title>Authors' contributions</title><p>A.G.H. conceived the research that led to this discussion paper and wrote the first version of the manuscript; P.C. formulated the original weak scaling argument, and B.C. developed the scaling analysis; all authors contributed to the content of the manuscript and in revising the manuscript.</p></sec><sec id="s7" sec-type="COI-statement"><title>Competing interests</title><p>We declare we have no competing interests.</p></sec><sec id="s8"><title>Funding</title><p>P.C., D.C., S.P.Z. and A.G.H. have received funding from the European Union Horizon 2020 research and innovation programme under grant agreement no. 671564 (ComPat project). P.C., B.C. and A.G.H. have received funding from the European Union Horizon 2020 research and innovation programme under grant agreement no. 671351 (CompBioMed project). P.C., D.C. and A.G.H. have received funding from the European Union Horizon 2020 research and innovation programme under grant agreement no. 800925 (VECMA project).</p></sec><ref-list><title>References</title><ref id="RSTA20180144C1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name></person-group>
<year>2014</year>
<article-title>Multiscale modelling and simulation: a position paper</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>372</volume>, <elocation-id content-type="artnum">20130377</elocation-id> (<pub-id pub-id-type="doi">10.1098/rsta.2013.0377</pub-id>)<pub-id pub-id-type="pmid">24982256</pub-id></mixed-citation></ref><ref id="RSTA20180144C2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weinan</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Engquist</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Weiqing</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Vanden-Eijnden</surname><given-names>E</given-names></name></person-group>
<year>2007</year>
<article-title>Heterogeneous multiscale methods: a review</article-title>. <source>Commun. Comput. Phys.</source>
<volume>2</volume>, <fpage>367</fpage>&#x02013;<lpage>450</lpage>.</mixed-citation></ref><ref id="RSTA20180144C3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sloot</surname><given-names>PMA</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name></person-group>
<year>2010</year>
<article-title>Multi-scale modelling in computational biomedicine</article-title>. <source>Brief Bioinform.</source>
<volume>11</volume>, <fpage>142</fpage>&#x02013;<lpage>152</lpage>. (<pub-id pub-id-type="doi">10.1093/bib/bbp038</pub-id>)<pub-id pub-id-type="pmid">20028713</pub-id></mixed-citation></ref><ref id="RSTA20180144C4"><label>4</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Fish</surname><given-names>J</given-names></name></person-group>
<year>2009</year>
<source>Multiscale methods: bridging the scales in Science and Engineering</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref><ref id="RSTA20180144C5"><label>5</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Engquist</surname><given-names>B</given-names></name>, <name name-style="western"><surname>L&#x000f6;tstedt</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Runborg</surname><given-names>O</given-names></name></person-group>
<year>2009</year>
<source>Multiscale modeling and simulation in Science</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>.</mixed-citation></ref><ref id="RSTA20180144C6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karabasov</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Nerukh</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name></person-group>
<year>2014</year>
<article-title>Multiscale modelling: approaches and challenges</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>372</volume>, <elocation-id content-type="artnum">20130390</elocation-id> (<pub-id pub-id-type="doi">10.1098/rsta.2013.0390</pub-id>)<pub-id pub-id-type="pmid">24982248</pub-id></mixed-citation></ref><ref id="RSTA20180144C7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name>, <name name-style="western"><surname>Boon</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Succi</surname><given-names>S</given-names></name></person-group>
<year>2016</year>
<article-title>Bridging the gaps at the physics chemistry biology interface</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>374</volume>, <elocation-id content-type="artnum">20160335</elocation-id> (<pub-id pub-id-type="doi">10.1098/rsta.2016.0335</pub-id>)<pub-id pub-id-type="pmid">27698047</pub-id></mixed-citation></ref><ref id="RSTA20180144C8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Portegies Zwart</surname><given-names>S</given-names></name></person-group>
<italic>et al.</italic>
<year>2010</year>
<article-title>Simulating the universe on an intercontinental grid of supercomputers</article-title>. <source>IEEE Comput.</source>
<volume>43</volume>, <fpage>63</fpage>&#x02013;<lpage>70</lpage>. (<pub-id pub-id-type="doi">10.1109/mc.2009.419</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C9"><label>9</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Portegies Zwart</surname><given-names>S</given-names></name>, <name name-style="western"><surname>McMillan</surname><given-names>S</given-names></name></person-group>
<year>2018</year>
<source>Astrophysical recipes: the art of AMUSE</source>. <publisher-loc>Bristol, UK</publisher-loc>: <publisher-name>IOP Publishers</publisher-name>.</mixed-citation></ref><ref id="RSTA20180144C10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suter</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Groen</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name></person-group>
<year>2015</year>
<article-title>Chemically specific multiscale modeling of clay-polymer nanocomposites reveals intercalation dynamics, tactoid self-assembly and emergent materials properties</article-title>. <source>Adv. Mater.</source>
<volume>27</volume>, <fpage>966</fpage>&#x02013;<lpage>984</lpage>. (<pub-id pub-id-type="doi">10.1002/adma.201403361</pub-id>)<pub-id pub-id-type="pmid">25488829</pub-id></mixed-citation></ref><ref id="RSTA20180144C11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Silani</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Talebi</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hamouda</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Rabczuk</surname><given-names>T</given-names></name></person-group>
<year>2016</year>
<article-title>Nonlocal damage modelling in clay/epoxy nanocomposites using a multiscale approach</article-title>. <source>J. Comput. Sci.</source>
<volume>15</volume>, <fpage>18</fpage>&#x02013;<lpage>23</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2015.11.007</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Laurini</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Posocco</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Fermeglia</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pricl</surname><given-names>S</given-names></name></person-group>
<year>2016</year>
<article-title>MoDeNa nanotools: an integrated multiscale simulation workflow to predict thermophysical properties of thermoplastic polyurethanes</article-title>. <source>J. Comput. Sci.</source>
<volume>15</volume>, <fpage>24</fpage>&#x02013;<lpage>33</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2015.11.006</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bin</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>Z</given-names></name></person-group>
<year>2016</year>
<article-title>Multi-scale modeling and trans-level simulation from material meso-damage to structural failure of reinforced concrete frame structures under seismic loading</article-title>. <source>J. Comput. Sci.</source>
<volume>12</volume>, <fpage>38</fpage>&#x02013;<lpage>50</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2015.11.003</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suter</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Greenwell</surname><given-names>HC</given-names></name>, <name name-style="western"><surname>Cliffe</surname><given-names>S</given-names></name></person-group>
<year>2011</year>
<article-title>Rule based design of clay-swelling inhibitors</article-title>. <source>Energy Environ Sci.</source>
<volume>4</volume>, <fpage>4572</fpage>&#x02013;<lpage>4586</lpage>. (<pub-id pub-id-type="doi">10.1039/c1ee01280k</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Anzai</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Ohta</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Falcone</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name></person-group>
<year>2012</year>
<article-title>Optimization of flow diverters for cerebral aneurysms</article-title>. <source>J. Comput. Sci.</source>
<volume>3</volume>, <fpage>1</fpage>&#x02013;<lpage>7</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2011.12.006</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name>, <name name-style="western"><surname>Diaz-Zuccarini</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Graf</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Hunter</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kohl</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Tegner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Viceconti</surname><given-names>M</given-names></name></person-group>
<year>2013</year>
<article-title>Integrative approaches to computational biomedicine</article-title>. <source>Interface Focus</source>
<volume>3</volume>, <fpage>737</fpage>&#x02013;<lpage>738</lpage>. (<pub-id pub-id-type="doi">10.1098/rsfs.2013.0003</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garbey</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Rahman</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Berceli</surname><given-names>S</given-names></name></person-group>
<year>2015</year>
<article-title>A multiscale computational framework to understand vascular adaptation</article-title>. <source>J. Comput. Sci.</source>
<volume>8</volume>, <fpage>32</fpage>&#x02013;<lpage>47</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2015.02.002</pub-id>)<pub-id pub-id-type="pmid">25977733</pub-id></mixed-citation></ref><ref id="RSTA20180144C18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Itani</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Schiller</surname><given-names>UD</given-names></name>, <name name-style="western"><surname>Schmieschek</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Hetherington</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Bernabeu</surname><given-names>MO</given-names></name>, <name name-style="western"><surname>Chandrashekar</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Robertson</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name>, <name name-style="western"><surname>Groen</surname><given-names>D</given-names></name></person-group>
<year>2015</year>
<article-title>An automated multiscale ensemble simulation approach for vascular blood flow</article-title>. <source>J. Comput. Sci.</source>
<volume>9</volume>, <fpage>150</fpage>&#x02013;<lpage>155</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2015.04.008</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kohl</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Viceconti</surname><given-names>M</given-names></name></person-group>
<year>2010</year>
<article-title>The virtual physiological human: computer simulation for integrative biomedicine II</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>368</volume>, <fpage>2591</fpage>&#x02013;<lpage>2594</lpage>. (<pub-id pub-id-type="doi">10.1098/rsta.2010.0098</pub-id>)<pub-id pub-id-type="pmid">20439263</pub-id></mixed-citation></ref><ref id="RSTA20180144C20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Omholt</surname><given-names>SW</given-names></name>, <name name-style="western"><surname>Hunter</surname><given-names>PJ</given-names></name></person-group>
<year>2016</year>
<article-title>The Human Physiome: a necessary key for the creative destruction of medicine</article-title>. <source>Interface Focus</source>
<volume>6</volume>, <fpage>237</fpage>&#x02013;<lpage>243</lpage>. (<pub-id pub-id-type="doi">10.1098/rsfs.2016.0003</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paredes</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Rocha</surname><given-names>T</given-names></name>, <name name-style="western"><surname>de Carvalho</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Henriques</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mendes</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Cabete</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bianchi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Morais</surname><given-names>J</given-names></name></person-group>
<year>2015</year>
<article-title>The CardioRisk project: improvement of cardiovascular risk assessment</article-title>. <source>J. Comput. Sci.</source>
<volume>9</volume>, <fpage>39</fpage>&#x02013;<lpage>44</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2015.04.025</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zasada</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Haidar</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Graf</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Clapworthy</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Manos</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name></person-group>
<year>2012</year>
<article-title>IMENSE: An e-infrastructure environment for patient specific multiscale data integration, modelling and clinical treatment</article-title>. <source>J. Comput. Sci.</source>
<volume>3</volume>, <fpage>314</fpage>&#x02013;<lpage>327</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2011.07.001</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Falchetto</surname><given-names>GL</given-names></name></person-group>
<italic>et al.</italic>
<year>2014</year>
<article-title>The European Integrated Tokamak Modelling (ITM) effort: achievements and first physics results</article-title>. <source>Nuclear Fusion</source>
<volume>54</volume>, <elocation-id content-type="artnum">043018</elocation-id> (<pub-id pub-id-type="doi">10.1088/0029-5515/54/4/043018</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bruzzone</surname><given-names>AG</given-names></name></person-group>
<year>2015</year>
<article-title>Perspectives of modeling; applied simulation: modeling, algorithms and Simulations: advances and novel researches for problem-solving and decision-making in complex, multi-scale and multi-domain systems</article-title>. <source>J. Comput. Sci.</source>
<volume>10</volume>, <fpage>63</fpage>&#x02013;<lpage>65</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2015.06.004</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stevens</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Bony</surname><given-names>S</given-names></name></person-group>
<year>2013</year>
<article-title>Climate change. What are climate models missing?</article-title>
<source>Science</source>
<volume>340</volume>, <fpage>1053</fpage>&#x02013;<lpage>1054</lpage>.<pub-id pub-id-type="pmid">23723223</pub-id></mixed-citation></ref><ref id="RSTA20180144C26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Groen</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Zasada</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name></person-group>
<year>2014</year>
<article-title>Survey of multiscale and multiphysics applications and communities</article-title>. <source>Comput. Sci. Eng.</source>
<volume>16</volume>, <fpage>34</fpage>&#x02013;<lpage>43</lpage>. (<pub-id pub-id-type="doi">10.1109/MCSE.2013.47</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Portegies Zwart</surname><given-names>S</given-names></name></person-group>
<year>2018</year>
<article-title>Computational astrophysics for the future</article-title>. <source>Science</source>
<volume>361</volume>, <fpage>979</fpage>&#x02013;<lpage>980</lpage>. (<pub-id pub-id-type="doi">10.1126/science.aau3206</pub-id>)<pub-id pub-id-type="pmid">30190394</pub-id></mixed-citation></ref><ref id="RSTA20180144C28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Borgdorff</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Falcone</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Lorenz</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Bona-Casas</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name></person-group>
<year>2013</year>
<article-title>Foundations of distributed multiscale computing: formalization, specification, and analysis</article-title>. <source>J. Parallel Distrib. Comput.</source>
<volume>73</volume>, <fpage>465</fpage>&#x02013;<lpage>483</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jpdc.2012.12.011</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Borgdorff</surname><given-names>J</given-names></name></person-group>
<italic>et al.</italic>
<year>2014</year>
<article-title>Performance of distributed multiscale simulations</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>372</volume>, <elocation-id content-type="artnum">20130407</elocation-id> (<pub-id pub-id-type="doi">10.1098/rsta.2013.0407</pub-id>)<pub-id pub-id-type="pmid">24982258</pub-id></mixed-citation></ref><ref id="RSTA20180144C30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Falcone</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Kunzli</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Veen</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>A</given-names></name></person-group>
<year>2018</year>
<article-title>Multiscale modeling: recent progress and open questions</article-title>. <source>Multiscale Multidisc. Model. Exp. Des.</source>
<volume>1</volume>, <fpage>57</fpage>&#x02013;<lpage>68</lpage>. (<pub-id pub-id-type="doi">10.1007/s41939-017-0006-4</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alowayyed</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Groen</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name></person-group>
<year>2017</year>
<article-title>Multiscale computing in the exascale era</article-title>. <source>J. Comput. Sci.</source>
<volume>22</volume>, <fpage>15</fpage>&#x02013;<lpage>25</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2017.07.004</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Borgdorff</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name></person-group>
<year>2014</year>
<article-title>A framework for multi-scale modelling</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>372</volume>, <elocation-id content-type="artnum">20130378</elocation-id> (<pub-id pub-id-type="doi">10.1098/rsta.2013.0378</pub-id>)<pub-id pub-id-type="pmid">24982249</pub-id></mixed-citation></ref><ref id="RSTA20180144C33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Borgdorff</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mamonski</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bosak</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Kurowski</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Ben Belgacem</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Groen</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name></person-group>
<year>2014</year>
<article-title>Distributed multiscale computing with MUSCLE 2, the Multiscale Coupling Library and Environment</article-title>. <source>J. Comput. Sci.</source>
<volume>5</volume>, <fpage>719</fpage>&#x02013;<lpage>731</lpage>. (<pub-id pub-id-type="doi">10.1016/j.jocs.2014.04.004</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C34"><label>34</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hoenen</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Fazendeiro</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Scott</surname><given-names>BD</given-names></name>, <name name-style="western"><surname>Borgdorff</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Strand</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Coster</surname><given-names>DP</given-names></name></person-group>
<year>2013</year>
<source>Designing and running turbulence transport simulations using a distributed multiscale computing approach</source>. <publisher-loc>Mulhouse, France</publisher-loc>: <publisher-name>European Physical Society</publisher-name>.</mixed-citation></ref><ref id="RSTA20180144C35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Belgacem</surname><given-names>MB</given-names></name>, <name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Borgdorff</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mamonski</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Rycerz</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Harezlak</surname><given-names>D</given-names></name></person-group>
<year>2013</year>
<article-title>Distributed multiscale computations using the MAPPER framework</article-title>. <source>Procedia Comput. Sci.</source>
<volume>18</volume>, <fpage>1106</fpage>&#x02013;<lpage>1115</lpage>. (<pub-id pub-id-type="doi">10.1016/j.procs.2013.05.276</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Falcone</surname><given-names>JLJL</given-names></name>, <name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>A</given-names></name></person-group>
<year>2010</year>
<article-title>MML: towards a multiscale modeling language</article-title>. <source>Procedia Comput. Sci.</source>
<volume>1</volume>, <fpage>819</fpage>&#x02013;<lpage>826</lpage>. (<pub-id pub-id-type="doi">10.1016/j.procs.2010.04.089</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ben Belgacem</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Chopard</surname><given-names>B</given-names></name></person-group>
<year>2017</year>
<article-title>MUSCLE-HPC: a new high performance API to couple multiscale parallel applications</article-title>. <source>Future Gen. Comput. Syst.</source>
<volume>67</volume>, <fpage>72</fpage>&#x02013;<lpage>82</lpage>. (<pub-id pub-id-type="doi">10.1016/J.FUTURE.2016.08.009</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Neumann</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Flohr</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Arora</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Jarmatz</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Tchipev</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Bungartz</surname><given-names>HJ</given-names></name></person-group>
<year>2016</year>
<article-title>MaMiCo: Software design for parallel molecular-continuum flow simulations</article-title>. <source>Comput. Phys. Commun.</source>
<volume>200</volume>, <fpage>324</fpage>&#x02013;<lpage>335</lpage>. (<pub-id pub-id-type="doi">10.1016/J.CPC.2015.10.029</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Portegies Zwart</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Bedorf</surname><given-names>J</given-names></name></person-group>
<year>2016</year>
<article-title>Creating the virtual universe</article-title>. <source>IEEE Software</source>
<volume>33</volume>, <fpage>25</fpage>&#x02013;<lpage>29</lpage>. (<pub-id pub-id-type="doi">10.1109/MS.2016.113</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Piontek</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bosak</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Grabowski</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kopta</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kulczewski</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Szejnfeld</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Kurowski</surname><given-names>K</given-names></name></person-group>
<year>2016</year>
<article-title>Development of science gateways using QCG, lessons learned from the deployment on large scale distributed and HPC infrastructures</article-title>. <source>J. Grid Comput.</source>
<volume>14</volume>, <fpage>559</fpage>&#x02013;<lpage>573</lpage>. (<pub-id pub-id-type="doi">10.1007/s10723-016-9384-9</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alowayyed</surname><given-names>S</given-names></name></person-group>
<italic>et al.</italic>
<year>2018</year>
<article-title>Patterns for high performance multiscale computing</article-title>. <source>Future Gen. Comput. Syst.</source>
<volume>91</volume>, <fpage>335</fpage>&#x02013;<lpage>346</lpage>. (<pub-id pub-id-type="doi">10.1016/j.future.2018.08.045</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Groen</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Knap</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Neumann</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Suleimenova</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Veen</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Leiter</surname><given-names>K</given-names></name></person-group>
<year>2019</year>
<article-title>Mastering the scales: a survey on the benefits of multiscale computing software</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>377</volume>, <elocation-id content-type="artnum">20180147</elocation-id> (<pub-id pub-id-type="doi">10.1098/rsta.2018.0147</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Valcke</surname><given-names>S</given-names></name></person-group>
<italic>et al.</italic>
<year>2012</year>
<article-title>Coupling technologies for Earth System Modelling</article-title>. <source>Geosci. Model Dev.</source>
<volume>5</volume>, <fpage>1589</fpage>&#x02013;<lpage>1596</lpage>. (<pub-id pub-id-type="doi">10.5194/gmd-5-1589-2012</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C44"><label>44</label><mixed-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>B&#x000e9;dorf</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gaburov</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Fujii</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Nitadori</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Ishiyama</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Portegies Zwart</surname><given-names>S</given-names></name></person-group>, <year>2014</year>
<article-title>24.77 Pflops on a gravitational tree-code to simulate the Milky Way Galaxy with 18600&#x02009;GPUs</article-title>. In <conf-name>Proc. of the Int. Conf. for High Performance Computing, Storage and Analysis 2014</conf-name>, <conf-loc>New Orleans, LA</conf-loc>, <conf-date>16&#x02013;21 November</conf-date>, pp. <fpage>54</fpage>&#x02013;<lpage>65</lpage>. <publisher-name>IEEE Press</publisher-name>.</mixed-citation></ref><ref id="RSTA20180144C45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hucka</surname><given-names>M</given-names></name></person-group>
<italic>et al.</italic>
<year>2003</year>
<article-title>The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models</article-title>. <source>Bioinformatics.</source>
<volume>19</volume>, <fpage>524</fpage>&#x02013;<lpage>531</lpage>. (<pub-id pub-id-type="doi">10.1093/bioinformatics/btg015</pub-id>)<pub-id pub-id-type="pmid">12611808</pub-id></mixed-citation></ref><ref id="RSTA20180144C46"><label>46</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lloyd</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Halstead</surname><given-names>MDB</given-names></name>, <name name-style="western"><surname>Nielsen</surname><given-names>PF</given-names></name></person-group>
<year>2004</year>
<article-title>Cell ML: its future, present and past</article-title>. <source>Progress Biophys. Mol. Biol.</source>
<volume>85</volume>, <fpage>433</fpage>&#x02013;<lpage>450</lpage>. (<pub-id pub-id-type="doi">10.1016/J.PBIOMOLBIO.2004.01.004</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nickerson</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Buist</surname><given-names>ML</given-names></name></person-group>
<year>2009</year>
<article-title>A physiome standards-based model publication paradigm</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>367</volume>, <fpage>1823</fpage>&#x02013;<lpage>1844</lpage>. (<pub-id pub-id-type="doi">10.1098/rsta.2008.0296</pub-id>)<pub-id pub-id-type="pmid">19380314</pub-id></mixed-citation></ref><ref id="RSTA20180144C48"><label>48</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garny</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Nickerson</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Santos</surname><given-names>RWD</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>AK</given-names></name>, <name name-style="western"><surname>McKeever</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Nielsen</surname><given-names>P MF</given-names></name>, <name name-style="western"><surname>Hunter</surname><given-names>PJ</given-names></name></person-group>
<year>2008</year>
<article-title>Cell ML and associated tools and techniques</article-title>. <source>Phil. Trans. R. Soc. A</source>
<volume>366</volume>, <fpage>3017</fpage>&#x02013;<lpage>3043</lpage>. (<pub-id pub-id-type="doi">10.1098/rsta.2008.0094</pub-id>)<pub-id pub-id-type="pmid">18579471</pub-id></mixed-citation></ref><ref id="RSTA20180144C49"><label>49</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nikishova</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Veen</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Zun</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name></person-group>
<year>2018</year>
<article-title>Uncertainty quantification of a multiscale model for in-stent restenosis</article-title>. <source>Cardiovasc. Eng. Technol.</source>
<volume>9</volume>, <fpage>1</fpage>&#x02013;<lpage>14</lpage>. (<pub-id pub-id-type="doi">10.1007/s13239-018-00372-4</pub-id>)<pub-id pub-id-type="pmid">29124548</pub-id></mixed-citation></ref><ref id="RSTA20180144C50"><label>50</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Neumann</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Bian</surname><given-names>X</given-names></name></person-group>
<year>2017</year>
<article-title>MaMiCo: Transient multi-instance molecular-continuum flow simulation on supercomputers</article-title>. <source>Comput. Phys. Commun.</source>
<volume>220</volume>, <fpage>390</fpage>&#x02013;<lpage>402</lpage>. (<pub-id pub-id-type="doi">10.1016/J.CPC.2017.06.026</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C51"><label>51</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jacob</surname><given-names>CR</given-names></name>, <name name-style="western"><surname>Beyhan</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Bulo</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Gomes</surname><given-names>ASP</given-names></name>, <name name-style="western"><surname>G&#x000f6;tz</surname><given-names>AW</given-names></name>, <name name-style="western"><surname>Kiewisch</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Sikkema</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Visscher</surname><given-names>L</given-names></name></person-group>
<year>2011</year>
<article-title>PyADF - A scripting framework for multiscale quantum chemistry</article-title>. <source>J. Comput. Chem.</source>
<volume>32</volume>, <fpage>2328</fpage>&#x02013;<lpage>2338</lpage>. (<pub-id pub-id-type="doi">10.1002/jcc.21810</pub-id>)<pub-id pub-id-type="pmid">21541961</pub-id></mixed-citation></ref><ref id="RSTA20180144C52"><label>52</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alowayyed</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Z&#x000e1;vodszky</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Azizi</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Hoekstra</surname><given-names>AG</given-names></name></person-group>
<year>2018</year>
<article-title>Load balancing of parallel cell-based blood flow simulations</article-title>. <source>J. Comput. Sci.</source>
<volume>24</volume>, <fpage>1</fpage>&#x02013;<lpage>7</lpage>. (<pub-id pub-id-type="doi">10.1016/J.JOCS.2017.11.008</pub-id>)</mixed-citation></ref><ref id="RSTA20180144C53"><label>53</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bhati</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Wan</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Wright</surname><given-names>DW</given-names></name>, <name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name></person-group>
<year>2017</year>
<article-title>Rapid, accurate, precise, and reliable relative free energy prediction using ensemble based thermodynamic integration</article-title>. <source>J. Chem. Theory Comput.</source>
<volume>13</volume>, <fpage>210</fpage>&#x02013;<lpage>222</lpage>. (<pub-id pub-id-type="doi">10.1021/acs.jctc.6b00979</pub-id>)<pub-id pub-id-type="pmid">27997169</pub-id></mixed-citation></ref><ref id="RSTA20180144C54"><label>54</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Coveney</surname><given-names>PV</given-names></name>, <name name-style="western"><surname>Wan</surname><given-names>S</given-names></name></person-group>
<year>2016</year>
<article-title>On the calculation of equilibrium thermodynamic properties from molecular dynamics</article-title>. <source>Phys. Chem. Chem. Phys.</source>
<volume>18</volume>, <fpage>30&#x02009;236</fpage>&#x02013;<lpage>30&#x02009;240</lpage>. (<pub-id pub-id-type="doi">10.1039/C6CP02349E</pub-id>)</mixed-citation></ref></ref-list></back></article>