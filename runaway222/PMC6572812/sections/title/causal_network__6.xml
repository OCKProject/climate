<?xml version="1.0" encoding="UTF-8"?>
<sec id="Sec6" class="sec">
 <div class="title" xmlns="http://www.w3.org/1999/xhtml">Causal network learning algorithms</div>
 <p id="Par17" xmlns="http://www.w3.org/1999/xhtml">For time series that are of a stochastic nature, CCM is less well suited. Multivariate extensions of GC fail if too many variables are considered or dependencies are contemporaneous due to time-sampling
  <span class="sup">
   <a ref-type="bibr" rid="CR24" href="#CR24">24</a>
  </span> and in other cases (see also the challenges section). Causal network learning algorithms of various types have been developed for the reconstruction of large-scale causal graphical models. They can be classified by their search architecture, that is, whether they start with an empty or fully connected graph, and the statistical criterion for removing or adding an edge. The common feature of these algorithms is that they assume the Markov condition mentioned above together with the Faithfulness assumption, which requires that all observed conditional independencies arise from the causal structure
  <span class="sup">
   <a ref-type="bibr" rid="CR12" href="#CR12">12</a>
  </span>. Taken together, these two conditions allow to infer information about causal interactions from testing which conditional independencies hold true for the observed data. For example, the PC algorithm
  <span class="sup">
   <a ref-type="bibr" rid="CR45" href="#CR45">45</a>
  </span> (named after its inventors Peter and Clark) and related approaches
  <span class="sup">
   <a ref-type="bibr" rid="CR23" href="#CR23">23</a>,
   <a ref-type="bibr" rid="CR24" href="#CR24">24</a>,
   <a ref-type="bibr" rid="CR46" href="#CR46">46</a>,
   <a ref-type="bibr" rid="CR47" href="#CR47">47</a>
  </span> start with a fully connected graph and test for the removal of a link between two variables iteratively based on conditioning sets of growing cardinality (Fig. 
  <a rid="Fig2" ref-type="fig" href="#Fig2">2c</a>). In this way also causal directions for contemporaneous links can often be assessed. Greedy equivalence search
  <span class="sup">
   <a ref-type="bibr" rid="CR48" href="#CR48">48</a>
  </span>, on the other hand, starts with an empty graph and iteratively adds edges. The statistical criterion for removing or adding an edge can either be a conditional independence test or a properly defined score function that quantifies the likelihood of a particular graph structure given the data. Conditional independencies can flexibly be tested with different types of tests: Linear conditional independence can be assessed with partial correlation, while a wealth of recent machine learning approaches on nonparametric tests addresses a wide range of independence and dependence types
  <span class="sup">
   <a ref-type="bibr" rid="CR24" href="#CR24">24</a>,
   <a ref-type="bibr" rid="CR49" href="#CR49">49</a>,
   <a ref-type="bibr" rid="CR50" href="#CR50">50</a>
  </span>. Score functions can be based on Bayesian or information-theoretic approaches. Sun et al.
  <span class="sup">
   <a ref-type="bibr" rid="CR51" href="#CR51">51</a>
  </span>, for example, cast causal network learning as an information-theoretic optimization problem. Causal network learning algorithms can incorporate time-order as a constraint (causes precede effects) and utilize a set of causal orientation rules to identify causal directions. The PC-based method PCMCI
  <span class="sup">
   <a ref-type="bibr" rid="CR23" href="#CR23">23</a>,
   <a ref-type="bibr" rid="CR24" href="#CR24">24</a>
  </span> applied in Fig. 
  <a rid="Fig1" ref-type="fig" href="#Fig1">1a</a> addresses the particular challenges of autocorrelated high-dimensional and nonlinear time series data based on a condition-selection step (PC), followed by the momentary conditional independence (MCI) test. As illustrated in Box 
  <a rid="Sec8" ref-type="sec" href="#Sec8">1</a>, some network learning approaches, e.g., FCI
  <span class="sup">
   <a ref-type="bibr" rid="CR12" href="#CR12">12</a>
  </span>, account for unobserved direct common drivers and can still partially identify which links must be causal. Causal network learning algorithms have started to be applied in Earth system sciences only recently, mainly focusing on climate science
  <span class="sup">
   <a ref-type="bibr" rid="CR23" href="#CR23">23</a>,
   <a ref-type="bibr" rid="CR25" href="#CR25">25</a>,
   <a ref-type="bibr" rid="CR52" href="#CR52">52</a>,
   <a ref-type="bibr" rid="CR53" href="#CR53">53</a>
  </span>.
 </p>
</sec>
