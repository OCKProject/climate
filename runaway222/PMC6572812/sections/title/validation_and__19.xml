<?xml version="1.0" encoding="UTF-8"?>
<sec id="Sec20" class="sec">
 <div class="title" xmlns="http://www.w3.org/1999/xhtml">Validation and a benchmark platform</div>
 <p id="Par42" xmlns="http://www.w3.org/1999/xhtml">Method development and comparison require benchmark datasets with known causal ground truth for validation. Ideally, such ground truth comes from expert knowledge on real data or real experiments that can also be used for falsification of causal relationships predicted from observational causal inference methods. Unfortunately, in Earth system sciences such datasets currently exist only for expert-labeled causal relations among few variables (e.g., some bivariate examples in ref. 
  <span class="sup">
   <a ref-type="bibr" rid="CR92" href="#CR92">92</a>
  </span>). To some extent, out-of-sample predictions can provide partial validation, but the main alternative in Earth system sciences is experiments from physical simulation models. Such experiments, however, are computationally expensive and carry the challenge how these have to be designed. A more tractable approach is to generate synthetic data with simple model systems that mimic properties and challenges of geoscientific data, but where the underlying ground truth is known. These can then be used to study the performance of causal inference methods for different challenges in realistic finite sample situations. From a practitionerâ€™s perspective, it is important to find out which method is best suited for a particular task with particular challenges and for a particular set of assumptions. Synthetic data, adapted to the problem at hand, can be used to choose the right method including method parameters. As a first step to close the gap between method users and developers, we accompany this Perspective by a causality benchmark platform (
  <span ext-link-type="uri" xlink:href="http://www.causeme.net" class="ext-link" xmlns:xlink="http://www.w3.org/1999/xlink">causeme.net</span>) with synthetic models mimicking real data challenges on which causal inference methods can be compared. Next to method comparison, the platform also calls for submissions of real and modeled data sets where the causal structure is known with high confidence. Insights from such benchmark studies are relevant also for many other fields.
 </p>
</sec>
