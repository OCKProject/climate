<?xml version="1.0" encoding="UTF-8"?><html xmlns:h="http://www.w3.org/1999/xhtml" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
   <div class="sec" title="sec">
      <div xmlns="http://www.w3.org/1999/xhtml">Stylized model of a human-environment tipping element</div>
      <p xmlns="http://www.w3.org/1999/xhtml">We use the mathematical framework of Markov Decision Processes
         <span><a>45</a>,
            <a>46</a></span>, in which an agent makes decisions about how to interact with its environment (Fig. 
         <a>1a</a>). Our particular environment can reside in either a prosperous state, which provides
         immediate rewards (also called payoffs) to the agent, or a degraded state, from which
         the agent receives no payoff. At each time step, the agent chooses between two actions
         
         <span>a</span>, exerting either a high or low pressure on the environment. Depending on the current
         state 
         <span>s</span>, the current action 
         <span>a</span> and the subsequent state 
         <span>s′</span>, the agent receives an immediate reward 
         <span>r</span> (Fig. 
         <a>1b</a>). At the prosperous state, taking the low pressure action the agent is guaranteed
         to receive reward 
         <span>r</span><sub>l</sub> and remain at the prosperous state. However, taking the high pressure action, the
         agent may receive reward 
         <span>r</span><sub>h</sub> (which is typically larger than 
         <span>r</span><sub>l</sub>), but risks triggering a collapse of the environment to the degraded system state
         with non-zero probability 
         <span>δ</span> and no immediate reward at all. From there, only the low pressure action opens the
         option to recover to the prosperous state with non-zero probability 
         <span>ρ</span>.
         
         <div><span>Fig. 1</span><div>
               <p>Conceptual model of a human-environment tipping element. 
                  <span>a</span> Agent-environment interface: based on the state information and received reward,
                  the agent chooses an action 
                  <span>a</span> from its actions set to gain rewards. 
                  <span>b</span> The transition graph gives state transition probabilities and corresponding rewards
                  for all triples of state 
                  <span>s</span>, action 
                  <span>a</span>, next state 
                  <span>s</span>′, i.e., in state 
                  <span>s</span> the agent takes action 
                  <span>a</span> and moves to state 
                  <span>s</span>′. 
                  <span>c</span> Risky and cautious policies including the resulting Markov chains as a transition
                  graph
                  
               </p>
            </div>
            <div></div>
         </div>
      </p>
      <p xmlns="http://www.w3.org/1999/xhtml">For example, the high pressure action could correspond to emitting a business-as-usual
         amount of carbon to the atmosphere yielding a reward of high, short-term economic
         output as long as the system has not tipped. The low pressure action resembles emitting
         a reduced amount of carbon, assuming a lower short-term economic output for the guarantee
         to not trigger climate tipping elements into a disastrous state.
      </p>
      <p xmlns="http://www.w3.org/1999/xhtml">A policy 
         <span>π</span> is a function that specifies what action 
         <span>a</span> to apply at a system state 
         <span>s</span>. The agent receives reward 
         <span>r</span><sub><span>t</span></sub> at time step 
         <span>t</span>. The value 
         <span>v</span><sub><span>π</span></sub>(
         <span>s</span>) of a state 
         <span>s</span> under a given policy 
         <span>π</span> is given by the expected value of the normalized accumulated discounted rewards 
         <span>r</span> with discount factor 0 ≤ 
         <span>γ</span> ≤ 1 when starting in state 
         <span>S</span><sub>0</sub> = 
         <span>s</span> and following policy 
         <span>π</span>:
         
         <div><span>1</span><div><span>\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts}
                  \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek}
                  \setlength{\oddsidemargin}{-69pt} \begin{document}$$v_\pi \left( s \right) = {\Bbb
                  E}_\pi \left[ {\mathop {{{\mathrm{lim}}}}\limits_{T \to \infty } \frac{{\mathop {\sum}\nolimits_{t
                  = 0}^T {\gamma ^tr_t} }}{{\mathop {\sum}\nolimits_{t = 0}^T {\gamma ^t} }}\left| {S_0
                  = s} \right.} \right].$$\end{document}</span><div>
                  <div>
                     <div><span>v</span></div>
                     <div><span>π</span></div>
                  </div>
                  <div>
                     <div><span>s</span></div>
                  </div><span>=</span><div>
                     <div><span>E</span></div>
                     <div><span>π</span></div>
                  </div>
                  <div>
                     <div>
                        <div>
                           <div><span>lim</span></div>
                           <div><span>T</span><span>→</span><span>∞</span></div>
                        </div>
                        <div>
                           <div>
                              <div>
                                 <div><span>∑</span></div>
                                 <div><span>t</span><span>=</span><span>0</span></div>
                                 <div><span>T</span></div>
                              </div>
                              <div>
                                 <div><span>γ</span></div>
                                 <div><span>t</span></div>
                              </div>
                              <div>
                                 <div><span>r</span></div>
                                 <div><span>t</span></div>
                              </div>
                           </div>
                           <div>
                              <div>
                                 <div><span>∑</span></div>
                                 <div><span>t</span><span>=</span><span>0</span></div>
                                 <div><span>T</span></div>
                              </div>
                              <div>
                                 <div><span>γ</span></div>
                                 <div><span>t</span></div>
                              </div>
                           </div>
                        </div>
                        <div>
                           <div>
                              <div>
                                 <div><span>S</span></div>
                                 <div><span>0</span></div>
                              </div><span>=</span><span>s</span></div>
                        </div>
                     </div>
                  </div><span>.</span></div>
               <div></div>
            </div>
         </div>
      </p>
      <p xmlns="http://www.w3.org/1999/xhtml">Note that the discount factor actually denotes the farsightedness of the agent. Thus,
         
         <span>γ</span> = 1 corresponds to no discounting (weighting all rewards equally regardless of when
         they are expected), whereas 
         <span>γ</span> = 0 corresponds to completely myopic, fully discounting agents.
         
      </p>
   </div>
</html>