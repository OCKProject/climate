<?xml version="1.0" encoding="UTF-8"?>
<sec id="Sec2" class="sec">
 <div class="title" xmlns="http://www.w3.org/1999/xhtml">Methods</div>
 <sec id="Sec3" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Setting</div>
  <p id="Par15" xmlns="http://www.w3.org/1999/xhtml">This mixed methods study took place in the Los Angeles County + University of Southern California (LAC+USC) Medical Center ED. LAC+USC is a large public hospital owned and operated by the LAC Department of Health Services and is a major teaching hospital. The University of Southern California (USC) provides most of the attending services at the hospital. The ED is staffed by approximately 60 attending physicians and 70 ED residents. With over 100,000 patient visits per year, this ED is one of the largest in the country and serves predominantly uninsured and underinsured populations. The median LOS in 2016 was approximately 9 h for admitted patients and 6 h for discharged patients, which is well above the LOS observed in other EDs serving un- and underinsured populations [
   <a ref-type="bibr" rid="CR30" href="#CR30">30</a>]. Additionally, there was substantial variation between providers in LOS and utilization metrics; for example, the interquartile range (IQR) of the median LOS per provider was 6–9 h and the IQR of the % patients with a CT scan was 10–30%.
  </p>
 </sec>
 <sec id="Sec4" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Human-centered design approach</div>
  <p id="Par16" xmlns="http://www.w3.org/1999/xhtml">We employed a human-centered design approach to inform the development of a performance feedback dashboard for attending physicians. The goal of the performance dashboard was to reduce overall LOS by changing attending physicians’ practice patterns. Before developing the initial design, we performed a series of 60-min semi-structured interviews with ED leadership and ED attending physicians. Leadership interviews were performed to understand the strategic goals of the department and how a dashboard with specific performance measures could help attain those goals. Attending physician interviews were conducted to obtain end-user input on the measures to include and the design and functionality of the dashboard as well as to identify barriers to its implementation. The interview guides were developed specifically for this study (Additional file 
   <a rid="MOESM1" ref-type="media" href="#MOESM1">1</a>).
  </p>
  <p id="Par17" xmlns="http://www.w3.org/1999/xhtml">In total, two leadership and six attending physician interviews were performed to inform the initial design. The ED leaders were ED physicians whose primary responsibility was leadership but who also worked as attending physicians in this ED. We used a purposive sampling strategy to ensure a diverse representation of ED physicians. We included both male and female physicians with various levels of experience and different responsibilities (e.g. clinical, educational, academic, and administrative). Physicians employed by both USC and by the LA County Department of Health Services were included because incentive structures are different in the two settings. All participants were required to have worked in this ED for a minimum of 2 years to ensure they had a thorough understanding of the ED workflow. Twelve attending physicians were invited to participate, of which six completed interviews. Interviews were recorded and transcribed before analysis. Interviewees received a $25 gift card for their participation.</p>
  <p id="Par18" xmlns="http://www.w3.org/1999/xhtml">All transcripts were analyzed for comments related to relevant peer-comparison metrics, desired functionality, and perceived barriers. Based on the interview results, a guide was developed that specified design requirements for the performance dashboard. A team with experience in human-centered design developed (DW and KP) a front-end prototype in Axure RP PRO 8 (Axure Software Solutions, Inc., San Diego, CA). The design-team (DW, KP, ESC, WKvD) met on a bi-weekly basis to review the design and discuss appropriate solutions for design challenges. After the front-end prototype was developed using insights from the initial round of interviews, additional feedback was sought in a second round of 45-min interviews with five attending physicians, during which iterative improvements were made to the design.</p>
 </sec>
 <sec id="Sec5" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Survey</div>
  <p id="Par19" xmlns="http://www.w3.org/1999/xhtml">We conducted pre- and post-surveys (Additional file 
   <a rid="MOESM2" ref-type="media" href="#MOESM2">2</a>) 6 weeks apart to test whether the performance feedback dashboard (1) increases physicians’ motivation to make faster decisions about the patients’ management and (2) decreases physicians’ likelihood to order tests in a specific scenario; two outcomes that could lead to reductions in ED LOS. Screenshots of the prototype dashboard developed during the human-centered design process were displayed prior to the post-survey. Questions related to the culture and social network in the ED were included in the pre-survey, and feedback on the usability of the dashboard was obtained in the post-survey. Both surveys were hosted in Qualtrics (Provo, UT) and were electronically distributed to the 56 attending physicians that were active in the ED at the time the pre-survey was sent out. The pre-survey was sent out on January 1, 2018, with three reminders in the following month. The post-survey was sent out on March 13, 2018, with two reminders in the following month. Two OpenTable gift cards with $150 value were raffled among participants as an incentive to complete the pre-survey. In the post-survey, a $5 Starbucks gift card was offered to each participant as an incentive.
  </p>
 </sec>
 <sec id="Sec6" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Measures</div>
  <p id="Par20" xmlns="http://www.w3.org/1999/xhtml">The primary outcome was the effect of the performance feedback dashboard on physicians’ motivation to make a quick disposition decision (i.e. the moment the physician decides whether the patients will be admitted to the hospital or discharged from the ED). To measure motivation, we used a set of measures derived from Self-Determination Theory which posits that perceived value/usefulness, competence, and autonomy support are important factors for motivation [
   <a ref-type="bibr" rid="CR15" href="#CR15">15</a>]. We used the Value/Usefulness and Competence subscales of the Intrinsic Motivation Inventory and the six-question version of the Work Climate Questionnaire for Perceived Autonomy Support [
   <a ref-type="bibr" rid="CR31" href="#CR31">31</a>], which have been used and validated in a variety of settings including educational settings, sports psychology, and the workplace environment [
   <a ref-type="bibr" rid="CR32" href="#CR32">32</a>, 
   <a ref-type="bibr" rid="CR33" href="#CR33">33</a>]. All questions were rated on a seven-point Likert scale ranging from “Not at all true” to “Very true”. These questions were included in both the pre- and post-survey.
  </p>
  <p id="Par21" xmlns="http://www.w3.org/1999/xhtml">To estimate changes in physicians’ practice patterns, we focused on test-ordering, which is a known predictor of LOS and is directly influenced by physicians’ decisions [
   <a ref-type="bibr" rid="CR1" href="#CR1">1</a>, 
   <a ref-type="bibr" rid="CR5" href="#CR5">5</a>]. To assess physicians’ likelihood of ordering diagnostics tests, we included a vignette adapted from Kool et al. [
   <a ref-type="bibr" rid="CR34" href="#CR34">34</a>] in which the need for imaging is purposefully ambiguous. After reviewing the vignette, physicians were asked whether they would order diagnostic imaging, and if so, what type of diagnostic order. The vignette was included in both the pre- and post-survey.
  </p>
  <p id="Par22" xmlns="http://www.w3.org/1999/xhtml">To evaluate the design of the prototype performance dashboard in the post-survey, we assessed the perceived usefulness and ease of use based on the Technology Acceptance Model [
   <a ref-type="bibr" rid="CR20" href="#CR20">20</a>]. The Technology Acceptance Model has been used extensively to evaluate physician acceptance of health technology [
   <a ref-type="bibr" rid="CR35" href="#CR35">35</a>, 
   <a ref-type="bibr" rid="CR36" href="#CR36">36</a>]. Questions such as “I believe that the performance feedback tool is flexible to interact with” were scored on a seven-point Likert-scale ranging from “Disagree strongly” and “Agree strongly”. We also assessed physicians’ perception of the importance of the metrics included in the dashboard and physicians’ perceptions of their ability to affect these measures on the same seven-point Likert-scale. Lastly, we asked physicians a Net Promotor Score question, which is widely used in various industries to understand the quality of the product or service: 
   <span class="italic">“How likely are you to recommend this performance feedback tool to your colleagues and peers to visualize and improve their performance as ED physicians?”</span> with an eleven-point response scale ranging from “Not likely at all” to “Extremely likely.” We also asked for suggestions to improve the tool and inquired about additional physician comments.
  </p>
  <p id="Par23" xmlns="http://www.w3.org/1999/xhtml">To assess the ED culture, we asked physicians about their perceptions of the social, technical, and environmental aspects of their workplace in the pre-survey. We used the first thirteen questions of the Safety Attitudes and Safety Climate Questionnaire, which has been extensively psychometrically validated [
   <a ref-type="bibr" rid="CR21" href="#CR21">21</a>, 
   <a ref-type="bibr" rid="CR37" href="#CR37">37</a>]. These thirteen questions specifically address the teamwork and safety climate. All questions were scored on a five-point Likert scale ranging from “Disagree strongly” to “Agree strongly”, with an N/A option for each question. To describe the underlying social network, we included the question “
   <span class="italic">Who do you discuss problems with at work?</span>” which returns social network data indicating specifically who communicates with whom in the organization [
   <a ref-type="bibr" rid="CR38" href="#CR38">38</a>]. Understanding this network is important if there are cultural barriers that limit the adoption of an intervention [
   <a ref-type="bibr" rid="CR39" href="#CR39">39</a>]. Up to seven names could be selected from a pre-specified list of ED provider-names, including those in leadership roles. See Additional file 
   <a rid="MOESM2" ref-type="media" href="#MOESM2">2</a> for the complete surveys.
  </p>
 </sec>
 <sec id="Sec7" class="sec">
  <div class="title" xmlns="http://www.w3.org/1999/xhtml">Statistical analysis</div>
  <p id="Par24" xmlns="http://www.w3.org/1999/xhtml">To assess the importance of end-user involvement in the development process, respondents were categorized as (1) directly involved in the dashboard development (interviewees, co-author MM), (2) not directly involved in the development but ≥30% of their network was involved, or (3) not directly involved and &lt; 30% of their network was involved. The social network question was used to create a model that assessed the level of exposure that each participant had to people who were involved in the development of the dashboard. Exposure was defined as the percentage of nominated people who were involved in the development [
   <a ref-type="bibr" rid="CR23" href="#CR23">23</a>].
  </p>
  <p id="Par25" xmlns="http://www.w3.org/1999/xhtml">Centrality measures for the social network analysis (in-degree, out-degree, closeness, and betweenness) were calculated using the igraph library [
   <a ref-type="bibr" rid="CR40" href="#CR40">40</a>]. Out-degree is the number of people someone selected in the network question; in-degree is the number of times a person was selected by someone else; closeness measures the average distance someone has to everyone else in the network; and betweenness is a measure of how often each person lies on the shortest paths connecting others [
   <a ref-type="bibr" rid="CR41" href="#CR41">41</a>]. People with high betweenness scores are thought to be key in the early adoption of innovation, people with a high in-degree are ‘popular’ figures in the network that are important for the innovation to spread to the early majority, and people with a high closeness score are thought to be important for an innovation to spread to a large amount of people [
   <a ref-type="bibr" rid="CR41" href="#CR41">41</a>].
  </p>
  <p id="Par26" xmlns="http://www.w3.org/1999/xhtml">Descriptive statistics were used to describe the sample and survey results. Pre- and post-survey data were compared using a Wilcoxon Ranked Sum test for continuous measures and the McNemar’s Test for categorical variables. The Kruskal-Wallis Test was used to compare change-scores between groups. A Spearman correlation coefficient was used to describe relations between continuous data. Rho correlations coefficients of &gt; 0.7 were considered strong correlations, coefficients between 0.5–0.7 were considered moderate, and coefficients between 0.3 and 0.5 were considered weak. A 
   <span class="italic">p</span>-value of &lt; 0.05 was considered significant. Analyses were performed in SAS 9.4 (Cary, NC).
  </p>
 </sec>
</sec>
