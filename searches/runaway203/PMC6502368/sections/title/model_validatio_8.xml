<?xml version="1.0" encoding="UTF-8"?>
<sec id="s2d" class="sec">
 <span class="label" xmlns="http://www.w3.org/1999/xhtml">2.4.</span>
 <div class="title" xmlns="http://www.w3.org/1999/xhtml">Model validation</div>
 <p xmlns="http://www.w3.org/1999/xhtml">Model performance was assessed using the area under the curve (AUC) statistic [
  <a rid="RSOS182111C56" ref-type="bibr" href="#RSOS182111C56">56</a>], derived from receiver operating characteristic (ROC) analyses, and the true skill statistic (TSS) [
  <a rid="RSOS182111C57" ref-type="bibr" href="#RSOS182111C57">57</a>]. In general, evaluation criteria for AUC values have been interpreted as excellent (0.90–1.00), very good (0.8–0.9), good (0.7–0.8), fair (0.6–0.7) and poor (0.5–0.6), whereas a score of less than 0.5 is worse than one would expect from a random model [
  <a rid="RSOS182111C58" ref-type="bibr" href="#RSOS182111C58">58</a>]. Evaluation scores for TSS range from −1 to 1, where a score of 1 demonstrates perfect model performance, while less than 0 is considered worse than a random model [
  <a rid="RSOS182111C57" ref-type="bibr" href="#RSOS182111C57">57</a>]. As we used two ENM constructs (LIG- and modern-trained), and generated respective forecasting and hindcasting (backtesting) projections, we calculated the AUC scores using LIG occurrences and modern occurrences as an evaluation dataset for model projections to other climate scenarios (i.e. modern-trained and LIG projection; LIG-trained and modern projection).
 </p>
</sec>
