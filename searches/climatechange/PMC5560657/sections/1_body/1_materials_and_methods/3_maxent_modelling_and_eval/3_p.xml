<?xml version="1.0" encoding="UTF-8"?>
<p class="p">The performance of each species model was evaluated by setting the random test percentage to 30; seventy percent of the observations were randomly selected as a training data set and the remaining 30% were used to test the model. In order to estimate the robustness of model outputs, the replicates option was set at 10, so that the selection of training and test data from the species observations, as well as the calculation of performance statistics, were repeated. Model fit was evaluated as the average area under the curve (AUC) of the receiver operating characteristic (ROC) across all ten model replicates. AUC is a widely-used, threshold-independent measure of model evaluation, which represents the probability that the model classifier will correctly identify a randomly-chosen true presence [
 <xref rid="pone.0183132.ref030" ref-type="bibr" class="xref">30</xref>â€“
 <xref rid="pone.0183132.ref031" ref-type="bibr" class="xref">31</xref>]. AUC values range from 0 to 1, with a value of 0.5 representing a model that correctly identifies true species occurrences equal to a random guess, and a value of 1 indicating a perfect model fit. While an alternative jackknife or "leave-one-out" procedure has been suggested to evaluate model accuracy for small sample sizes [
 <xref rid="pone.0183132.ref032" ref-type="bibr" class="xref">32</xref>], the number of observations for both species in the current study is at least one and a half times greater than in the study for which that procedure was recommended. Thus, to prevent overoptimistic estimates of predictive power, we opted to use the standard AUC calculations.
</p>
