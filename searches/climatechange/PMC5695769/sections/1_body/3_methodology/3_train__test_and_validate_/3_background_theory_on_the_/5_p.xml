<?xml version="1.0" encoding="UTF-8"?>
<p class="p">The generalization performance of any statistical model depends on its ability to produce accurate predictions on an independent test sample [
 <xref rid="pone.0188033.ref043" ref-type="bibr" class="xref">43</xref>, 
 <xref rid="pone.0188033.ref044" ref-type="bibr" class="xref">44</xref>]. Bias-variance trade-off is central for minimizing the generalization error [
 <xref rid="pone.0188033.ref043" ref-type="bibr" class="xref">43</xref>]. Cross-validation is one of the most widely used resampling techniques for balancing bias-variance trade-off in statistical models [
 <xref rid="pone.0188033.ref011" ref-type="bibr" class="xref">11</xref>, 
 <xref rid="pone.0188033.ref043" ref-type="bibr" class="xref">43</xref>]. More specifically, 
 <italic class="italic">k</italic>-fold cross-validation is commonly leveraged to assess statistical models’ predictive accuracy (aka generalization performance). 
 <italic class="italic">k</italic>-fold cross-validation involves randomly subdividing the dataset into 
 <italic class="italic">k</italic> (equally-sized) subsets. In each iteration, the statistical model is fitted to the training subset that includes all observations except for the 
 <italic class="italic">k</italic>
 <sup class="sup">
  <italic class="italic">th</italic>
 </sup> held-out sample. The predictive accuracy is then calculated based on the model’s performance on the 
 <italic class="italic">k</italic>
 <sup class="sup">
  <italic class="italic">th</italic>
 </sup> held-out subset. This process is repeated until all data is used at least once and the average performance across all iterations is recorded as a measure of the model’s predictive performance.
</p>
