<?xml version="1.0" encoding="UTF-8"?>
<p class="p">The predictive ability of all models was evaluated by using the area under the receiver-operator curve (AUC) which is a threshold-independent measure of predictive accuracy based only on the ranking of locations (Fielding and Bell, 
 <xref rid="B21" ref-type="bibr" class="xref">1997</xref>). AUC is interpreted as the probability that a randomly chosen presence location is ranked higher than a randomly chosen background point (Merow et al., 
 <xref rid="B52" ref-type="bibr" class="xref">2013</xref>). This approach corresponds to finding a model that identifies attributes of the species distribution and not artifacts of noise such as sampling bias. The AUC statistic measures the quality of a fitted model when calculated for the training data set, and it is a measure of the quality of prediction for novel environments. The AUC for our models ranged from good (0.7) to near perfect discrimination (â‰¥0.9) (Table 
 <xref ref-type="supplementary-material" rid="SM1" class="xref">S1</xref>). If species had samples sizes of &lt;25 records, models were tested and trained using the re-sampling K-fold cross-validation method, where the data are split into training data (to fit the model) and test data (to evaluate model predictions). Using this approach, the data are split into K independent subsets, where K is the number of replicates you specify, and one subset is left out while the model is fit to the other n-1 subsets. The subset withheld is used to test the model and calculate AUC (Elith et al., 
 <xref rid="B16" ref-type="bibr" class="xref">2011</xref>). For species with samples sizes of &gt;100, 25% of the training data were set aside for testing. If the number of species occurrence points was 10 or less, one data point was used for testing (Pearson et al., 
 <xref rid="B57" ref-type="bibr" class="xref">2007</xref>).
</p>
