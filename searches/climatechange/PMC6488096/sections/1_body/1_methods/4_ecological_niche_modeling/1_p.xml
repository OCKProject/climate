<?xml version="1.0" encoding="UTF-8"?>
<p class="p">Ecological niche models reflecting current and future climate conditions were built using DesktopGARP ver. 1.1.3 (DG) [
 <xref rid="pntd.0007322.ref037" ref-type="bibr" class="xref">37</xref>]. Although more contemporary methods of building ENMs are available, GARP was chosen for this study because of its demonstrated ability to produce models that are transferable to novel time periods [
 <xref rid="pntd.0007322.ref052" ref-type="bibr" class="xref">52</xref>]. Furthermore, while other methods of estimating species distributions are known to overfit geographic models to training data, an issue which could exacerbate any spurious errors in our disaggregated occurrence data, GARP has been shown in other studies to exclude a degree of outlier data from geographic predictions [
 <xref rid="pntd.0007322.ref053" ref-type="bibr" class="xref">53</xref>,
 <xref rid="pntd.0007322.ref054" ref-type="bibr" class="xref">54</xref>]. LI point records and environmental coverage datasets were prepared for modeling using the ‘GARPTools’ package (co-developed by C.G. Haase and J.K. Blackburn) in the program R (ver. 3.3.1). Spatially unique LI records (n = 478) were split into 75% training (n = 358) and 25% testing datasets (n = 119) for ten randomly selected iterations; training datasets were used in model building and testing datasets were used to compute model accuracy metrics [
 <xref rid="pntd.0007322.ref036" ref-type="bibr" class="xref">36</xref>,
 <xref rid="pntd.0007322.ref037" ref-type="bibr" class="xref">37</xref>,
 <xref rid="pntd.0007322.ref055" ref-type="bibr" class="xref">55</xref>,
 <xref rid="pntd.0007322.ref056" ref-type="bibr" class="xref">56</xref>]. Ten experiments were run in DG, each using one of the randomly selected LI training datasets and the full set of current environmental coverage variables (
 <xref rid="pntd.0007322.t002" ref-type="table" class="xref">Table 2</xref>). Each experiment was run for 200 models, allowing for a maximum of 1,000 iterations with a convergence limit of 0.01. Occurrence training data were internally partitioned in DG into 75% training/25% testing for model building and subset selection, and top models were selected using the ‘Best Subsets’ option, specifying a 10% hard omission threshold and 50% commission threshold [
 <xref rid="pntd.0007322.ref057" ref-type="bibr" class="xref">57</xref>]. The top ten best subsets models from each GARP experiment were summated with the GARPTools package to assess model agreement and accuracy. Model accuracy metrics for each GARP experiment were calculated from the 25% testing dataset withheld from the model building process. Three standard measures of accuracy, calculated in GARPTools, were used to compare best subsets from each experiment: receiver operator characteristic (ROC) curve with area under the curve (AUC), commission (i.e. false positives), and omission (i.e. false negatives) [
 <xref rid="pntd.0007322.ref058" ref-type="bibr" class="xref">58</xref>]. The AUC is an indicator of a model’s ability to predict areas of species presence versus absence, with an AUC of 0.5 indicating a model that performs no better than random, and an AUC of 1.0 indicating a perfect model [
 <xref rid="pntd.0007322.ref058" ref-type="bibr" class="xref">58</xref>]. We additionally performed partial ROC (pROC) analyses for model accuracy, a method which addresses some of the limitations identified in the classic ROC approach [
 <xref rid="pntd.0007322.ref059" ref-type="bibr" class="xref">59</xref>]. Partial ROC analyses were performed with Niche Toolbox (ver. 0.2.5.4), specifying an omission threshold of E = 10 and 1000 bootstrap replicates, where resulting AUC ratios &gt;1 indicate that model predictions are significantly better than random (p &lt; 0.01) [
 <xref rid="pntd.0007322.ref059" ref-type="bibr" class="xref">59</xref>,
 <xref rid="pntd.0007322.ref060" ref-type="bibr" class="xref">60</xref>].
</p>
