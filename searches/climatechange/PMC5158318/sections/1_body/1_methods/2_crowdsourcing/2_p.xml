<?xml version="1.0" encoding="UTF-8"?>
<p class="p">The CrowdFlower platform allowed us to: 1) collect reader-coded information for a large number of abstracts that could not be collected by text-mining or other means; 2) collect multiple (n = 7) independent assessments (“judgments”) about the narrativity of each abstract; and simultaneously 3) include human interpretation and discretion in the quantification of narrativity. We collected multiple judgments for each abstract as a means of quality-control, given that individual readers can perceive narrativity somewhat differently [
 <xref rid="pone.0167983.ref026" ref-type="bibr" class="xref">26</xref>].
</p>
