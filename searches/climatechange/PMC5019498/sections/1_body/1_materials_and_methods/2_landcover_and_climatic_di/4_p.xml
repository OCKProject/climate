<?xml version="1.0" encoding="UTF-8"?>
<p class="p">The decision tree is a logical model, so named due to its graphical representation in binary tree form, which shows how the response variable (in this case, the native landcover category) can be predicted by explanatory variables (here, the climatic variables) [
 <xref rid="pone.0162500.ref040" ref-type="bibr" class="xref">40</xref>]. The decision tree algorithm divides the initial dataset (grid cells) into homogeneous subsets, with one variable in each subdivision step (called in decision tree terminology a 
 <italic class="italic">node</italic>), until homogeneous and indivisible subsets remain (
 <italic class="italic">leaves</italic>), which are our clusters [
 <xref rid="pone.0162500.ref040" ref-type="bibr" class="xref">40</xref>]. Due to its progressive approach, the decision tree can be considered a hierarchical method, but starting with only one group (all sampling units) that is progressively subdivided to maximize the similarity within groups [
 <xref rid="pone.0162500.ref040" ref-type="bibr" class="xref">40</xref>]. This is the opposite approach to UPGMA whereby each sampling unit is progressively aggregated until only one group remains.
</p>
