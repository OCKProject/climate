<?xml version="1.0" encoding="UTF-8"?>
<p id="Par21" class="p">Climate data (mean, minimum and maximum temperature and precipitation) of each month separately in the time period 1985â€“2009 as well as the land cover data served as predictors for vector occurrences. In order to quantify the relationships between vector occurrences with climate and land cover variables and to map and project the occurrences under present and future climate conditions Boosted Regression Trees (BRTs) were used. Detailed descriptions of BRT are provided by Elith et al. [
 <xref ref-type="bibr" rid="CR14" class="xref">14</xref>] and Hastie et al. [
 <xref ref-type="bibr" rid="CR22" class="xref">22</xref>]. BRT combines regression trees and boosting. BRT attempts to minimize a loss function, which involves jointly optimizing the number of trees, learning rate, and tree complexity. The learning rate is used to shrink the contribution of each tree as it is added to the model. Slowing the learning rate increases the number of trees required. In general, a smaller learning rate (and consequently a larger number of trees) is preferable. Tree complexity (number of nodes in a tree) relates to the interaction order in the predictand. With increasing tree complexity, learning rate must be decreased if sufficient trees are to be fitted to minimize predictive error. The tree complexity should reflect the correct interaction order in the response variable. However, since an adequate tree complexity is usually unknown, it is best evaluated using independent data. As in Elith et al. [
 <xref ref-type="bibr" rid="CR14" class="xref">14</xref>] the optimal number of trees, learning rate and tree complexity were estimated with a cross-validation approach, using deviance reduction as performance measure. The 
 <italic class="italic">dismo</italic> and 
 <italic class="italic">gbm</italic> packages in R were used to assess the optimal number of boosting trees using 10-fold cross-validation. In the present study models were developed with 50% of the data, and were validated with the remaining data. Tree complexity of 2 up to 8, and learning rates of 0.005, 0.1 and 0.5 were evaluated.
</p>
