<?xml version="1.0" encoding="UTF-8"?>
<p id="p0105" class="p">Systematic assessment might seem to be a way to ensure objectivity. However, herein lies the thorny issue at the heart of uncertainty analysis: attempts to be systematic, for example by quantifying parametric uncertainty by using ranges of values, can result in ranges that are not informative, and even unrealistic (
 <xref rid="bb0060" ref-type="bibr" class="xref">Challinor et al., 2007</xref>). The same may be said of using multiple models. 
 <xref rid="f0015" ref-type="fig" class="xref">Fig. 3</xref> illustrates this issue. The range of all simulated events is an attempt to capture all possible events, yet the overlap is not only partial; models and model ensembles are collections of methodological choices and assumptions that may not explore the full range of possibilities (
 <xref rid="bb0475" ref-type="bibr" class="xref">Whitfield, 2013</xref>). Equally, the range of model results may extend beyond the realms of possibility (
 <xref rid="bb0355" ref-type="bibr" class="xref">Spiegelhalter and Riesch, 2011</xref>). Hence risk assessment with models should not be reduced to the process of equating multiple model outputs with a probability distribution. If a meaningful risk is to be calculated then a framework is needed, even if it exists only to highlight the limitations of the analysis. In 
 <xref rid="f0015" ref-type="fig" class="xref">Fig. 3</xref>, the real risk is represented by the ratio of “Adverse” to “All possible” events (i.e. (
 <italic class="italic">b</italic> + 
 <italic class="italic">c</italic>)/(
 <italic class="italic">a</italic> + 
 <italic class="italic">b</italic> + 
 <italic class="italic">c</italic> + 
 <italic class="italic">d</italic>)), which in this case is under-estimated by the model as being 
 <italic class="italic">c</italic>/(
 <italic class="italic">c</italic> + 
 <italic class="italic">d</italic> + 
 <italic class="italic">e</italic>).
</p>
