<?xml version="1.0" encoding="UTF-8"?>
<p class="p">Classification Trees (CTs) are predictive models which use a set of decision rules inferred from features (predictor) of the input data to determine group membership of the predictand. The model constitutes a hierarchical tree with root, decision, and leaf nodes. While having powerful analytical capabilities, it is recognized that CTs are prone to overfitting (Hastie et al., 
 <xref rid="wrcr23804-bib-0040" ref-type="ref" class="xref">2009</xref>). By aggregating predictions from an ensemble of CTs, wherein the influence of individual biases are offset by other ensemble members (EMs), a RF can generalize more successfully (Breiman, 
 <xref rid="wrcr23804-bib-0011" ref-type="ref" class="xref">1996</xref>, 
 <xref rid="wrcr23804-bib-0012" ref-type="ref" class="xref">2001</xref>). RFs are suitable for mixed data types and can handle collinear/interactive predictors. The ensemble is composed of multiple CTs, each developed using a bootstrapped sample of the original data. In addition, at each decision node a random subset of predictors is selected from which the most important split features are retained. This reduces correlation between individual trees and lessens the influence of noise/outliers. While the RF does not produce an explicit model revealing the decision process and relationship between predictors (as in the cases of CTs) it does return a measure of predictor importance.
</p>
