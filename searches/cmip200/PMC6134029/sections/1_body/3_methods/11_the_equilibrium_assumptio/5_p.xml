<?xml version="1.0" encoding="UTF-8"?>
<p id="Par125" class="p">For our AMIP5/CMIP5 simulations the ∆
 <italic class="italic">T</italic> averaging period is ≥ 25 years. For the HadGEM2 historical run, the standard deviation of d
 <italic class="italic">H</italic>/d
 <italic class="italic">t</italic> for 40–60°S, estimated from six different 20 year averaging periods, is 0.2 Wm
 <sup class="sup">−2</sup>(for a 10 year averaging period it is 0.4 Wm
 <sup class="sup">−2</sup>). This value of 0.2 Wm
 <sup class="sup">−2</sup> is clearly small compared to the AMIP5 net flux bias standard deviation of 6.6 Wm
 <sup class="sup">−2</sup> (~ 3%) and the CMIP5 net flux standard deviation of 3.7 Wm
 <sup class="sup">−2</sup> (~ 5%). It would also remain small even if mixed layer temperature tendencies associated with variability were several times larger in other models, which is not expected for an average over such a large region. Note also from Eq. 
 <xref rid="Equ26" ref-type="" class="xref">M18</xref> that this term would have acted as an additional noise term in our existing regression analyses, limiting the correlations. If this term were to be an important term in the budget, we would therefore not see such a strong CMIP5 SST bias on AMIP5 net flux bias correlation (
 <italic class="italic">r</italic> = 0.84).
</p>
