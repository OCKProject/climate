<?xml version="1.0" encoding="UTF-8"?>
<p class="p">One of the first calls for a more collaborative approach to model development arose from the World Modelling Summit for Climate Prediction [
 <xref rid="RSPA20150772C31" ref-type="bibr" class="xref">31</xref>]. A recent report from the US National Research Councils [
 <xref rid="RSPA20150772C32" ref-type="bibr" class="xref">32</xref>] also calls for a consolidation of climate modelling centres, and better links between weather and climate. With a number of colleagues here in Europe, I am currently writing a framework document for the creation of a new European Flagship Programme on Extreme Computing and Climate, designed to fund a significant new effort to create one or more global cloud-resolved climate models, to run the new generation of exascale computers when they emerge in the next decade, with the specific focus of understanding and simulating climate extremes (for example, understanding the drivers of the persistent jet-stream anomalies that caused extensive flooding over the UK in 2013/2014 and again in 2015/2016). In partnership with the existing climate centres, an initial goal will include the development of one or more global climate models with a global 1â€‰km grid to allow deep convective cloud systems to be partially resolved (and convective parametrizations switched off). However, research with limited domain models suggest that ultimately we should be striving for resolutions of a few hundred metres or less. This may seem unfeasible in the foreseeable future. However, if code can be developed where the bit lengths of model variables can be made to vary with spatial scale (consistent with the dependence of atmospheric Lyapunov exponents on horizontal scale [
 <xref rid="RSPA20150772C33" ref-type="bibr" class="xref">33</xref>]), then such a goal may become viable in the next decade. A formal presentation will be made to national and international funding agencies for such a programme in the coming year or so.
</p>
