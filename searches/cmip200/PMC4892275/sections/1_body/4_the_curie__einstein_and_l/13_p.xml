<?xml version="1.0" encoding="UTF-8"?>
<p class="p">I have discussed the case for pooling resources for building high-resolution climate models with next-generation exascale computing elsewhere [
 <xref rid="RSPA20150772C29" ref-type="bibr" class="xref">29</xref>]. In this context, one can perhaps make the point that the essential stochasticity of the closure problem for climate, as discussed above, does not require future generation supercomputers to be the paragons of determinism and precision that they have been in the past. This is particularly relevant when one considers that supercomputing performance is now limited by power consumption, not FLOP rate. Essentially, most of this power is needed to move data around in the supercomputerâ€”making a climate model bandwidth limited. As discussed in [
 <xref rid="RSPA20150772C30" ref-type="bibr" class="xref">30</xref>], relaxing the requirements for determinism and precision (where it is not needed) could allow one to increase model resolution without increasing power consumption. However, this will require some changes to current hardware configurations and design. Possible synergies with other areas of computational science (e.g. astrophysics, plasma physics and neuroscience) may help speed up the developments of new types of hybrid computing hardware with variable levels of precision and determinism.
</p>
