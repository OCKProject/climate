<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Biophys Rev</journal-id><journal-id journal-id-type="iso-abbrev">Biophys Rev</journal-id><journal-title-group><journal-title>Biophysical Reviews</journal-title></journal-title-group><issn pub-type="ppub">1867-2450</issn><issn pub-type="epub">1867-2469</issn><publisher><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin/Heidelberg</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6381359</article-id><article-id pub-id-type="publisher-id">499</article-id><article-id pub-id-type="doi">10.1007/s12551-019-00499-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>Bayesian statistical learning for big data biology</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7615-8523</contrib-id><name><surname>Yau</surname><given-names>Christopher</given-names></name><address><email>c.yau@bham.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Campbell</surname><given-names>Kieran</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7486</institution-id><institution-id institution-id-type="GRID">grid.6572.6</institution-id><institution>Institute of Cancer and Genomic Sciences, </institution><institution>University of Birmingham, </institution></institution-wrap>Birmingham, UK </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 5903 3632</institution-id><institution-id institution-id-type="GRID">grid.499548.d</institution-id><institution>The Alan Turing Institute, </institution></institution-wrap>London, UK </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2288 9830</institution-id><institution-id institution-id-type="GRID">grid.17091.3e</institution-id><institution>Department of Statistics, </institution><institution>University of British Columbia, </institution></institution-wrap>Vancouver, Canada </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0702 3000</institution-id><institution-id institution-id-type="GRID">grid.248762.d</institution-id><institution>Department of Molecular Oncology, </institution><institution>BC Cancer Agency, </institution></institution-wrap>Vancouver, Canada </aff></contrib-group><pub-date pub-type="epub"><day>7</day><month>2</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>7</day><month>2</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>2</month><year>2019</year></pub-date><volume>11</volume><issue>1</issue><fpage>95</fpage><lpage>102</lpage><history><date date-type="received"><day>7</day><month>1</month><year>2019</year></date><date date-type="accepted"><day>8</day><month>1</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2019</copyright-statement><license license-type="OpenAccess"><license-p><bold>OpenAccess</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Bayesian statistical learning provides a coherent probabilistic framework for modelling uncertainty in systems. This review describes the theoretical foundations underlying Bayesian statistics and outlines the computational frameworks for implementing Bayesian inference in practice. We then describe the use of Bayesian learning in single-cell biology for the analysis of high-dimensional, large data sets.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Bayesian</kwd><kwd>Computational biology</kwd><kwd>Statistical modelling</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/P02646X/1</award-id><principal-award-recipient><name><surname>Yau</surname><given-names>Christopher</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>University of Birmingham</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; International Union for Pure and Applied Biophysics (IUPAB) and Springer-Verlag GmbH Germany, part of Springer Nature 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Statistics provides a theoretical foundation for rigorous and cohe-rent data analysis by providing a mathematical framework in which to unify models of how data are produced by systems or experiment with techniques to handle uncertainty associated with these processes (Friedman et al. <xref ref-type="bibr" rid="CR15">2001</xref>). Whilst there is no single universal statistical approach, one philosophy that has gathered strength in the last 30 years is Bayesian statistical inference (Lindley <xref ref-type="bibr" rid="CR28">1972</xref>; Robert <xref ref-type="bibr" rid="CR41">2007</xref>; Bernardo and Smith <xref ref-type="bibr" rid="CR3">2009</xref>; Gelman et al. <xref ref-type="bibr" rid="CR16">2013</xref>). Bayesian statistics offers certain capabilities that enable it to be amenable to a variety of complex statistical applications and constraints, notably in machine learning, where other statistical frameworks would find difficulty. As a consequence, Bayesian approaches are now widely used in a variety of scientific and technological applications including biological research.</p><p id="Par3">In this review, we will examine the fundamental concepts that underpin Bayesian Statistics and consider a concise but otherwise precise overview of the mechanics of applying Bayesian methodology. We will then consider applications of Bayesian techniques in the field of single-cell biology in which technological advances have enabled the high-throughput collection of massive quantities of data that have given us an unprecedented insight into cell function.</p></sec><sec id="Sec2"><title>Fundamentals of Bayesian modelling</title><p id="Par4">Bayesian modelling requires three components (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a). The first is <italic>data</italic> (<italic>D</italic>) corresponding to measurements that are taken from the system of interest. Data can range from simple scalar values or, in big data applications, potentially complex structured tuples of multidimensional tensors (Rukat et al. <xref ref-type="bibr" rid="CR45">2017</xref>, <xref ref-type="bibr" rid="CR44">2018</xref>). The second component is a <italic>generative model</italic> (<italic>M</italic>) which describes a stochastic process by which the observed data arises. The generative model can be mechanistically inspired and based upon real-world physical laws and measurement processes, or may be given by generic statistical models that attempt to describe the dependencies between observed data sources and possibly unseen (latent) factors. Finally, an object of inference (<italic>&#x1d703;</italic>) that we wish to learn about is required. This could be a set of unknown parameters that govern the properties of the generative model which need to be estimated or predictions of future data under alternate conditions.
<fig id="Fig1"><label>Fig. 1</label><caption><p><bold>a</bold> Overview of Bayesian modelling. Data is assumed to be generated by a stochastic model which describes various underlying processes and is specified by some unknown parameters. Bayesian inference seeks to recover those parameters from the observed data. <bold>b</bold> Prior beliefs are expressed as a probability distribution over parameters <italic>&#x1d703;</italic> = (<italic>&#x1d703;</italic><sub>1</sub>,<italic>&#x1d703;</italic><sub>2</sub>) which are updated when data is collected via the likelihood function to give a posterior distribution over <italic>&#x1d703;</italic>. <bold>c</bold> Real-world posterior distributions often contain a number of separated high probability regions. An ideal Metropolis-Hastings algorithm would possess a proposal mechanism that allows regular movement between different high-probability regions without the need to tranverse through low-probability intermediate regions. <bold>d</bold> Variational methods build approximations of the true posterior distribution. In this example, a mean-field approximation breaks the dependencies between the parameters (<italic>&#x1d703;</italic><sub>1</sub>,<italic>&#x1d703;</italic><sub>2</sub>) so the variational posterior models each dimension separately</p></caption><graphic xlink:href="12551_2019_499_Fig1_HTML" id="MO1"/></fig></p><p id="Par5">We can define the <italic>posterior probability</italic> of the object of inference given the observed data in terms of the <italic>likelihood</italic> and <italic>prior probabilities</italic> and the <italic>evidence</italic> via Bayes&#x02019; theorem:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \underbrace{p(\theta | D, M )}_{\text{Posterior}} = \frac{ \overbrace{p(D | \theta, M )}^{\text{Likelihood}} \times \overbrace{p(\theta )}^{\text{Prior}}} { \underbrace{p(D|M)}_{\text{Marginal Likelihood}}}  $$\end{document}</tex-math><mml:math id="M2"><mml:munder><mml:mrow><mml:munder accentunder="false"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x1d703;</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x023df;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Posterior</mml:mtext></mml:mrow></mml:munder><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo><mml:mi>&#x1d703;</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x023de;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>Likelihood</mml:mtext></mml:mrow></mml:mover><mml:mo>&#x000d7;</mml:mo><mml:mover><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>&#x1d703;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x023de;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>Prior</mml:mtext></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:munder accentunder="false"><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>|</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x023df;</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>Marginal Likelihood</mml:mtext></mml:mrow></mml:munder></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12551_2019_499_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par6">What this says is that, given a generative model <italic>M</italic> and data <italic>D</italic>, the posterior probability distribution over the object of inference <italic>&#x1d703;</italic> is given by our prior belief that <italic>&#x1d703;</italic> takes on a certain value, scaled by the likelihood that the generative model under those beliefs would give rise to the data observed (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b). The denominator corresponds to a normalising term to ensure that the probability distributions are valid but also describes the <italic>marginal likelihood</italic> of the data under the assumed model. The latter quantity is useful if alternate generative models are available and one can use the marginal likelihood as a means of determining which generative model is likely to be the most consistent with nature. Ratios of marginal likelihoods for different models, say <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub>, <italic>P</italic>(<italic>D</italic>|<italic>M</italic><sub>2</sub>)/<italic>P</italic>(<italic>D</italic>|<italic>M</italic><sub>1</sub>), are known as <italic>Bayes Factors</italic>.</p><p id="Par7">Bayesian statistics can be seen as a coherent system for probability-based belief updating. We begin with some prior knowledge about <italic>&#x1d703;</italic>, we collect data and then we combine the data with our prior beliefs to give our posterior beliefs&#x02014;what we believe about <italic>&#x1d703;</italic><italic>after</italic> seeing data. Importantly, since <italic>&#x1d703;</italic> is an unobserved quantity, Bayesian inference describes our lack of certainty in its value via a probability distribution. If we take an interval of possible values for <italic>&#x1d703;</italic> (a <italic>posterior credible interval</italic>), we can compute the amount of probability mass contained within that interval from the posterior distribution and obtain the probability that the true parameters lie in that region. This interpretation is often considered more natural than the coverage (confidence) intervals used in frequentist-based statistics.</p></sec><sec id="Sec3"><title>Bayesian computation</title><p id="Par8">The implementation of Bayesian computation centres on the calculation of the marginal likelihood, <italic>p</italic>(<italic>D</italic>|<italic>M</italic>). This quantity is required to evaluate the posterior probability <italic>p</italic>(<italic>&#x1d703;</italic>|<italic>D</italic>,<italic>M</italic>) and requires a multidimensional integral over all parameters associated with the statistical model. Direct computation is typically intractable, due to the <italic>curse of dimensionality</italic> for any problem of even moderate dimensionality, which results in a combinatorial explosion in the number of configurations that must be summed/integrated over. The challenges are analogous to the computation of the <italic>partition function</italic> in statistic mechanics and Bayesian statisticians have utilised techniques inspired by statistical mechanics to overcome this obstacle in Bayesian computation.</p><sec id="Sec4"><title>Monte Carlo methods</title><p id="Par9"><italic>Markov Chain Monte Carlo</italic> (MCMC) simulations (Gilks et al. <xref ref-type="bibr" rid="CR17">1995</xref>; Brooks et al. <xref ref-type="bibr" rid="CR6">2011</xref>) generate sequences of random numbers such that their long-term statistical properties converge towards the target posterior distribution of interest. The predominant MCMC implementation derives from the Metropolis algorithm formulation in the 1953 paper by Metropolis et al. (<xref ref-type="bibr" rid="CR32">1953</xref>, whose work was motivated by statistical mechanics applications involving sampling low-energy configurations of complex molecular systems). The technique was later extended in generality by Hastings (<xref ref-type="bibr" rid="CR20">1970</xref>) to give the <italic>Metropolis-Hastings</italic> (M-H) algorithm. The key insight by Metropolis et al. (<xref ref-type="bibr" rid="CR32">1953</xref>) was to derive a sampling algorithm which did not require the evaluation of the partition function (marginal likelihood) but only point-wise evaluation of the Boltzmann factors. Given a current configuration of the system <italic>&#x1d703;</italic>, the Metropolis algorithm proceeds by proposing a new state <italic>&#x1d703;</italic><sup>&#x02032;</sup> via any <italic>proposal distribution</italic> and then evaluate the Boltzmann factor exp(&#x02212;<italic>E</italic>(<italic>&#x1d703;</italic><sup>&#x02032;</sup>)/<italic>k</italic><italic>T</italic>) at the proposed new state. If the new state results in a lower energy configuration then move to that new state, if it results in a higher energy configuration then choose to move to the new state with a probability which is given by the ratio of the Boltzmann factors: <italic>&#x003b1;</italic> = exp(&#x02212;(<italic>E</italic>(<italic>&#x1d703;</italic><sup>&#x02032;</sup>) &#x02212; <italic>E</italic>(<italic>&#x1d703;</italic>))/<italic>k</italic><italic>T</italic>). By treating the negative logarithm of the unnormalised posterior probability distribution as an energy function, <italic>E</italic>(<italic>&#x1d703;</italic>) = &#x02212;log <italic>p</italic>(<italic>&#x1d703;</italic>|<italic>D</italic>), the Metropolis algorithm (and its derivatives) has been co-opted by Bayesian statisticians as a means of efficiently performing from complex posterior distributions.</p><p id="Par10">MCMC algorithms provide theoretical guarantees that the stationary distribution of the random number sequences will asymptotically converge to the posterior distribution of interest. The determination of when convergence occurs and designing efficient proposal schemes to enable that convergence to be achieved in the shortest time is highly challenging and remains an area of ongoing research. The critical design choice in the M-H algorithm is the proposal mechanism. If the proposed states are randomly chosen, they are less likely to yield high-probability configurations and will be rejected. If the new states are too similar to the current state, then their probabilities will be similar but the configurations will not be fundamentally different leading to poor exploration of the overall probability space (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>c). The proposal mechanism must therefore balance the need to search the configuration space globally whilst maintaining a sufficient locality to provide a useful acceptance rate.</p><p id="Par11">A variety of modern MCMC variants now exist (Girolami and Calderhead <xref ref-type="bibr" rid="CR18">2011</xref>; Chen et al. <xref ref-type="bibr" rid="CR12">2014</xref>; Hoffman and Gelman <xref ref-type="bibr" rid="CR22">2014</xref>; Shahbaba et al. <xref ref-type="bibr" rid="CR48">2014</xref>). For instance, originally conceived by Duane et al. (Duane et al. <xref ref-type="bibr" rid="CR14">1987</xref>) for lattice field theory simulations of quantum chromodynamics, Bayesians have generalised <italic>Hamiltonian Monte Carlo</italic> (HMC) methods (Neal et al. <xref ref-type="bibr" rid="CR33">2011</xref>) which exploit geometric information to greatly increase the sampling efficiency of MCMC algorithms. Whilst standard M-H algorithms can be described as a <italic>propose-and-check</italic> approach, HMC biases proposals along trajectories that are likely to lead to high-probability configurations. Probabilistic programming languages such as Stan (Carpenter et al. <xref ref-type="bibr" rid="CR11">2016</xref>) and PyMC3 (Salvatier et al. <xref ref-type="bibr" rid="CR46">2016</xref>) contain prebuilt implementations of HMC and variants freeing modellers from many of the detailed requirements of building HMC algorithms.</p></sec><sec id="Sec5"><title>Variational methods</title><p id="Par12">The computational requirements of MCMC methods can be prohibitive in applications that involve large, high-dimensional data sets or complex models. As the dimensionality of <italic>&#x1d703;</italic> increases, the convergence complexity of MCMC algorithms also increases when sampling from high-dimensional posteriors (Mengersen et al. <xref ref-type="bibr" rid="CR31">1999</xref>; Rajaratnam and Sparks <xref ref-type="bibr" rid="CR38">2015</xref>). An alternative is to abandon the theoretical guarantees of MCMC methods and to construct analytically tractable approximations <italic>q</italic><sub><italic>&#x003bd;</italic></sub>(<italic>&#x1d703;</italic>|<italic>D</italic>) to the true posterior distribution <italic>p</italic>(<italic>&#x1d703;</italic>|<italic>D</italic>)&#x02014;this is the motivation underlying <italic>Variational Bayesian</italic> methods (Blei et al. <xref ref-type="bibr" rid="CR4">2017</xref>).</p><p id="Par13">In the construction of variational approximations, it is typical to assume that the approximating distribution has a simplified structure (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>d). The frequently used <italic>mean-field</italic> approximation assumes a fully factorisable form of the approximate posterior, <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$q_{\nu }(\theta |D) = {\prod }_{t = 1}^{T} q_{\nu }^{(t)}(\theta _{t}|D)$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003bd;</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>&#x1d703;</mml:mi><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>&#x0220f;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003bd;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>&#x1d703;</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12551_2019_499_Article_IEq1.gif"/></alternatives></inline-formula> where the dependencies between the different elements of <italic>&#x1d703;</italic> are uncoupled and each factor <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$q_{\nu }^{(t)}$\end{document}</tex-math><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003bd;</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12551_2019_499_Article_IEq2.gif"/></alternatives></inline-formula> is typically given by a simple distribution (e.g. Gaussian, Gamma). If the approximating distribution <italic>q</italic><sub><italic>&#x003bd;</italic></sub> is parameterised by <italic>&#x003bd;</italic>, the variational approach seeks to optimise these <italic>variational parameters</italic> to minimise the difference&#x02014;measured using the Kullback-Leibler (KL) divergence&#x02014;between the true and approximate posterior distributions. Therefore, unlike Monte Carlo methods which use stochastic sampling, variational methods transform the inference problem into an optimisation task. The latter means that assessing the convergence of variational methods is relatively straightforward and typically requires significantly less time for complex models than MCMC approaches.</p><p id="Par14">Classic variational algorithms used analytically derived optimisation steps (coordinate ascent VI) but, more recently, stochastic variational inference (SVI) methods employ stochastic gradient descent algorithms instead (Hoffman et al. <xref ref-type="bibr" rid="CR21">2013</xref>; Titsias and L&#x000e1;zaro-Gredilla <xref ref-type="bibr" rid="CR51">2014</xref>). SVI uses cheap to compute, &#x0201c;noisy&#x0201d; estimates of natural gradients based on a subset of data points instead of the true gradients which require a pass through all data points. This exploits the fact that the expected value of these noisy gradients is equal to the true gradient and so convergence of the SVI algorithm can be guaranteed under certain conditions. As a consequence, SVI allows the application of variational methods to a wider class of models and by operating on <italic>mini-batches</italic> of data in each optimisation step provides substantial speed-ups in large data settings.</p><p id="Par15"><italic>Amortised variational inference</italic> uses <italic>inference networks</italic> within variational inference algorithms for latent variable models&#x02014;where each data item is associated with its own set of parameters (Zhang et al. <xref ref-type="bibr" rid="CR57">2018</xref>). In such situations, a typical variational approximation would result in each data item also being associated with its own compliment of variational parameters; thus, with larger data sets, there would be an increase in the number of variational parameters to optimise. Inference networks replace these <italic>local</italic> variational parameters with a single set of global parameters associated with a neural network. The goal is to use variational inference to optimise the parameters associated with this neural network and to use the optimised network to <italic>predict</italic> the local variational parameters. Inference networks therefore offer another layer of approximation that breaks the dependency between the computational requirements of the variational inference algorithm and the size of the data set.</p><p id="Par16">Whilst the accuracy of variational approximations is often impossible to quantify (Yao et al. <xref ref-type="bibr" rid="CR54">2018</xref>), they provide the current mainstay inferential approach for high-dimensional Bayesian modelling. Current machine learning development libraries, such as TensorFlow (Abadi et al. <xref ref-type="bibr" rid="CR1">2016</xref>) and PyTorch (Paszke et al. <xref ref-type="bibr" rid="CR35">2017</xref>), provide a substantial body of tools for the construction of neural networks, and optimisation algorithms for the implementation of variational inference algorithms.</p></sec></sec><sec id="Sec6"><title>Bayesian applications in single-cell biology</title><p id="Par17">The recent availability of a plethora of relatively low-cost experimental methods and protocols for high-throughput screening of individual cells has lead to an explosion in single-cell biological data (Theillet <xref ref-type="bibr" rid="CR50">1998</xref>). A single-cell experiment can generate data that is both high-dimensional and large in sample size with recent studies involving single-cell RNA sequencing routinely able to produce cell numbers on the order of 10<sup>5</sup> cells measuring 10<sup>3</sup> &#x02212;&#x02009;10<sup>4</sup> genes. International endeavours, such as The Human Cell Atlas (HCA) project (Regev et al. <xref ref-type="bibr" rid="CR39">2017</xref>), will seek to catalogue and sequence all known human cell types in the coming years.</p><p id="Par18">The benefit of single-cell measurements is to remove the averaging effect when measurements are taken on populations of cells which can obscure important stochastic dynamics operating in individual cells. However, the challenge when working with single cells is the inherent sensitivity of cells to physical manipulation and the difficulties of robustly measuring minuscule quantities of potentially unstable molecules, e.g. RNA. Consequently, single-cell data from any technical platform is inherently noisy, contains various levels of missingness and may harbour many sources of bias&#x02014;all of which could have both a biological or technical origin (Stegle et al. <xref ref-type="bibr" rid="CR49">2015</xref>; Poirion et al. <xref ref-type="bibr" rid="CR37">2016</xref>). Probabilistic modelling of single-cell data, based on a Bayesian framework, provides a coherent strategy for encapsulating these complexities.</p><sec id="Sec7"><title>Differential expression</title><p id="Par19"><italic>Differential expression</italic> (DE) aims to identify genes that are up- or downregulated between cell types (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>a). Whilst standard frequentist-based hypothesis testing procedures can be employed, Bayesian DE alternatives offer certain benefits. Kharcenko et al. (<xref ref-type="bibr" rid="CR25">2014</xref>) introduced a generative model that includes drop-outs for differential expression analysis. Dropouts are frequent occurrences in single-cell expression data due to the low quantities of mRNA involved which means the presence of some transcripts cannot always be reliably detected by sequencing&#x02014;the result is a zero expression measurement for cells that might actually be expressing a gene at a low level. BASiCS (Bayesian Analysis of Single-Cell Sequencing data (Vallejos et al. <xref ref-type="bibr" rid="CR52">2015</xref>; Vallejos et al. <xref ref-type="bibr" rid="CR53">2016</xref>)) jointly models highly variable genes and differential expression between cell populations which allows it to detect <italic>differential variability</italic>&#x02014;an effect often masked by both standard differential expression methods. A related analysis is identifying differential splicing in which exon usage varies between cells or cell populations. Differential splicing can be difficult to detect in single-cell RNA-seq data due to amplification biases, shallow read depths, and other technical artefacts. To solve this, the Bayesian method BRIE (Bayesian regression for isoform estimation (Huang and Sanguinetti <xref ref-type="bibr" rid="CR23">2017</xref>)) leverages sequence-derived features as an informative prior distribution in a hierarchical model to greatly increase the accuracy of inference.
<fig id="Fig2"><label>Fig. 2</label><caption><p><bold>a</bold> Single-cell differential expression analysis aims to identify differences in expression level and variability between cell types. Confounding effects such as dropout and batch effects must be accounted for in order to avoid false conclusions. <bold>b</bold> Variational autoencoders use deep neural networks to <italic>encode</italic> input expression data vectors into low-dimensional latent representations whilst simultaneously learning <italic>decoders</italic> that can generate realistic expression data from these latent representations. <bold>c</bold> Pseudotemporal model aims to identify latent uni-dimensional representations that correspond to physical time variation from high-dimensional cross-sectional single-cell data. <bold>d</bold> Probabilistic approaches to tumour phylogeny inference are essential in the presence of sequencing noise since genotyping errors can lead to uncertainties in phylogenetic reconstruction. Here, the presence of allelic dropout leading to genotyping error in a single-cell type could lead to alternate phylogenetic histories and different interpretations of the importance of acquired mutations</p></caption><graphic xlink:href="12551_2019_499_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec8"><title>Deep learning representations</title><p id="Par20">One particular analytical problem is the identification of latent structure within these high-dimensional data sets in order to understand the underlying fundamental biological processes. Ideas inspired from <italic>deep learning</italic> (LeCun et al. <xref ref-type="bibr" rid="CR27">2015</xref>) have recently emerged in single-cell biology as a means of extracting low-dimensional representations from high-dimensional data (Ding et al. <xref ref-type="bibr" rid="CR13">2018</xref>; Lopez et al. <xref ref-type="bibr" rid="CR29">2018</xref>). For instance, scVI (Lopez et al. <xref ref-type="bibr" rid="CR29">2018</xref>) uses a hierarchical Bayesian model&#x02014;a variational autoencoder (Kingma and Welling <xref ref-type="bibr" rid="CR26">2013</xref>), incorporating deep neural networks and stochastic variational inference stochastic optimisation to aggregate information across similar cells and genes whilst simultaneously adjusting for batch effects and lack of measurement sensitivity (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>b). The benefit of the deep neural networks is that the functional relationship between the measured gene expression and the latent representations does not need to be prespecified by the modeller and scVI is able to exploit the vast array of data available to <italic>learn</italic> these relationships from the data itself. Implementations using modern machine learning development frameworks allow a vast array of high-performance computational machinery (such as graphics processing units) to be exploited permitting methods such as scVI to make short work of data sets involving millions of cells.</p></sec><sec id="Sec9"><title>Temporal modelling</title><p id="Par21">High-throughput single-cell molecular technologies provide an instantaneous measurement of the molecular state of individual cells. Genuine time series measurements of individual cells undergoing dynamic processes, such as differentiation or cell cycle, are difficult due to the inherently destructive nature of the measurement process and asynchronicity of cellular progression. To circumvent this, analytical methods have been developed that use a cross-sectional &#x0201c;snapshot&#x0201d; of cells&#x02019; gene expression to assign a <italic>pseudotime</italic> to each cell&#x02014;a surrogate measure of progression through the process of interest (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>c). Downstream analyses such as differential expression (Campbell and Yau <xref ref-type="bibr" rid="CR9">2017b</xref>; Sander et al. <xref ref-type="bibr" rid="CR47">2017</xref>) can then be performed using the pseudo times in lieu of physical time information.</p><p id="Par22">A majority of Bayesian pseudotime inference methods build upon the Gaussian Process Latent Variable (GPLVM) framework. The first model for single-cell RNA-seq was DeLorean (Reid and Wernisch <xref ref-type="bibr" rid="CR40">2016</xref>) that uses a Matern<sub>3/2</sub> kernel with a Gaussian likelihood on suitably log-transformed data. DeLorean uses the probabilistic programming language Stan (Carpenter et al. <xref ref-type="bibr" rid="CR11">2016</xref>) for inference that performs an adaptive version of Hamiltonian Monte Carlo. This was recently reimplemented in the method GrandPrix (Ahmed et al. <xref ref-type="bibr" rid="CR2">2019</xref>) with fast inducing point variational inference implemented in the GPFlow framework (Matthews et al. <xref ref-type="bibr" rid="CR30">2017</xref>) to achieve order-of-magnitude faster inference. A related model is the PseudoGP framework (Campbell and Yau <xref ref-type="bibr" rid="CR7">2016</xref>) that uses the posterior distributions from probabilistic pseudotime to quantify the uncertainty in downstream analyses such as differential expression. Branching differentiation processes can also be modelled using Gaussian processes (Boukouvalas et al. <xref ref-type="bibr" rid="CR5">2018</xref>; Penfold et al. <xref ref-type="bibr" rid="CR36">2018</xref>).</p><p id="Par23">Further, Bayesian pseudotime methods have been developed based on dimensionality reduction techniques other than GPLVM. A popular class of these are factor analysis models that seek a probabilistic mapping from the latent space (pseudotimes) through a linear or parametric nonlinear function. Such an approach was successfully applied in the Ouija framework (Campbell and Yau <xref ref-type="bibr" rid="CR10">2018</xref>) that uses a sigmoidal mapping to learn pseudotimes from small marker gene panels along with interpretable parameters corresponding to activation times of genes. A related model is MFA (Campbell and Yau <xref ref-type="bibr" rid="CR8">2017a</xref>) that implements a mixture of linear factor analysers to infer bifurcations from single-cell gene expression data, using MCMC sampling for inference. Finally, a Bayesian variant of unidimensional scaling (BUDS, Nguyen and Holmes <xref ref-type="bibr" rid="CR34">2017</xref>) has been proposed for ordering single cell with an emphasis on visualising uncertainty.</p></sec><sec id="Sec10"><title>Tumour evolution</title><p id="Par24">Bayesian approaches have also been developed for single-cell&#x02013;based modelling of cancer evolution (Zafar et al. <xref ref-type="bibr" rid="CR56">2018</xref>; Goh et al. <xref ref-type="bibr" rid="CR19">2019</xref>). Here, the data corresponds to genome sequences of tumour samples and the unobserved object of inference is the evolutionary tree relating the different cancer cell populations within the tumour (Yuan et al. <xref ref-type="bibr" rid="CR55">2015</xref>; Roth et al. <xref ref-type="bibr" rid="CR43">2016</xref>) or a mutation tree representing the partial (temporal) order of the mutation events (Jahn et al. <xref ref-type="bibr" rid="CR24">2016</xref>; Ross and Markowetz <xref ref-type="bibr" rid="CR42">2016</xref>). Since an arbitrary number of evolutionary mechanisms may be possible, the information included in the priors help to <italic>regularise</italic> the inferential problem to make it tractable by limiting the space of possible evolutionary trajectories. Uncertainty propagation is also of specific help in this problem. Allelic dropout in single-cell sequencing can cause mutations to become undetected and lead to errors in the genotyping of individual cells. Errors in cellular mutation profiles could fundamentally alter the inferred evolutionary trees hence joint modelling of sequencing errors and evolutionary trajectories is critical (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>d).</p></sec></sec><sec id="Sec11"><title>Discussion</title><p id="Par25">Bayesian methodology is a conceptually natural approach to apply to biological research applications. Modern probabilistic programming language environments for Bayesian computation have further facilitated its application by providing interfaces for specifying potentially highly complex models even for non-experts. This review has described the underlying theoretical framework as well as the computational techniques required to implement Bayesian modelling with a focus on applications in single-cell biology. Nonetheless, further research into improved and faster Bayesian computation techniques for big data biology is required. Despite its strong theoretical foundations, Bayesian approaches are still relatively underused in biological sciences. Bayesian modelling requires considerable thought to be given to the constitution of the generative models and the specification of prior beliefs. Probabilistic programming languages have simplified model development by allowing users to focus on model specification rather than the computational implementations but there remains a considerable &#x0201c;art&#x0201d; to designing good models and expertise is gained through experience. Research to develop more <italic>automatic</italic> tools for Bayesian model specification would be beneficial. Posterior uncertainty characterisation intrinsically means that there is no &#x0201c;right answer&#x0201d; in Bayesian modelling&#x02014;only a distribution over possibilities. Probabilistic outcomes can be difficult to interpret even for seasoned experts and non-experts may find such summaries challenging to palate. Finally, in high-dimensional, large data settings, recent computational advances have made Bayesian inference feasible for increasingly larger problems but often remains more computationally taxing than alternative approaches that might forego uncertainty characterisation for point estimation. However, as described in many of the single-cell applications, without formal uncertainty modelling, erroneous inferences can be made in the presence of confounding factors or noisy/missing data.</p></sec></body><back><fn-group><fn><p>This article is part of a Special Issue on &#x02018;Big Data&#x02019; edited by Joshua WK Ho and Eleni Giannoulatou.</p></fn><fn><p><bold>Publisher&#x02019;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>CY is supported by a UK Medical Research Council Research Grant (Ref: MR/P02646X/1) and by The Alan Turing Institute under the EPSRC grant EP/N510129/1.</p></ack><notes notes-type="COI-statement"><sec id="FPar1"><title><bold>Conflict of interests</bold></title><p>Christopher Yau declares that he has no conflict of interest. Kieran Campbell declares that he has no conflict of interest.</p></sec><sec id="FPar2"><title><bold>Ethical approval</bold></title><p>This article does not contain any studies with human participants or animals performed by any of the authors.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name><name><surname>Devin</surname><given-names>M</given-names></name><name><surname>Ghemawat</surname><given-names>S</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Isard</surname><given-names>M</given-names></name><etal/></person-group><article-title>Tensorflow: a system for large-scale machine learning</article-title><source>OSDI</source><year>2016</year><volume>16</volume><fpage>265</fpage><lpage>283</lpage></element-citation></ref><ref id="CR2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmed</surname><given-names>S</given-names></name><name><surname>Rattray</surname><given-names>M</given-names></name><name><surname>Boukouvalas</surname><given-names>A</given-names></name></person-group><article-title>Grandprix: scaling up the bayesian gplvm for single-cell data</article-title><source>Bioinformatics (Oxford England)</source><year>2019</year><volume>35</volume><fpage>47</fpage><lpage>54</lpage></element-citation></ref><ref id="CR3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bernardo</surname><given-names>JM</given-names></name><name><surname>Smith</surname><given-names>AF</given-names></name></person-group><source>Bayesian theory, vol 405</source><year>2009</year><publisher-loc>New York</publisher-loc><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="CR4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Kucukelbir</surname><given-names>A</given-names></name><name><surname>McAuliffe</surname><given-names>JD</given-names></name></person-group><article-title>Variational inference: A review for statisticians</article-title><source>J Am Stat Assoc</source><year>2017</year><volume>112</volume><issue>518</issue><fpage>859</fpage><lpage>877</lpage></element-citation></ref><ref id="CR5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boukouvalas</surname><given-names>A</given-names></name><name><surname>Hensman</surname><given-names>J</given-names></name><name><surname>Rattray</surname><given-names>M</given-names></name></person-group><article-title>Bgp: identifying gene-specific branching dynamics from single-cell data with a branching gaussian process</article-title><source>Genome Biol</source><year>2018</year><volume>19</volume><issue>1</issue><fpage>65</fpage><pub-id pub-id-type="pmid">29843817</pub-id></element-citation></ref><ref id="CR6"><mixed-citation publication-type="other">Brooks S, Gelman A, Jones G, Meng XL (2011) Handbook of Markov chain Monte Carlo. CRC Press, Boca Raton</mixed-citation></ref><ref id="CR7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>KR</given-names></name><name><surname>Yau</surname><given-names>C</given-names></name></person-group><article-title>Order under uncertainty: robust differential expression analysis using probabilistic models for pseudotime inference</article-title><source>PLoS Comput Biol</source><year>2016</year><volume>12</volume><issue>11</issue><fpage>e1005,212</fpage></element-citation></ref><ref id="CR8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>KR</given-names></name><name><surname>Yau</surname><given-names>C</given-names></name></person-group><article-title>Probabilistic modeling of bifurcations in single-cell gene expression data using a bayesian mixture of factor analyzers</article-title><source>Wellcome Open Res</source><year>2017</year><volume>2</volume><fpage>19</fpage><pub-id pub-id-type="pmid">28503665</pub-id></element-citation></ref><ref id="CR9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>KR</given-names></name><name><surname>Yau</surname><given-names>C</given-names></name></person-group><article-title>Switchde: inference of switch-like differential expression along single-cell trajectories</article-title><source>Bioinformatics</source><year>2017</year><volume>33</volume><issue>8</issue><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="pmid">28011787</pub-id></element-citation></ref><ref id="CR10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>KR</given-names></name><name><surname>Yau</surname><given-names>C</given-names></name></person-group><article-title>A descriptive marker gene approach to single-cell pseudotime inference</article-title><source>Bioinformatics</source><year>2018</year><volume>35</volume><issue>1</issue><fpage>28</fpage><lpage>35</lpage></element-citation></ref><ref id="CR11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>B</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hoffman</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Goodrich</surname><given-names>B</given-names></name><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Brubaker</surname><given-names>MA</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Riddell</surname><given-names>A</given-names></name></person-group><article-title>Stan: a probabilistic programming language</article-title><source>J Stat Softw</source><year>2016</year><volume>20</volume><fpage>1</fpage><lpage>37</lpage></element-citation></ref><ref id="CR12"><mixed-citation publication-type="other">Chen T, Fox E, Guestrin C (2014) Stochastic gradient Hamiltonian Monte Carlo. In: International conference on machine learning, pp 1683&#x02013;1691</mixed-citation></ref><ref id="CR13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>J</given-names></name><name><surname>Condon</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>SP</given-names></name></person-group><article-title>Interpretable dimensionality reduction of single cell transcriptome data with deep generative models</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><issue>1</issue><fpage>2002</fpage><pub-id pub-id-type="pmid">29784946</pub-id></element-citation></ref><ref id="CR14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duane</surname><given-names>S</given-names></name><name><surname>Kennedy</surname><given-names>AD</given-names></name><name><surname>Pendleton</surname><given-names>BJ</given-names></name><name><surname>Roweth</surname><given-names>D</given-names></name></person-group><article-title>Hybrid monte carlo</article-title><source>Phys Lett B</source><year>1987</year><volume>195</volume><issue>2</issue><fpage>216</fpage><lpage>222</lpage></element-citation></ref><ref id="CR15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><source>The elements of statistical learning, vol 1</source><year>2001</year><publisher-loc>New York</publisher-loc><publisher-name>Springer Series in Statistics</publisher-name></element-citation></ref><ref id="CR16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Stern</surname><given-names>HS</given-names></name><name><surname>Carlin</surname><given-names>JB</given-names></name><name><surname>Dunson</surname><given-names>DB</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><source>Bayesian data analysis</source><year>2013</year><publisher-loc>Boca Raton</publisher-loc><publisher-name>Chapman and Hall/CRC</publisher-name></element-citation></ref><ref id="CR17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gilks</surname><given-names>WR</given-names></name><name><surname>Richardson</surname><given-names>S</given-names></name><name><surname>Spiegelhalter</surname><given-names>D</given-names></name></person-group><source>Markov chain Monte Carlo in practice</source><year>1995</year><publisher-loc>Boca Raton</publisher-loc><publisher-name>Chapman and Hall/CRC</publisher-name></element-citation></ref><ref id="CR18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girolami</surname><given-names>M</given-names></name><name><surname>Calderhead</surname><given-names>B</given-names></name></person-group><article-title>Riemann manifold langevin and Hamiltonian Monte Carlo methods</article-title><source>J R Stat Soc Ser B Stat Methodol</source><year>2011</year><volume>73</volume><issue>2</issue><fpage>123</fpage><lpage>214</lpage></element-citation></ref><ref id="CR19"><mixed-citation publication-type="other">Goh G, McGranahan N, Wilson G A (2019) Computational methods for analysis of tumor clonality and evolutionary history. In: Cancer bioinformatics. Springer, pp 217&#x02013;226</mixed-citation></ref><ref id="CR20"><mixed-citation publication-type="other">Hastings WK (1970) Monte Carlo sampling methods using Markov chains and their applications</mixed-citation></ref><ref id="CR21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Paisley</surname><given-names>J</given-names></name></person-group><article-title>Stochastic variational inference</article-title><source>J Mach Learn Res</source><year>2013</year><volume>14</volume><issue>1</issue><fpage>1303</fpage><lpage>1347</lpage></element-citation></ref><ref id="CR22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><article-title>The no-u-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo</article-title><source>J Mach Learn Res</source><year>2014</year><volume>15</volume><issue>1</issue><fpage>1593</fpage><lpage>1623</lpage></element-citation></ref><ref id="CR23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Sanguinetti</surname><given-names>G</given-names></name></person-group><article-title>Brie: transcriptome-wide splicing quantification in single cells</article-title><source>Genome Biol</source><year>2017</year><volume>18</volume><issue>1</issue><fpage>123</fpage><pub-id pub-id-type="pmid">28655331</pub-id></element-citation></ref><ref id="CR24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jahn</surname><given-names>K</given-names></name><name><surname>Kuipers</surname><given-names>J</given-names></name><name><surname>Beerenwinkel</surname><given-names>N</given-names></name></person-group><article-title>Tree inference for single-cell data</article-title><source>Genome Biol</source><year>2016</year><volume>17</volume><issue>1</issue><fpage>86</fpage><pub-id pub-id-type="pmid">27149953</pub-id></element-citation></ref><ref id="CR25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kharchenko</surname><given-names>PV</given-names></name><name><surname>Silberstein</surname><given-names>L</given-names></name><name><surname>Scadden</surname><given-names>DT</given-names></name></person-group><article-title>Bayesian approach to single-cell differential expression analysis</article-title><source>Nat Methods</source><year>2014</year><volume>11</volume><issue>7</issue><fpage>740</fpage><lpage>742</lpage><pub-id pub-id-type="pmid">24836921</pub-id></element-citation></ref><ref id="CR26"><mixed-citation publication-type="other">Kingma DP, Welling M (2013) Auto-encoding variational bayes. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1312.6114">1312.6114</ext-link></mixed-citation></ref><ref id="CR27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><issue>7553</issue><fpage>436</fpage><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="CR28"><mixed-citation publication-type="other">Lindley DV (1972) Bayesian statistics, a review, vol 2. SIAM</mixed-citation></ref><ref id="CR29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez</surname><given-names>R</given-names></name><name><surname>Regier</surname><given-names>J</given-names></name><name><surname>Cole</surname><given-names>MB</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Yosef</surname><given-names>N</given-names></name></person-group><article-title>Deep generative modeling for single-cell transcriptomics</article-title><source>Nat Methods</source><year>2018</year><volume>15</volume><issue>12</issue><fpage>1053</fpage><lpage>1058</lpage><pub-id pub-id-type="pmid">30504886</pub-id></element-citation></ref><ref id="CR30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthews</surname><given-names>DG</given-names></name><name><surname>Alexander</surname><given-names>G</given-names></name><name><surname>Van Der Wilk</surname><given-names>M</given-names></name><name><surname>Nickson</surname><given-names>T</given-names></name><name><surname>Fujii</surname><given-names>K</given-names></name><name><surname>Boukouvalas</surname><given-names>A</given-names></name><name><surname>Le&#x000f3;n-Villagr&#x000e1;</surname><given-names>P</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Hensman</surname><given-names>J</given-names></name></person-group><article-title>Gpflow: a Gaussian process library using tensorflow</article-title><source>J Mach Learn Res</source><year>2017</year><volume>18</volume><issue>1</issue><fpage>1299</fpage><lpage>1304</lpage></element-citation></ref><ref id="CR31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mengersen</surname><given-names>KL</given-names></name><name><surname>Robert</surname><given-names>CP</given-names></name><name><surname>Guihenneuc-Jouyaux</surname><given-names>C</given-names></name></person-group><article-title>Mcmc convergence diagnostics: a reviewww</article-title><source>Bayesian Stat</source><year>1999</year><volume>6</volume><fpage>415</fpage><lpage>440</lpage></element-citation></ref><ref id="CR32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metropolis</surname><given-names>N</given-names></name><name><surname>Rosenbluth</surname><given-names>AW</given-names></name><name><surname>Rosenbluth</surname><given-names>MN</given-names></name><name><surname>Teller</surname><given-names>AH</given-names></name><name><surname>Teller</surname><given-names>E</given-names></name></person-group><article-title>Equation of state calculations by fast computing machines</article-title><source>J Chem Phys</source><year>1953</year><volume>21</volume><issue>6</issue><fpage>1087</fpage><lpage>1092</lpage></element-citation></ref><ref id="CR33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>RM</given-names></name><etal/></person-group><article-title>Mcmc using Hamiltonian dynamics</article-title><source>Handbook of Markov Chain Monte Carlo</source><year>2011</year><volume>2</volume><issue>11</issue><fpage>2</fpage></element-citation></ref><ref id="CR34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>LH</given-names></name><name><surname>Holmes</surname><given-names>S</given-names></name></person-group><article-title>Bayesian unidimensional scaling for visualizing uncertainty in high dimensional datasets with latent ordering of observations</article-title><source>BMC Bioinf</source><year>2017</year><volume>18</volume><issue>10</issue><fpage>394</fpage></element-citation></ref><ref id="CR35"><mixed-citation publication-type="other">Paszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison A, Antiga L, Lerer A (2017) Automatic differentiation in pytorch</mixed-citation></ref><ref id="CR36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Penfold</surname><given-names>CA</given-names></name><name><surname>Sybirna</surname><given-names>A</given-names></name><name><surname>Reid</surname><given-names>JE</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Wernisch</surname><given-names>L</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Grant</surname><given-names>M</given-names></name><name><surname>Surani</surname><given-names>MA</given-names></name></person-group><article-title>Branch-recombinant gaussian processes for analysis of perturbations in biological time series</article-title><source>Bioinformatics</source><year>2018</year><volume>34</volume><issue>17</issue><fpage>i1005</fpage><lpage>i1013</lpage><pub-id pub-id-type="pmid">30423108</pub-id></element-citation></ref><ref id="CR37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirion</surname><given-names>OB</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name><name><surname>Ching</surname><given-names>T</given-names></name><name><surname>Garmire</surname><given-names>L</given-names></name></person-group><article-title>Single-cell transcriptomics bioinformatics and computational challenges</article-title><source>Front Genet</source><year>2016</year><volume>7</volume><fpage>163</fpage><pub-id pub-id-type="pmid">27708664</pub-id></element-citation></ref><ref id="CR38"><mixed-citation publication-type="other">Rajaratnam B, Sparks D (2015) Mcmc-based inference in the era of big data: a fundamental analysis of the convergence complexity of high-dimensional chains. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1508.00947">1508.00947</ext-link></mixed-citation></ref><ref id="CR39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Regev</surname><given-names>A</given-names></name><name><surname>Teichmann</surname><given-names>SA</given-names></name><name><surname>Lander</surname><given-names>ES</given-names></name><name><surname>Amit</surname><given-names>I</given-names></name><name><surname>Benoist</surname><given-names>C</given-names></name><name><surname>Birney</surname><given-names>E</given-names></name><name><surname>Bodenmiller</surname><given-names>B</given-names></name><name><surname>Campbell</surname><given-names>P</given-names></name><name><surname>Carninci</surname><given-names>P</given-names></name><name><surname>Clatworthy</surname><given-names>M</given-names></name><etal/></person-group><article-title>Science forum: the human cell atlas</article-title><source>Elife</source><year>2017</year><volume>6</volume><fpage>e27,041</fpage></element-citation></ref><ref id="CR40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reid</surname><given-names>JE</given-names></name><name><surname>Wernisch</surname><given-names>L</given-names></name></person-group><article-title>Pseudotime estimation: deconfounding single cell time series</article-title><source>Bioinformatics</source><year>2016</year><volume>32</volume><issue>19</issue><fpage>2973</fpage><lpage>2980</lpage><pub-id pub-id-type="pmid">27318198</pub-id></element-citation></ref><ref id="CR41"><mixed-citation publication-type="other">Robert C (2007) The Bayesian choice: from decision-theoretic foundations to computational implementation. Springer Science &#x00026; Business Media</mixed-citation></ref><ref id="CR42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ross</surname><given-names>EM</given-names></name><name><surname>Markowetz</surname><given-names>F</given-names></name></person-group><article-title>Onconem: inferring tumor evolution from single-cell sequencing data</article-title><source>Genome Biol</source><year>2016</year><volume>17</volume><issue>1</issue><fpage>69</fpage><pub-id pub-id-type="pmid">27083415</pub-id></element-citation></ref><ref id="CR43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>A</given-names></name><name><surname>McPherson</surname><given-names>A</given-names></name><name><surname>Laks</surname><given-names>E</given-names></name><name><surname>Biele</surname><given-names>J</given-names></name><name><surname>Yap</surname><given-names>D</given-names></name><name><surname>Wan</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>MA</given-names></name><name><surname>Nielsen</surname><given-names>CB</given-names></name><name><surname>McAlpine</surname><given-names>JN</given-names></name><name><surname>Aparicio</surname><given-names>S</given-names></name><etal/></person-group><article-title>Clonal genotype and population structure inference from single-cell tumor sequencing</article-title><source>Nat Methods</source><year>2016</year><volume>13</volume><issue>7</issue><fpage>573</fpage><lpage>576</lpage><pub-id pub-id-type="pmid">27183439</pub-id></element-citation></ref><ref id="CR44"><mixed-citation publication-type="other">Rukat T, Holmes C, Yau C (2018) Probabilistic boolean tensor decomposition. In: International conference on machine learning, pp 4410&#x02013;4419</mixed-citation></ref><ref id="CR45"><mixed-citation publication-type="other">Rukat T, Holmes CC, Titsias MK, Yau C (2017) Bayesian Boolean matrix factorisation. In: International conference on machine learning, pp 2969&#x02013;2978</mixed-citation></ref><ref id="CR46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salvatier</surname><given-names>J</given-names></name><name><surname>Wiecki</surname><given-names>TV</given-names></name><name><surname>Fonnesbeck</surname><given-names>C</given-names></name></person-group><article-title>Probabilistic programming in python using pymc3</article-title><source>PeerJ Comput Sci</source><year>2016</year><volume>2</volume><fpage>e55</fpage></element-citation></ref><ref id="CR47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sander</surname><given-names>J</given-names></name><name><surname>Schultze</surname><given-names>JL</given-names></name><name><surname>Yosef</surname><given-names>N</given-names></name></person-group><article-title>Impulsede: detection of differentially expressed genes in time series data using impulse models</article-title><source>Bioinformatics</source><year>2017</year><volume>33</volume><issue>5</issue><fpage>757</fpage><lpage>759</lpage><pub-id pub-id-type="pmid">27797772</pub-id></element-citation></ref><ref id="CR48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahbaba</surname><given-names>B</given-names></name><name><surname>Lan</surname><given-names>S</given-names></name><name><surname>Johnson</surname><given-names>WO</given-names></name><name><surname>Neal</surname><given-names>RM</given-names></name></person-group><article-title>Split Hamiltonian Monte Carlo</article-title><source>Stat Comput</source><year>2014</year><volume>24</volume><issue>3</issue><fpage>339</fpage><lpage>349</lpage></element-citation></ref><ref id="CR49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stegle</surname><given-names>O</given-names></name><name><surname>Teichmann</surname><given-names>SA</given-names></name><name><surname>Marioni</surname><given-names>JC</given-names></name></person-group><article-title>Computational and analytical challenges in single-cell transcriptomics</article-title><source>Nat Rev Genet</source><year>2015</year><volume>16</volume><issue>3</issue><fpage>133</fpage><pub-id pub-id-type="pmid">25628217</pub-id></element-citation></ref><ref id="CR50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theillet</surname><given-names>C</given-names></name></person-group><article-title>Full speed ahead for tumor screening</article-title><source>Nat Med</source><year>1998</year><volume>4</volume><issue>7</issue><fpage>767</fpage><lpage>768</lpage><pub-id pub-id-type="pmid">9662361</pub-id></element-citation></ref><ref id="CR51"><mixed-citation publication-type="other">Titsias M, L&#x000e1;zaro-Gredilla M (2014) Doubly stochastic variational bayes for non-conjugate inference. In: International conference on machine learning, pp 1971&#x02013;1979</mixed-citation></ref><ref id="CR52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallejos</surname><given-names>CA</given-names></name><name><surname>Marioni</surname><given-names>JC</given-names></name><name><surname>Richardson</surname><given-names>S</given-names></name></person-group><article-title>Basics: Bayesian analysis of single-cell sequencing data</article-title><source>PLoS Comput Biol</source><year>2015</year><volume>11</volume><fpage>e1004,333</fpage></element-citation></ref><ref id="CR53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallejos</surname><given-names>CA</given-names></name><name><surname>Richardson</surname><given-names>S</given-names></name><name><surname>Marioni</surname><given-names>JC</given-names></name></person-group><article-title>Beyond comparisons of means: understanding changes in gene expression at the single-cell level</article-title><source>Genome Biol</source><year>2016</year><volume>17</volume><fpage>70</fpage><pub-id pub-id-type="pmid">27083558</pub-id></element-citation></ref><ref id="CR54"><mixed-citation publication-type="other">Yao Y, Vehtari A, Simpson D, Gelman A (2018) Yes, but did it work?: evaluating variational inference. arXiv:<ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1802.02538">1802.02538</ext-link></mixed-citation></ref><ref id="CR55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>K</given-names></name><name><surname>Sakoparnig</surname><given-names>T</given-names></name><name><surname>Markowetz</surname><given-names>F</given-names></name><name><surname>Beerenwinkel</surname><given-names>N</given-names></name></person-group><article-title>Bitphylogeny: a probabilistic framework for reconstructing intra-tumor phylogenies</article-title><source>Genome Biol</source><year>2015</year><volume>16</volume><issue>1</issue><fpage>36</fpage><pub-id pub-id-type="pmid">25786108</pub-id></element-citation></ref><ref id="CR56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zafar</surname><given-names>H</given-names></name><name><surname>Navin</surname><given-names>N</given-names></name><name><surname>Nakhleh</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name></person-group><article-title>Computational approaches for inferring tumor evolution from single-cell genomic data</article-title><source>Curr Opin Syst Biol</source><year>2018</year><volume>7</volume><fpage>16</fpage><lpage>25</lpage></element-citation></ref><ref id="CR57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Butepage</surname><given-names>J</given-names></name><name><surname>Kjellstrom</surname><given-names>H</given-names></name><name><surname>Mand</surname><given-names>S</given-names></name></person-group><source>Advances in variational inference. IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2018</year></element-citation></ref></ref-list></back></article>